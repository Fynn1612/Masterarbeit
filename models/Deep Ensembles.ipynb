{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0d3b88",
   "metadata": {},
   "source": [
    "Heteroscedastic Implementation of Deep Ensembles\n",
    "\n",
    "no adversarial Training is used\n",
    "\n",
    "Most of the code inspired by:\n",
    "https://github.com/cameronccohen/deep-ensembles/blob/master/Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981020c",
   "metadata": {},
   "source": [
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23cc680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import uncertainty_toolbox as uct\n",
    "import time\n",
    "\n",
    "# define the device for the setting\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# check the computer name and set the path accordingly\n",
    "if os.environ['COMPUTERNAME'] == 'FYNN':            # name of surface PC\n",
    "    sys.path.append(r'C:\\Users\\Surface\\Masterarbeit')\n",
    "elif os.environ['COMPUTERNAME'] == 'FYNNS-PC':  # desktop name\n",
    "    sys.path.append(r'C:\\Users\\test\\Masterarbeit')\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"Unbekannter Computername: \" + os.environ['COMPUTERNAME'])\n",
    "\n",
    "from utils.data_prep import load_tranform_and_split_data, set_seed\n",
    "from utils.metrices import evaluate_intervals\n",
    "from utils.NN_model import Custom_NN_Model, train_model, heteroscedastic_loss\n",
    "from utils.NN_model import create_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405e4f5",
   "metadata": {},
   "source": [
    "Load, Transform and Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35e781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and transform the data, split it into training, validation, and test sets\n",
    "# the split ratio is 60% training, 20% validation, and 20%\n",
    "# return the feature names for later use\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, feature_names = load_tranform_and_split_data('C1_V01_delta_kan', split_ratio=(0.6, 0.2, 0.2))\n",
    "\n",
    "# convert the data to PyTorch tensors\n",
    "# and add an extra dimension for the target variable\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float().reshape(-1,1) # Add extra dimension for compatibility\n",
    "y_val_tensor = torch.from_numpy(y_val).float().reshape(-1,1)\n",
    "y_test_tensor = torch.from_numpy(y_test).float().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2c5d5",
   "metadata": {},
   "source": [
    "Optuna Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    # hyperparameter for sampling with Optuna\n",
    "    n_layer = trial.suggest_int(\"n_layer\", 2, 5)  # number of hidden layers\n",
    "    n_neurons = trial.suggest_int(\"n_neurons\", 64, 320, step=32)  # number of neurons in each hidden layer\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)    # learning rate\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128]) \n",
    "    decay = trial.suggest_float('decay', 0.5, 0.7, step = 0.1)\n",
    "\n",
    "    # decay factor for the number of neurons in each layer\n",
    "    # e.g. if n_neurons = 256 and n_layer = 3\n",
    "    # then the hidden_dims will be [256, 128, 64]\n",
    "    # this creates a list of integers representing the number of neurons in each hidden layer\n",
    "    hidden_dims = [int(n_neurons * decay**i) for i in range(n_layer)]\n",
    "    print(f\"Hidden dimensions: {hidden_dims}\")\n",
    "        \n",
    "    # generate the model with the sampled hyperparameters\n",
    "    # and move it to the device (GPU or CPU)\n",
    "    model = Custom_NN_Model(\n",
    "        input_dim=X_train_tensor.shape[1],\n",
    "        hidden_dims=hidden_dims,\n",
    "        output_dim=1,\n",
    "        do_rate=0,\n",
    "        loss_type = 'heteroscedastic'\n",
    "    ).to(device)\n",
    "\n",
    "    # AdamW optimizer, where weight decay does not accumulate in the momentum nor variance.\n",
    "    optimizer = torch.optim.AdamW(params = model.parameters(), lr = lr, weight_decay=0.0001)  \n",
    "\n",
    "    # DataLoader for batching the data\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # integrate early stopping\n",
    "    patience = 20  # number of epochs with no improvement after which training will be stopped\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # training the model\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)  # Move data to the device (GPU or CPU)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = heteroscedastic_loss(model, X_batch, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # validation loss calculation after each epoch\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = heteroscedastic_loss(model, X_val_tensor, y_val_tensor)\n",
    "        \n",
    "        # report the validation loss to Optuna\n",
    "        trial.report(val_loss, step=epoch)\n",
    "        # handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "           \n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}, Best Val Loss: {best_val_loss:.4f}\")\n",
    "                break\n",
    "    \n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074311a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs for training\n",
    "epochs = 500\n",
    "\n",
    "# create a study object for Optuna\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(),                       #TPE (Tree-structured Parzen Estimator) sampler by default\n",
    "    pruner=optuna.pruners.MedianPruner(        \n",
    "        n_startup_trials=20,                                    # Number of trials to run before pruning starts\n",
    "        n_warmup_steps=5                                        # Number of warmup steps before pruning starts)\n",
    "    )\n",
    ")\n",
    "\n",
    "# move the tensors to the device\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "X_val_tensor = X_val_tensor.to(device)\n",
    "y_val_tensor = y_val_tensor.to(device)\n",
    "\n",
    "# optimize the objective function with Optuna\n",
    "# timeout=None means no time limit for the optimization, all trials will be run\n",
    "study.optimize(objective, n_trials=200, timeout=None, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89386231",
   "metadata": {},
   "source": [
    "Create an Ensemble, train it and make predictions\n",
    "\n",
    "best Model Architectur implemented:\n",
    "hidden_dims=[320,224,156]\n",
    "\n",
    "Best trial:\n",
    "  Value:  1.4413212537765503\n",
    "  Params: \n",
    "    n_layer: 3\n",
    "    n_neurons: 320\n",
    "    lr: 0.0004348009611810878\n",
    "    batch_size: 64\n",
    "    decay: 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fafe1be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 522.9876, Val Loss: 6.6945, Best Val Loss: 6.6945\n",
      "Epoch 2/1000, Train Loss: 5.8651, Val Loss: 5.5703, Best Val Loss: 5.5703\n",
      "Epoch 3/1000, Train Loss: 5.5277, Val Loss: 5.5006, Best Val Loss: 5.5006\n",
      "Epoch 4/1000, Train Loss: 5.4858, Val Loss: 5.4681, Best Val Loss: 5.4681\n",
      "Epoch 5/1000, Train Loss: 5.4547, Val Loss: 5.4375, Best Val Loss: 5.4375\n",
      "Epoch 6/1000, Train Loss: 5.4239, Val Loss: 5.4063, Best Val Loss: 5.4063\n",
      "Epoch 7/1000, Train Loss: 5.3917, Val Loss: 5.3730, Best Val Loss: 5.3730\n",
      "Epoch 8/1000, Train Loss: 5.3561, Val Loss: 5.3354, Best Val Loss: 5.3354\n",
      "Epoch 9/1000, Train Loss: 5.3144, Val Loss: 5.2902, Best Val Loss: 5.2902\n",
      "Epoch 10/1000, Train Loss: 5.2630, Val Loss: 5.2332, Best Val Loss: 5.2332\n",
      "Epoch 11/1000, Train Loss: 5.1968, Val Loss: 5.1564, Best Val Loss: 5.1564\n",
      "Epoch 12/1000, Train Loss: 5.0995, Val Loss: 5.0301, Best Val Loss: 5.0301\n",
      "Epoch 13/1000, Train Loss: 4.8993, Val Loss: 4.6902, Best Val Loss: 4.6902\n",
      "Epoch 14/1000, Train Loss: 3.4928, Val Loss: 2.4327, Best Val Loss: 2.4327\n",
      "Epoch 15/1000, Train Loss: 2.3059, Val Loss: 2.2380, Best Val Loss: 2.2380\n",
      "Epoch 16/1000, Train Loss: 2.1518, Val Loss: 2.1195, Best Val Loss: 2.1195\n",
      "Epoch 17/1000, Train Loss: 2.0587, Val Loss: 2.0552, Best Val Loss: 2.0552\n",
      "Epoch 18/1000, Train Loss: 1.9929, Val Loss: 1.9858, Best Val Loss: 1.9858\n",
      "Epoch 19/1000, Train Loss: 1.9424, Val Loss: 1.9391, Best Val Loss: 1.9391\n",
      "Epoch 20/1000, Train Loss: 1.8998, Val Loss: 1.8961, Best Val Loss: 1.8961\n",
      "Epoch 21/1000, Train Loss: 1.8648, Val Loss: 1.8605, Best Val Loss: 1.8605\n",
      "Epoch 22/1000, Train Loss: 1.8360, Val Loss: 1.8369, Best Val Loss: 1.8369\n",
      "Epoch 23/1000, Train Loss: 1.8140, Val Loss: 1.8297, Best Val Loss: 1.8297\n",
      "Epoch 24/1000, Train Loss: 1.7907, Val Loss: 1.8194, Best Val Loss: 1.8194\n",
      "Epoch 25/1000, Train Loss: 1.7755, Val Loss: 1.7812, Best Val Loss: 1.7812\n",
      "Epoch 26/1000, Train Loss: 1.7580, Val Loss: 1.7651, Best Val Loss: 1.7651\n",
      "Epoch 27/1000, Train Loss: 1.7449, Val Loss: 1.7483, Best Val Loss: 1.7483\n",
      "Epoch 29/1000, Train Loss: 1.7197, Val Loss: 1.7262, Best Val Loss: 1.7262\n",
      "Epoch 30/1000, Train Loss: 1.7129, Val Loss: 1.7178, Best Val Loss: 1.7178\n",
      "Epoch 31/1000, Train Loss: 1.6987, Val Loss: 1.7082, Best Val Loss: 1.7082\n",
      "Epoch 33/1000, Train Loss: 1.6768, Val Loss: 1.6842, Best Val Loss: 1.6842\n",
      "Epoch 36/1000, Train Loss: 1.6461, Val Loss: 1.6795, Best Val Loss: 1.6795\n",
      "Epoch 37/1000, Train Loss: 1.6376, Val Loss: 1.6737, Best Val Loss: 1.6737\n",
      "Epoch 38/1000, Train Loss: 1.6311, Val Loss: 1.6429, Best Val Loss: 1.6429\n",
      "Epoch 39/1000, Train Loss: 1.6182, Val Loss: 1.6414, Best Val Loss: 1.6414\n",
      "Epoch 41/1000, Train Loss: 1.6036, Val Loss: 1.6308, Best Val Loss: 1.6308\n",
      "Epoch 44/1000, Train Loss: 1.5835, Val Loss: 1.6212, Best Val Loss: 1.6212\n",
      "Epoch 46/1000, Train Loss: 1.5770, Val Loss: 1.6078, Best Val Loss: 1.6078\n",
      "Epoch 49/1000, Train Loss: 1.5854, Val Loss: 1.5947, Best Val Loss: 1.5947\n",
      "Epoch 50/1000, Train Loss: 1.5539, Val Loss: 1.5893, Best Val Loss: 1.5893\n",
      "Epoch 52/1000, Train Loss: 1.5448, Val Loss: 1.5841, Best Val Loss: 1.5841\n",
      "Epoch 54/1000, Train Loss: 1.5428, Val Loss: 1.5729, Best Val Loss: 1.5729\n",
      "Epoch 59/1000, Train Loss: 1.5194, Val Loss: 1.5640, Best Val Loss: 1.5640\n",
      "Epoch 63/1000, Train Loss: 1.5136, Val Loss: 1.5448, Best Val Loss: 1.5448\n",
      "Epoch 69/1000, Train Loss: 1.4897, Val Loss: 1.5361, Best Val Loss: 1.5361\n",
      "Epoch 77/1000, Train Loss: 1.4762, Val Loss: 1.5259, Best Val Loss: 1.5259\n",
      "Epoch 86/1000, Train Loss: 1.4646, Val Loss: 1.5207, Best Val Loss: 1.5207\n",
      "Epoch 91/1000, Train Loss: 1.4547, Val Loss: 1.5178, Best Val Loss: 1.5178\n",
      "Epoch 98/1000, Train Loss: 1.4386, Val Loss: 1.5122, Best Val Loss: 1.5122\n",
      "Epoch 99/1000, Train Loss: 1.4400, Val Loss: 1.5027, Best Val Loss: 1.5027\n",
      "Epoch 103/1000, Train Loss: 1.4382, Val Loss: 1.4991, Best Val Loss: 1.4991\n",
      "Epoch 110/1000, Train Loss: 1.4251, Val Loss: 1.4979, Best Val Loss: 1.4979\n",
      "Epoch 111/1000, Train Loss: 1.4244, Val Loss: 1.4934, Best Val Loss: 1.4934\n",
      "Epoch 121/1000, Train Loss: 1.4146, Val Loss: 1.4843, Best Val Loss: 1.4843\n",
      "Epoch 127/1000, Train Loss: 1.4144, Val Loss: 1.4765, Best Val Loss: 1.4765\n",
      "Epoch 139/1000, Train Loss: 1.4011, Val Loss: 1.4735, Best Val Loss: 1.4735\n",
      "Epoch 141/1000, Train Loss: 1.3997, Val Loss: 1.4708, Best Val Loss: 1.4708\n",
      "Epoch 150/1000, Train Loss: 1.3860, Val Loss: 1.4706, Best Val Loss: 1.4706\n",
      "Epoch 152/1000, Train Loss: 1.3907, Val Loss: 1.4611, Best Val Loss: 1.4611\n",
      "Epoch 181/1000, Train Loss: 1.3652, Val Loss: 1.4584, Best Val Loss: 1.4584\n",
      "Early stopping at epoch 231, Best Val Loss: 1.4584\n"
     ]
    }
   ],
   "source": [
    "net = Custom_NN_Model(\n",
    "        input_dim=X_train_tensor.shape[1],\n",
    "        hidden_dims=[320,224,156],\n",
    "        output_dim=1,\n",
    "        do_rate=0,\n",
    "        loss_type = 'heteroscedastic'\n",
    "    ).to(device)\n",
    "optimizer = torch.optim.AdamW(params = net.parameters(), lr = 0.0001, weight_decay=0.0001)\n",
    "model1 = train_model(model= net, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "                    X_val_tensor= X_val_tensor, y_val_tensor=y_val_tensor, batch_size=64, \n",
    "                    optimizer=optimizer, n_epochs=1000, patience=50, loss_type='heteroscedastic',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf93dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.cpu()\n",
    "model1.eval() \n",
    "with torch.no_grad():   \n",
    "    output_mean, output_log_var = model1(X_test_tensor.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98b6ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 133.2339, Val Loss: 5.5547, Best Val Loss: 5.5547\n",
      "Epoch 2/1000, Train Loss: 5.4808, Val Loss: 5.4084, Best Val Loss: 5.4084\n",
      "Epoch 3/1000, Train Loss: 5.3600, Val Loss: 5.3127, Best Val Loss: 5.3127\n",
      "Epoch 4/1000, Train Loss: 5.2746, Val Loss: 5.2341, Best Val Loss: 5.2341\n",
      "Epoch 5/1000, Train Loss: 5.1776, Val Loss: 5.0900, Best Val Loss: 5.0900\n",
      "Epoch 6/1000, Train Loss: 3.7884, Val Loss: 2.3260, Best Val Loss: 2.3260\n",
      "Epoch 7/1000, Train Loss: 2.1868, Val Loss: 2.1120, Best Val Loss: 2.1120\n",
      "Epoch 8/1000, Train Loss: 2.0184, Val Loss: 1.9830, Best Val Loss: 1.9830\n",
      "Epoch 9/1000, Train Loss: 1.9246, Val Loss: 1.8810, Best Val Loss: 1.8810\n",
      "Epoch 10/1000, Train Loss: 1.8695, Val Loss: 1.8453, Best Val Loss: 1.8453\n",
      "Epoch 11/1000, Train Loss: 1.8191, Val Loss: 1.8175, Best Val Loss: 1.8175\n",
      "Epoch 12/1000, Train Loss: 1.7879, Val Loss: 1.7776, Best Val Loss: 1.7776\n",
      "Epoch 14/1000, Train Loss: 1.7460, Val Loss: 1.7428, Best Val Loss: 1.7428\n",
      "Epoch 17/1000, Train Loss: 1.7051, Val Loss: 1.7163, Best Val Loss: 1.7163\n",
      "Epoch 19/1000, Train Loss: 1.6943, Val Loss: 1.7010, Best Val Loss: 1.7010\n",
      "Epoch 20/1000, Train Loss: 1.6753, Val Loss: 1.6615, Best Val Loss: 1.6615\n",
      "Epoch 27/1000, Train Loss: 1.6231, Val Loss: 1.6359, Best Val Loss: 1.6359\n",
      "Epoch 29/1000, Train Loss: 1.6142, Val Loss: 1.6145, Best Val Loss: 1.6145\n",
      "Epoch 31/1000, Train Loss: 1.5964, Val Loss: 1.6081, Best Val Loss: 1.6081\n",
      "Epoch 36/1000, Train Loss: 1.5621, Val Loss: 1.5817, Best Val Loss: 1.5817\n",
      "Epoch 37/1000, Train Loss: 1.5548, Val Loss: 1.5580, Best Val Loss: 1.5580\n",
      "Epoch 40/1000, Train Loss: 1.5399, Val Loss: 1.5444, Best Val Loss: 1.5444\n",
      "Epoch 52/1000, Train Loss: 1.4877, Val Loss: 1.5356, Best Val Loss: 1.5356\n",
      "Epoch 56/1000, Train Loss: 1.4794, Val Loss: 1.5231, Best Val Loss: 1.5231\n",
      "Epoch 58/1000, Train Loss: 1.4724, Val Loss: 1.4941, Best Val Loss: 1.4941\n",
      "Epoch 70/1000, Train Loss: 1.4282, Val Loss: 1.4876, Best Val Loss: 1.4876\n",
      "Epoch 73/1000, Train Loss: 1.4221, Val Loss: 1.4874, Best Val Loss: 1.4874\n",
      "Epoch 82/1000, Train Loss: 1.4010, Val Loss: 1.4766, Best Val Loss: 1.4766\n",
      "Epoch 96/1000, Train Loss: 1.3741, Val Loss: 1.4612, Best Val Loss: 1.4612\n",
      "Epoch 110/1000, Train Loss: 1.3486, Val Loss: 1.4583, Best Val Loss: 1.4583\n",
      "Early stopping at epoch 160, Best Val Loss: 1.4583\n",
      "Training time for this model: 376.0991 seconds\n",
      "Model training mode: False\n",
      "Model training mode: False\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 155.8659, Val Loss: 5.5320, Best Val Loss: 5.5320\n",
      "Epoch 2/1000, Train Loss: 5.4893, Val Loss: 5.4354, Best Val Loss: 5.4354\n",
      "Epoch 3/1000, Train Loss: 5.3945, Val Loss: 5.3397, Best Val Loss: 5.3397\n",
      "Epoch 4/1000, Train Loss: 5.2709, Val Loss: 5.1706, Best Val Loss: 5.1706\n",
      "Epoch 5/1000, Train Loss: 4.6568, Val Loss: 2.6710, Best Val Loss: 2.6710\n",
      "Epoch 6/1000, Train Loss: 2.4852, Val Loss: 2.3558, Best Val Loss: 2.3558\n",
      "Epoch 7/1000, Train Loss: 2.2376, Val Loss: 2.1726, Best Val Loss: 2.1726\n",
      "Epoch 8/1000, Train Loss: 2.0835, Val Loss: 2.0401, Best Val Loss: 2.0401\n",
      "Epoch 9/1000, Train Loss: 1.9745, Val Loss: 1.9330, Best Val Loss: 1.9330\n",
      "Epoch 10/1000, Train Loss: 1.9013, Val Loss: 1.8981, Best Val Loss: 1.8981\n",
      "Epoch 11/1000, Train Loss: 1.8517, Val Loss: 1.8572, Best Val Loss: 1.8572\n",
      "Epoch 12/1000, Train Loss: 1.8212, Val Loss: 1.8189, Best Val Loss: 1.8189\n",
      "Epoch 13/1000, Train Loss: 1.7911, Val Loss: 1.7949, Best Val Loss: 1.7949\n",
      "Epoch 14/1000, Train Loss: 1.7617, Val Loss: 1.7633, Best Val Loss: 1.7633\n",
      "Epoch 16/1000, Train Loss: 1.7382, Val Loss: 1.7265, Best Val Loss: 1.7265\n",
      "Epoch 19/1000, Train Loss: 1.7024, Val Loss: 1.6734, Best Val Loss: 1.6734\n",
      "Epoch 23/1000, Train Loss: 1.6911, Val Loss: 1.6719, Best Val Loss: 1.6719\n",
      "Epoch 27/1000, Train Loss: 1.6451, Val Loss: 1.6516, Best Val Loss: 1.6516\n",
      "Epoch 29/1000, Train Loss: 1.6296, Val Loss: 1.5999, Best Val Loss: 1.5999\n",
      "Epoch 35/1000, Train Loss: 1.5916, Val Loss: 1.5688, Best Val Loss: 1.5688\n",
      "Epoch 50/1000, Train Loss: 1.5242, Val Loss: 1.5626, Best Val Loss: 1.5626\n",
      "Epoch 51/1000, Train Loss: 1.5272, Val Loss: 1.5544, Best Val Loss: 1.5544\n",
      "Epoch 53/1000, Train Loss: 1.5117, Val Loss: 1.5522, Best Val Loss: 1.5522\n",
      "Epoch 56/1000, Train Loss: 1.5087, Val Loss: 1.5464, Best Val Loss: 1.5464\n",
      "Epoch 57/1000, Train Loss: 1.5105, Val Loss: 1.5286, Best Val Loss: 1.5286\n",
      "Epoch 62/1000, Train Loss: 1.4876, Val Loss: 1.5254, Best Val Loss: 1.5254\n",
      "Epoch 67/1000, Train Loss: 1.4786, Val Loss: 1.4978, Best Val Loss: 1.4978\n",
      "Epoch 78/1000, Train Loss: 1.4446, Val Loss: 1.4967, Best Val Loss: 1.4967\n",
      "Epoch 85/1000, Train Loss: 1.4347, Val Loss: 1.4871, Best Val Loss: 1.4871\n",
      "Epoch 88/1000, Train Loss: 1.4261, Val Loss: 1.4809, Best Val Loss: 1.4809\n",
      "Epoch 91/1000, Train Loss: 1.4277, Val Loss: 1.4614, Best Val Loss: 1.4614\n",
      "Early stopping at epoch 141, Best Val Loss: 1.4614\n",
      "Training time for this model: 308.7340 seconds\n",
      "Model training mode: False\n",
      "Model training mode: False\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 161.6388, Val Loss: 5.5242, Best Val Loss: 5.5242\n",
      "Epoch 2/1000, Train Loss: 5.4553, Val Loss: 5.3881, Best Val Loss: 5.3881\n",
      "Epoch 3/1000, Train Loss: 5.3390, Val Loss: 5.2775, Best Val Loss: 5.2775\n",
      "Epoch 4/1000, Train Loss: 5.1800, Val Loss: 5.0205, Best Val Loss: 5.0205\n",
      "Epoch 5/1000, Train Loss: 3.8773, Val Loss: 2.4623, Best Val Loss: 2.4623\n",
      "Epoch 6/1000, Train Loss: 2.2827, Val Loss: 2.1714, Best Val Loss: 2.1714\n",
      "Epoch 7/1000, Train Loss: 2.0924, Val Loss: 2.0325, Best Val Loss: 2.0325\n",
      "Epoch 8/1000, Train Loss: 1.9676, Val Loss: 1.9471, Best Val Loss: 1.9471\n",
      "Epoch 9/1000, Train Loss: 1.8902, Val Loss: 1.8875, Best Val Loss: 1.8875\n",
      "Epoch 10/1000, Train Loss: 1.8459, Val Loss: 1.8234, Best Val Loss: 1.8234\n",
      "Epoch 12/1000, Train Loss: 1.7900, Val Loss: 1.7652, Best Val Loss: 1.7652\n",
      "Epoch 14/1000, Train Loss: 1.7513, Val Loss: 1.7599, Best Val Loss: 1.7599\n",
      "Epoch 15/1000, Train Loss: 1.7297, Val Loss: 1.7538, Best Val Loss: 1.7538\n",
      "Epoch 17/1000, Train Loss: 1.7110, Val Loss: 1.6969, Best Val Loss: 1.6969\n",
      "Epoch 20/1000, Train Loss: 1.6829, Val Loss: 1.6961, Best Val Loss: 1.6961\n",
      "Epoch 23/1000, Train Loss: 1.6514, Val Loss: 1.6561, Best Val Loss: 1.6561\n",
      "Epoch 24/1000, Train Loss: 1.6494, Val Loss: 1.6328, Best Val Loss: 1.6328\n",
      "Epoch 30/1000, Train Loss: 1.6127, Val Loss: 1.6022, Best Val Loss: 1.6022\n",
      "Epoch 31/1000, Train Loss: 1.6092, Val Loss: 1.5934, Best Val Loss: 1.5934\n",
      "Epoch 33/1000, Train Loss: 1.5880, Val Loss: 1.5805, Best Val Loss: 1.5805\n",
      "Epoch 35/1000, Train Loss: 1.5816, Val Loss: 1.5786, Best Val Loss: 1.5786\n",
      "Epoch 42/1000, Train Loss: 1.5416, Val Loss: 1.5510, Best Val Loss: 1.5510\n",
      "Epoch 46/1000, Train Loss: 1.5299, Val Loss: 1.5424, Best Val Loss: 1.5424\n",
      "Epoch 54/1000, Train Loss: 1.5008, Val Loss: 1.5357, Best Val Loss: 1.5357\n",
      "Epoch 55/1000, Train Loss: 1.4990, Val Loss: 1.5165, Best Val Loss: 1.5165\n",
      "Epoch 66/1000, Train Loss: 1.4622, Val Loss: 1.5081, Best Val Loss: 1.5081\n",
      "Epoch 67/1000, Train Loss: 1.4600, Val Loss: 1.5071, Best Val Loss: 1.5071\n",
      "Epoch 68/1000, Train Loss: 1.4567, Val Loss: 1.5005, Best Val Loss: 1.5005\n",
      "Epoch 77/1000, Train Loss: 1.4375, Val Loss: 1.4979, Best Val Loss: 1.4979\n",
      "Epoch 82/1000, Train Loss: 1.4346, Val Loss: 1.4937, Best Val Loss: 1.4937\n",
      "Epoch 84/1000, Train Loss: 1.4209, Val Loss: 1.4857, Best Val Loss: 1.4857\n",
      "Epoch 85/1000, Train Loss: 1.4264, Val Loss: 1.4767, Best Val Loss: 1.4767\n",
      "Epoch 110/1000, Train Loss: 1.3822, Val Loss: 1.4625, Best Val Loss: 1.4625\n",
      "Epoch 111/1000, Train Loss: 1.3787, Val Loss: 1.4518, Best Val Loss: 1.4518\n",
      "Epoch 119/1000, Train Loss: 1.3679, Val Loss: 1.4510, Best Val Loss: 1.4510\n",
      "Early stopping at epoch 169, Best Val Loss: 1.4510\n",
      "Training time for this model: 364.7834 seconds\n",
      "Model training mode: False\n",
      "Model training mode: False\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 141.9633, Val Loss: 5.4522, Best Val Loss: 5.4522\n",
      "Epoch 2/1000, Train Loss: 5.3449, Val Loss: 5.2888, Best Val Loss: 5.2888\n",
      "Epoch 3/1000, Train Loss: 5.2614, Val Loss: 5.2350, Best Val Loss: 5.2350\n",
      "Epoch 4/1000, Train Loss: 5.2085, Val Loss: 5.1816, Best Val Loss: 5.1816\n",
      "Epoch 5/1000, Train Loss: 5.1380, Val Loss: 5.0627, Best Val Loss: 5.0627\n",
      "Epoch 6/1000, Train Loss: 4.4185, Val Loss: 3.0993, Best Val Loss: 3.0993\n",
      "Epoch 7/1000, Train Loss: 2.6637, Val Loss: 2.3641, Best Val Loss: 2.3641\n",
      "Epoch 8/1000, Train Loss: 2.1990, Val Loss: 2.1153, Best Val Loss: 2.1153\n",
      "Epoch 9/1000, Train Loss: 2.0275, Val Loss: 2.0083, Best Val Loss: 2.0083\n",
      "Epoch 10/1000, Train Loss: 1.9336, Val Loss: 1.9207, Best Val Loss: 1.9207\n",
      "Epoch 11/1000, Train Loss: 1.8774, Val Loss: 1.8536, Best Val Loss: 1.8536\n",
      "Epoch 12/1000, Train Loss: 1.8403, Val Loss: 1.8178, Best Val Loss: 1.8178\n",
      "Epoch 13/1000, Train Loss: 1.8107, Val Loss: 1.8060, Best Val Loss: 1.8060\n",
      "Epoch 14/1000, Train Loss: 1.7791, Val Loss: 1.7874, Best Val Loss: 1.7874\n",
      "Epoch 16/1000, Train Loss: 1.7522, Val Loss: 1.7454, Best Val Loss: 1.7454\n",
      "Epoch 17/1000, Train Loss: 1.7341, Val Loss: 1.7406, Best Val Loss: 1.7406\n",
      "Epoch 18/1000, Train Loss: 1.7247, Val Loss: 1.7390, Best Val Loss: 1.7390\n",
      "Epoch 20/1000, Train Loss: 1.6957, Val Loss: 1.6960, Best Val Loss: 1.6960\n",
      "Epoch 24/1000, Train Loss: 1.6670, Val Loss: 1.6727, Best Val Loss: 1.6727\n",
      "Epoch 26/1000, Train Loss: 1.6473, Val Loss: 1.6619, Best Val Loss: 1.6619\n",
      "Epoch 27/1000, Train Loss: 1.6427, Val Loss: 1.6471, Best Val Loss: 1.6471\n",
      "Epoch 28/1000, Train Loss: 1.6354, Val Loss: 1.6299, Best Val Loss: 1.6299\n",
      "Epoch 33/1000, Train Loss: 1.5994, Val Loss: 1.6186, Best Val Loss: 1.6186\n",
      "Epoch 34/1000, Train Loss: 1.6012, Val Loss: 1.6016, Best Val Loss: 1.6016\n",
      "Epoch 35/1000, Train Loss: 1.5737, Val Loss: 1.5919, Best Val Loss: 1.5919\n",
      "Epoch 39/1000, Train Loss: 1.5730, Val Loss: 1.5788, Best Val Loss: 1.5788\n",
      "Epoch 43/1000, Train Loss: 1.5636, Val Loss: 1.5608, Best Val Loss: 1.5608\n",
      "Epoch 47/1000, Train Loss: 1.5325, Val Loss: 1.5317, Best Val Loss: 1.5317\n",
      "Epoch 60/1000, Train Loss: 1.4919, Val Loss: 1.5105, Best Val Loss: 1.5105\n",
      "Epoch 72/1000, Train Loss: 1.4568, Val Loss: 1.5028, Best Val Loss: 1.5028\n",
      "Epoch 73/1000, Train Loss: 1.4528, Val Loss: 1.4913, Best Val Loss: 1.4913\n",
      "Epoch 85/1000, Train Loss: 1.4326, Val Loss: 1.4881, Best Val Loss: 1.4881\n",
      "Epoch 90/1000, Train Loss: 1.4129, Val Loss: 1.4839, Best Val Loss: 1.4839\n",
      "Epoch 91/1000, Train Loss: 1.4186, Val Loss: 1.4676, Best Val Loss: 1.4676\n",
      "Early stopping at epoch 141, Best Val Loss: 1.4676\n",
      "Training time for this model: 287.6257 seconds\n",
      "Model training mode: False\n",
      "Model training mode: False\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 135.4291, Val Loss: 5.4477, Best Val Loss: 5.4477\n",
      "Epoch 2/1000, Train Loss: 5.3666, Val Loss: 5.3189, Best Val Loss: 5.3189\n",
      "Epoch 3/1000, Train Loss: 5.2894, Val Loss: 5.2613, Best Val Loss: 5.2613\n",
      "Epoch 4/1000, Train Loss: 5.2343, Val Loss: 5.2061, Best Val Loss: 5.2061\n",
      "Epoch 5/1000, Train Loss: 5.1573, Val Loss: 5.0750, Best Val Loss: 5.0750\n",
      "Epoch 6/1000, Train Loss: 3.9839, Val Loss: 2.6854, Best Val Loss: 2.6854\n",
      "Epoch 7/1000, Train Loss: 2.3714, Val Loss: 2.1925, Best Val Loss: 2.1925\n",
      "Epoch 8/1000, Train Loss: 2.1001, Val Loss: 2.0522, Best Val Loss: 2.0522\n",
      "Epoch 9/1000, Train Loss: 1.9809, Val Loss: 1.9666, Best Val Loss: 1.9666\n",
      "Epoch 10/1000, Train Loss: 1.9053, Val Loss: 1.9498, Best Val Loss: 1.9498\n",
      "Epoch 11/1000, Train Loss: 1.8489, Val Loss: 1.8560, Best Val Loss: 1.8560\n",
      "Epoch 12/1000, Train Loss: 1.8208, Val Loss: 1.8349, Best Val Loss: 1.8349\n",
      "Epoch 13/1000, Train Loss: 1.7816, Val Loss: 1.7857, Best Val Loss: 1.7857\n",
      "Epoch 14/1000, Train Loss: 1.7588, Val Loss: 1.7741, Best Val Loss: 1.7741\n",
      "Epoch 15/1000, Train Loss: 1.7447, Val Loss: 1.7664, Best Val Loss: 1.7664\n",
      "Epoch 16/1000, Train Loss: 1.7178, Val Loss: 1.7099, Best Val Loss: 1.7099\n",
      "Epoch 17/1000, Train Loss: 1.7164, Val Loss: 1.7051, Best Val Loss: 1.7051\n",
      "Epoch 19/1000, Train Loss: 1.6901, Val Loss: 1.6762, Best Val Loss: 1.6762\n",
      "Epoch 26/1000, Train Loss: 1.6351, Val Loss: 1.6431, Best Val Loss: 1.6431\n",
      "Epoch 28/1000, Train Loss: 1.6132, Val Loss: 1.6377, Best Val Loss: 1.6377\n",
      "Epoch 31/1000, Train Loss: 1.5895, Val Loss: 1.6210, Best Val Loss: 1.6210\n",
      "Epoch 35/1000, Train Loss: 1.5677, Val Loss: 1.5813, Best Val Loss: 1.5813\n",
      "Epoch 39/1000, Train Loss: 1.5498, Val Loss: 1.5807, Best Val Loss: 1.5807\n",
      "Epoch 40/1000, Train Loss: 1.5436, Val Loss: 1.5664, Best Val Loss: 1.5664\n",
      "Epoch 42/1000, Train Loss: 1.5325, Val Loss: 1.5651, Best Val Loss: 1.5651\n",
      "Epoch 43/1000, Train Loss: 1.5290, Val Loss: 1.5624, Best Val Loss: 1.5624\n",
      "Epoch 44/1000, Train Loss: 1.5254, Val Loss: 1.5488, Best Val Loss: 1.5488\n",
      "Epoch 46/1000, Train Loss: 1.5155, Val Loss: 1.5252, Best Val Loss: 1.5252\n",
      "Epoch 55/1000, Train Loss: 1.4758, Val Loss: 1.5105, Best Val Loss: 1.5105\n",
      "Epoch 68/1000, Train Loss: 1.4429, Val Loss: 1.5023, Best Val Loss: 1.5023\n",
      "Epoch 73/1000, Train Loss: 1.4376, Val Loss: 1.4955, Best Val Loss: 1.4955\n",
      "Epoch 80/1000, Train Loss: 1.4169, Val Loss: 1.4864, Best Val Loss: 1.4864\n",
      "Epoch 91/1000, Train Loss: 1.4016, Val Loss: 1.4820, Best Val Loss: 1.4820\n",
      "Epoch 92/1000, Train Loss: 1.3917, Val Loss: 1.4760, Best Val Loss: 1.4760\n",
      "Epoch 122/1000, Train Loss: 1.3397, Val Loss: 1.4728, Best Val Loss: 1.4728\n",
      "Epoch 131/1000, Train Loss: 1.3197, Val Loss: 1.4660, Best Val Loss: 1.4660\n",
      "Epoch 149/1000, Train Loss: 1.2991, Val Loss: 1.4645, Best Val Loss: 1.4645\n",
      "Early stopping at epoch 199, Best Val Loss: 1.4645\n",
      "Training time for this model: 424.8821 seconds\n",
      "Model training mode: False\n",
      "Model training mode: False\n",
      "Train times (s): [376.0991415999597, 308.73397699999623, 364.78339500003494, 287.62565990001895, 424.8821116000181]\n",
      "Prediction times (s): [0.009060199954546988, 0.009223999921232462, 0.007931400090456009, 0.00916809996124357, 0.009345299913547933]\n"
     ]
    }
   ],
   "source": [
    "#create an ensemble of 5 networks with the defined net architecture and optimizer\n",
    "nets_ops = create_ensemble(5, input_dim = X_train.shape[1], hidden_dims=[320,224,156], \n",
    "                           do_rate=0, loss_type='heteroscedastic', lr=0.0004, weight_decay=0.0001)\n",
    "\n",
    "# lists to store the output means and log variances of each network in the ensemble\n",
    "outputs_mean = []\n",
    "outputs_log_var = []\n",
    "\n",
    "train_times = []\n",
    "pred_times = []\n",
    "\n",
    "#train the ensemble of networks and make predictions on the test set\n",
    "for net, ops in nets_ops:\n",
    "    start_train = time.perf_counter()\n",
    "    model = train_model(model= net, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "                        X_val_tensor= X_val_tensor, y_val_tensor=y_val_tensor, batch_size=64, \n",
    "                        optimizer=ops, n_epochs=1000, patience=50, loss_type='heteroscedastic',\n",
    "                        )\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    train_time = time.perf_counter() - start_train\n",
    "    train_times.append(train_time)\n",
    "    print(f\"Training time for this model: {train_time:.4f} seconds\")\n",
    "    print(\"Model training mode:\", model.training)\n",
    "\n",
    "    # set the model to evaluation mode and make predictions on the test set\n",
    "    model.eval()   \n",
    "        # --- prediction time ---\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    start_pred = time.perf_counter()\n",
    "\n",
    "    print(\"Model training mode:\", model.training)\n",
    "    with torch.no_grad():\n",
    "        output_mean, output_log_var = model(X_test_tensor.to(device))\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        output_mean_np, output_log_var_np = output_mean.detach().cpu().numpy(), output_log_var.detach().cpu().numpy()\n",
    "        outputs_mean.append(output_mean_np)\n",
    "        outputs_log_var.append(np.exp(output_log_var_np))\n",
    "        pred_time = time.perf_counter() - start_pred\n",
    "        pred_times.append(pred_time)\n",
    "\n",
    "outputs_mean = np.array(outputs_mean)\n",
    "outputs_log_var = np.array(outputs_log_var)\n",
    "print(f\"Train times (s): {train_times}\")\n",
    "print(f\"Prediction times (s): {pred_times}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2aa179e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.368738085000466\n",
      "0.04472899984102696\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(train_times)/60)\n",
    "print(np.sum(pred_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fba33e",
   "metadata": {},
   "source": [
    "Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a719f44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE Mean shape: (10403,)\n",
      "DE Mean: [111.16658  103.839615 118.50136  ... 121.4084   122.03428  111.72101 ]\n",
      "Epistemic Variance: 1.020178\n",
      "Aleatoric Variance: 8.606825\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n",
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           1.963\n",
      "  RMSE          2.792\n",
      "  MDAE          1.441\n",
      "  MARPD         1.714\n",
      "  R2            0.863\n",
      "  Correlation   0.929\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.043\n",
      "  Mean-absolute Calibration Error       0.039\n",
      "  Miscalibration Area                   0.040\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.051\n",
      "     Group Size: 0.56 -- Calibration Error: 0.043\n",
      "     Group Size: 1.00 -- Calibration Error: 0.039\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.059\n",
      "     Group Size: 0.56 -- Calibration Error: 0.046\n",
      "     Group Size: 1.00 -- Calibration Error: 0.043\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   3.103\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   2.276\n",
      "  CRPS                      1.411\n",
      "  Check Score               0.712\n",
      "  Interval Score            7.092\n",
      "{'accuracy': {'mae': 1.96305911067336, 'rmse': np.float64(2.791879790594302), 'mdae': 1.4413957031249964, 'marpd': np.float64(1.713757398998841), 'r2': 0.8634953002917888, 'corr': np.float64(0.9293401175643178)}, 'avg_calibration': {'rms_cal': np.float64(0.043030171006387015), 'ma_cal': np.float64(0.039349070829412996), 'miscal_area': np.float64(0.03974020581199746)}, 'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.34872222, 0.0514345 , 0.04855798, 0.04552983, 0.04463746,\n",
      "       0.04327471, 0.04195616, 0.04173738, 0.04040802, 0.03934907]), 'adv_group_cali_stderr': array([4.77356720e-02, 3.58724011e-03, 2.91711142e-03, 2.22426345e-03,\n",
      "       2.04363419e-03, 1.71720178e-03, 8.08211439e-04, 8.05554206e-04,\n",
      "       3.93702725e-04, 7.31423639e-18])}, 'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.4225263 , 0.05850629, 0.05151767, 0.0488768 , 0.04832814,\n",
      "       0.04637852, 0.04689931, 0.04495612, 0.04448677, 0.04303017]), 'adv_group_cali_stderr': array([0.06191824, 0.00485769, 0.00381926, 0.00180911, 0.00238097,\n",
      "       0.00149735, 0.00121711, 0.00073509, 0.0007865 , 0.        ])}}, 'sharpness': {'sharp': np.float32(3.1027412)}, 'scoring_rule': {'nll': np.float64(2.2761220640420725), 'crps': np.float64(1.4110229133828842), 'check': np.float64(0.7124503142494808), 'interval': np.float64(7.091611234649525)}}\n",
      "2.791879790594302\n",
      "Coverage 95%: 0.9574161299625108\n",
      "coverage: 0.9574161299625108, MPIW: 10.496780307967729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Adversarial Group Calibration'}, xlabel='Group size', ylabel='Calibration Error of Worst Group'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the mean and standard deviation of the predictions on the test data\n",
    "DE_mean = np.mean(outputs_mean, axis = 0).reshape(-1)\n",
    "\n",
    "# Calculate epistemic\n",
    "epistemic_var_heteroscedastic = np.var(outputs_mean, axis=0)\n",
    "print(f\"Epistemic Variance: {epistemic_var_heteroscedastic.mean():.6f}\")\n",
    "\n",
    "# Calculate aleatoric variance (heteroscedastic)\n",
    "aleatoric_var_heteroscedastic = np.mean(outputs_log_var, axis=0)\n",
    "print(f\"Aleatoric Variance: {aleatoric_var_heteroscedastic.mean():.6f}\")\n",
    "\n",
    "# Calculate total standard deviation\n",
    "DE_std = np.sqrt(epistemic_var_heteroscedastic + aleatoric_var_heteroscedastic).reshape(-1)\n",
    "\n",
    "# Calculate and print all metrics inclunding RMSE, MAE, R²-Score, NLL, CRPS\n",
    "pnn_metrics = uct.metrics.get_all_metrics( DE_mean, DE_std, y_test)\n",
    "print(pnn_metrics)\n",
    "print(pnn_metrics['accuracy']['rmse'])\n",
    "# Calculate coverage for 95% confidence interval\n",
    "coverage_95 = uct.metrics_calibration.get_proportion_in_interval(DE_mean, DE_std, y_test, quantile = 0.95 )\n",
    "print(f\"Coverage 95%: {coverage_95}\")\n",
    "\n",
    "# use own function to calculate coverage and MPIW\n",
    "ev_intervals = evaluate_intervals(DE_mean, DE_std, y_test, coverage=0.95)\n",
    "print(f'coverage: {ev_intervals[\"coverage\"]}, MPIW: {ev_intervals[\"MPIW\"]}')\n",
    "\n",
    "# calibration Curve with UCT\n",
    "uct.viz.plot_calibration(DE_mean, DE_std, y_test)\n",
    "\n",
    "# adversarial group calibration\n",
    "uct.viz.plot_adversarial_group_calibration(DE_mean, DE_std, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f40cb",
   "metadata": {},
   "source": [
    "Evaluate Deep Ensemble predictions with 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb045a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 with seed 42\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 142.1510, Val Loss: 5.5156, Best Val Loss: 5.5156\n",
      "Epoch 2/1000, Train Loss: 5.4695, Val Loss: 5.4248, Best Val Loss: 5.4248\n",
      "Epoch 3/1000, Train Loss: 5.3937, Val Loss: 5.3583, Best Val Loss: 5.3583\n",
      "Epoch 4/1000, Train Loss: 5.3288, Val Loss: 5.2952, Best Val Loss: 5.2952\n",
      "Epoch 5/1000, Train Loss: 5.2648, Val Loss: 5.2313, Best Val Loss: 5.2313\n",
      "Epoch 6/1000, Train Loss: 5.1928, Val Loss: 5.1450, Best Val Loss: 5.1450\n",
      "Epoch 7/1000, Train Loss: 5.0568, Val Loss: 4.9153, Best Val Loss: 4.9153\n",
      "Epoch 8/1000, Train Loss: 3.7240, Val Loss: 2.3674, Best Val Loss: 2.3674\n",
      "Epoch 9/1000, Train Loss: 2.2071, Val Loss: 2.1284, Best Val Loss: 2.1284\n",
      "Epoch 10/1000, Train Loss: 2.0266, Val Loss: 1.9779, Best Val Loss: 1.9779\n",
      "Epoch 11/1000, Train Loss: 1.9279, Val Loss: 1.9249, Best Val Loss: 1.9249\n",
      "Epoch 12/1000, Train Loss: 1.8630, Val Loss: 1.8318, Best Val Loss: 1.8318\n",
      "Epoch 13/1000, Train Loss: 1.8270, Val Loss: 1.8175, Best Val Loss: 1.8175\n",
      "Epoch 14/1000, Train Loss: 1.7861, Val Loss: 1.7813, Best Val Loss: 1.7813\n",
      "Epoch 15/1000, Train Loss: 1.7679, Val Loss: 1.7438, Best Val Loss: 1.7438\n",
      "Epoch 16/1000, Train Loss: 1.7418, Val Loss: 1.7127, Best Val Loss: 1.7127\n",
      "Epoch 18/1000, Train Loss: 1.7189, Val Loss: 1.7076, Best Val Loss: 1.7076\n",
      "Epoch 19/1000, Train Loss: 1.6995, Val Loss: 1.6964, Best Val Loss: 1.6964\n",
      "Epoch 20/1000, Train Loss: 1.6856, Val Loss: 1.6896, Best Val Loss: 1.6896\n",
      "Epoch 21/1000, Train Loss: 1.6860, Val Loss: 1.6644, Best Val Loss: 1.6644\n",
      "Epoch 23/1000, Train Loss: 1.6721, Val Loss: 1.6561, Best Val Loss: 1.6561\n",
      "Epoch 28/1000, Train Loss: 1.6316, Val Loss: 1.6491, Best Val Loss: 1.6491\n",
      "Epoch 29/1000, Train Loss: 1.6326, Val Loss: 1.6171, Best Val Loss: 1.6171\n",
      "Epoch 31/1000, Train Loss: 1.6136, Val Loss: 1.6087, Best Val Loss: 1.6087\n",
      "Epoch 36/1000, Train Loss: 1.5864, Val Loss: 1.5929, Best Val Loss: 1.5929\n",
      "Epoch 38/1000, Train Loss: 1.5755, Val Loss: 1.5553, Best Val Loss: 1.5553\n",
      "Epoch 47/1000, Train Loss: 1.5388, Val Loss: 1.5316, Best Val Loss: 1.5316\n",
      "Epoch 62/1000, Train Loss: 1.4905, Val Loss: 1.5098, Best Val Loss: 1.5098\n",
      "Epoch 80/1000, Train Loss: 1.4511, Val Loss: 1.5043, Best Val Loss: 1.5043\n",
      "Epoch 81/1000, Train Loss: 1.4442, Val Loss: 1.5028, Best Val Loss: 1.5028\n",
      "Epoch 87/1000, Train Loss: 1.4267, Val Loss: 1.4977, Best Val Loss: 1.4977\n",
      "Epoch 95/1000, Train Loss: 1.4166, Val Loss: 1.4736, Best Val Loss: 1.4736\n",
      "Epoch 118/1000, Train Loss: 1.4003, Val Loss: 1.4694, Best Val Loss: 1.4694\n",
      "Epoch 122/1000, Train Loss: 1.3683, Val Loss: 1.4691, Best Val Loss: 1.4691\n",
      "Early stopping at epoch 172, Best Val Loss: 1.4691\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 165.2325, Val Loss: 5.4124, Best Val Loss: 5.4124\n",
      "Epoch 2/1000, Train Loss: 5.3621, Val Loss: 5.3148, Best Val Loss: 5.3148\n",
      "Epoch 3/1000, Train Loss: 5.2822, Val Loss: 5.2471, Best Val Loss: 5.2471\n",
      "Epoch 4/1000, Train Loss: 5.2144, Val Loss: 5.1750, Best Val Loss: 5.1750\n",
      "Epoch 5/1000, Train Loss: 5.1080, Val Loss: 5.0058, Best Val Loss: 5.0058\n",
      "Epoch 6/1000, Train Loss: 4.1437, Val Loss: 2.4217, Best Val Loss: 2.4217\n",
      "Epoch 7/1000, Train Loss: 2.2318, Val Loss: 2.1062, Best Val Loss: 2.1062\n",
      "Epoch 8/1000, Train Loss: 2.0127, Val Loss: 1.9586, Best Val Loss: 1.9586\n",
      "Epoch 9/1000, Train Loss: 1.9073, Val Loss: 1.8927, Best Val Loss: 1.8927\n",
      "Epoch 10/1000, Train Loss: 1.8491, Val Loss: 1.8459, Best Val Loss: 1.8459\n",
      "Epoch 12/1000, Train Loss: 1.7778, Val Loss: 1.7564, Best Val Loss: 1.7564\n",
      "Epoch 14/1000, Train Loss: 1.7371, Val Loss: 1.7522, Best Val Loss: 1.7522\n",
      "Epoch 15/1000, Train Loss: 1.7256, Val Loss: 1.7275, Best Val Loss: 1.7275\n",
      "Epoch 17/1000, Train Loss: 1.6972, Val Loss: 1.7041, Best Val Loss: 1.7041\n",
      "Epoch 18/1000, Train Loss: 1.6867, Val Loss: 1.7003, Best Val Loss: 1.7003\n",
      "Epoch 19/1000, Train Loss: 1.6706, Val Loss: 1.6609, Best Val Loss: 1.6609\n",
      "Epoch 24/1000, Train Loss: 1.6360, Val Loss: 1.6412, Best Val Loss: 1.6412\n",
      "Epoch 30/1000, Train Loss: 1.6081, Val Loss: 1.6108, Best Val Loss: 1.6108\n",
      "Epoch 34/1000, Train Loss: 1.5875, Val Loss: 1.5709, Best Val Loss: 1.5709\n",
      "Epoch 46/1000, Train Loss: 1.5209, Val Loss: 1.5579, Best Val Loss: 1.5579\n",
      "Epoch 52/1000, Train Loss: 1.5094, Val Loss: 1.5468, Best Val Loss: 1.5468\n",
      "Epoch 55/1000, Train Loss: 1.4859, Val Loss: 1.5252, Best Val Loss: 1.5252\n",
      "Epoch 63/1000, Train Loss: 1.4670, Val Loss: 1.5246, Best Val Loss: 1.5246\n",
      "Epoch 69/1000, Train Loss: 1.4549, Val Loss: 1.5021, Best Val Loss: 1.5021\n",
      "Epoch 78/1000, Train Loss: 1.4278, Val Loss: 1.4930, Best Val Loss: 1.4930\n",
      "Epoch 81/1000, Train Loss: 1.4290, Val Loss: 1.4864, Best Val Loss: 1.4864\n",
      "Epoch 105/1000, Train Loss: 1.3854, Val Loss: 1.4806, Best Val Loss: 1.4806\n",
      "Early stopping at epoch 155, Best Val Loss: 1.4806\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 167.5436, Val Loss: 5.4986, Best Val Loss: 5.4986\n",
      "Epoch 2/1000, Train Loss: 5.4529, Val Loss: 5.4104, Best Val Loss: 5.4104\n",
      "Epoch 3/1000, Train Loss: 5.3757, Val Loss: 5.3390, Best Val Loss: 5.3390\n",
      "Epoch 4/1000, Train Loss: 5.2857, Val Loss: 5.2307, Best Val Loss: 5.2307\n",
      "Epoch 5/1000, Train Loss: 5.1812, Val Loss: 5.1224, Best Val Loss: 5.1224\n",
      "Epoch 6/1000, Train Loss: 4.9038, Val Loss: 4.1974, Best Val Loss: 4.1974\n",
      "Epoch 7/1000, Train Loss: 2.6824, Val Loss: 2.2995, Best Val Loss: 2.2995\n",
      "Epoch 8/1000, Train Loss: 2.1589, Val Loss: 2.0628, Best Val Loss: 2.0628\n",
      "Epoch 9/1000, Train Loss: 1.9994, Val Loss: 1.9471, Best Val Loss: 1.9471\n",
      "Epoch 10/1000, Train Loss: 1.9091, Val Loss: 1.8776, Best Val Loss: 1.8776\n",
      "Epoch 11/1000, Train Loss: 1.8587, Val Loss: 1.8402, Best Val Loss: 1.8402\n",
      "Epoch 12/1000, Train Loss: 1.8158, Val Loss: 1.8040, Best Val Loss: 1.8040\n",
      "Epoch 13/1000, Train Loss: 1.7863, Val Loss: 1.7953, Best Val Loss: 1.7953\n",
      "Epoch 14/1000, Train Loss: 1.7677, Val Loss: 1.7622, Best Val Loss: 1.7622\n",
      "Epoch 16/1000, Train Loss: 1.7301, Val Loss: 1.7131, Best Val Loss: 1.7131\n",
      "Epoch 17/1000, Train Loss: 1.7175, Val Loss: 1.6990, Best Val Loss: 1.6990\n",
      "Epoch 18/1000, Train Loss: 1.6974, Val Loss: 1.6908, Best Val Loss: 1.6908\n",
      "Epoch 22/1000, Train Loss: 1.6570, Val Loss: 1.6682, Best Val Loss: 1.6682\n",
      "Epoch 25/1000, Train Loss: 1.6392, Val Loss: 1.6650, Best Val Loss: 1.6650\n",
      "Epoch 27/1000, Train Loss: 1.6190, Val Loss: 1.6167, Best Val Loss: 1.6167\n",
      "Epoch 28/1000, Train Loss: 1.6105, Val Loss: 1.6121, Best Val Loss: 1.6121\n",
      "Epoch 32/1000, Train Loss: 1.5814, Val Loss: 1.6037, Best Val Loss: 1.6037\n",
      "Epoch 35/1000, Train Loss: 1.5677, Val Loss: 1.5781, Best Val Loss: 1.5781\n",
      "Epoch 40/1000, Train Loss: 1.5400, Val Loss: 1.5489, Best Val Loss: 1.5489\n",
      "Epoch 47/1000, Train Loss: 1.5083, Val Loss: 1.5387, Best Val Loss: 1.5387\n",
      "Epoch 56/1000, Train Loss: 1.4677, Val Loss: 1.5384, Best Val Loss: 1.5384\n",
      "Epoch 59/1000, Train Loss: 1.4628, Val Loss: 1.5183, Best Val Loss: 1.5183\n",
      "Epoch 61/1000, Train Loss: 1.4600, Val Loss: 1.5114, Best Val Loss: 1.5114\n",
      "Epoch 62/1000, Train Loss: 1.4510, Val Loss: 1.5079, Best Val Loss: 1.5079\n",
      "Epoch 65/1000, Train Loss: 1.4486, Val Loss: 1.4982, Best Val Loss: 1.4982\n",
      "Epoch 72/1000, Train Loss: 1.4291, Val Loss: 1.4972, Best Val Loss: 1.4972\n",
      "Epoch 79/1000, Train Loss: 1.4124, Val Loss: 1.4864, Best Val Loss: 1.4864\n",
      "Epoch 83/1000, Train Loss: 1.4038, Val Loss: 1.4767, Best Val Loss: 1.4767\n",
      "Epoch 95/1000, Train Loss: 1.3823, Val Loss: 1.4753, Best Val Loss: 1.4753\n",
      "Epoch 98/1000, Train Loss: 1.3835, Val Loss: 1.4738, Best Val Loss: 1.4738\n",
      "Epoch 105/1000, Train Loss: 1.3656, Val Loss: 1.4707, Best Val Loss: 1.4707\n",
      "Epoch 116/1000, Train Loss: 1.3420, Val Loss: 1.4698, Best Val Loss: 1.4698\n",
      "Epoch 123/1000, Train Loss: 1.3402, Val Loss: 1.4688, Best Val Loss: 1.4688\n",
      "Epoch 133/1000, Train Loss: 1.3297, Val Loss: 1.4583, Best Val Loss: 1.4583\n",
      "Early stopping at epoch 183, Best Val Loss: 1.4583\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 169.0705, Val Loss: 5.5546, Best Val Loss: 5.5546\n",
      "Epoch 2/1000, Train Loss: 5.5006, Val Loss: 5.4453, Best Val Loss: 5.4453\n",
      "Epoch 3/1000, Train Loss: 5.4082, Val Loss: 5.3640, Best Val Loss: 5.3640\n",
      "Epoch 4/1000, Train Loss: 5.3250, Val Loss: 5.2801, Best Val Loss: 5.2801\n",
      "Epoch 5/1000, Train Loss: 5.2314, Val Loss: 5.1742, Best Val Loss: 5.1742\n",
      "Epoch 6/1000, Train Loss: 5.0673, Val Loss: 4.8800, Best Val Loss: 4.8800\n",
      "Epoch 7/1000, Train Loss: 3.4578, Val Loss: 2.3928, Best Val Loss: 2.3928\n",
      "Epoch 8/1000, Train Loss: 2.2556, Val Loss: 2.1637, Best Val Loss: 2.1637\n",
      "Epoch 9/1000, Train Loss: 2.0756, Val Loss: 2.0178, Best Val Loss: 2.0178\n",
      "Epoch 10/1000, Train Loss: 1.9669, Val Loss: 1.9366, Best Val Loss: 1.9366\n",
      "Epoch 11/1000, Train Loss: 1.8946, Val Loss: 1.8698, Best Val Loss: 1.8698\n",
      "Epoch 12/1000, Train Loss: 1.8469, Val Loss: 1.8311, Best Val Loss: 1.8311\n",
      "Epoch 13/1000, Train Loss: 1.8060, Val Loss: 1.7983, Best Val Loss: 1.7983\n",
      "Epoch 14/1000, Train Loss: 1.7748, Val Loss: 1.7784, Best Val Loss: 1.7784\n",
      "Epoch 15/1000, Train Loss: 1.7587, Val Loss: 1.7739, Best Val Loss: 1.7739\n",
      "Epoch 16/1000, Train Loss: 1.7443, Val Loss: 1.7409, Best Val Loss: 1.7409\n",
      "Epoch 19/1000, Train Loss: 1.6990, Val Loss: 1.7237, Best Val Loss: 1.7237\n",
      "Epoch 20/1000, Train Loss: 1.6843, Val Loss: 1.7117, Best Val Loss: 1.7117\n",
      "Epoch 21/1000, Train Loss: 1.6832, Val Loss: 1.6820, Best Val Loss: 1.6820\n",
      "Epoch 23/1000, Train Loss: 1.6733, Val Loss: 1.6803, Best Val Loss: 1.6803\n",
      "Epoch 24/1000, Train Loss: 1.6700, Val Loss: 1.6762, Best Val Loss: 1.6762\n",
      "Epoch 25/1000, Train Loss: 1.6522, Val Loss: 1.6652, Best Val Loss: 1.6652\n",
      "Epoch 26/1000, Train Loss: 1.6517, Val Loss: 1.6438, Best Val Loss: 1.6438\n",
      "Epoch 27/1000, Train Loss: 1.6360, Val Loss: 1.6399, Best Val Loss: 1.6399\n",
      "Epoch 28/1000, Train Loss: 1.6304, Val Loss: 1.6077, Best Val Loss: 1.6077\n",
      "Epoch 36/1000, Train Loss: 1.5800, Val Loss: 1.5984, Best Val Loss: 1.5984\n",
      "Epoch 38/1000, Train Loss: 1.5819, Val Loss: 1.5752, Best Val Loss: 1.5752\n",
      "Epoch 42/1000, Train Loss: 1.5549, Val Loss: 1.5524, Best Val Loss: 1.5524\n",
      "Epoch 50/1000, Train Loss: 1.5174, Val Loss: 1.5281, Best Val Loss: 1.5281\n",
      "Epoch 51/1000, Train Loss: 1.5177, Val Loss: 1.5245, Best Val Loss: 1.5245\n",
      "Epoch 52/1000, Train Loss: 1.5102, Val Loss: 1.5165, Best Val Loss: 1.5165\n",
      "Epoch 67/1000, Train Loss: 1.4789, Val Loss: 1.5017, Best Val Loss: 1.5017\n",
      "Epoch 72/1000, Train Loss: 1.4562, Val Loss: 1.5012, Best Val Loss: 1.5012\n",
      "Epoch 76/1000, Train Loss: 1.4478, Val Loss: 1.4882, Best Val Loss: 1.4882\n",
      "Epoch 84/1000, Train Loss: 1.4272, Val Loss: 1.4774, Best Val Loss: 1.4774\n",
      "Epoch 94/1000, Train Loss: 1.4144, Val Loss: 1.4697, Best Val Loss: 1.4697\n",
      "Epoch 100/1000, Train Loss: 1.4025, Val Loss: 1.4685, Best Val Loss: 1.4685\n",
      "Epoch 128/1000, Train Loss: 1.3646, Val Loss: 1.4596, Best Val Loss: 1.4596\n",
      "Epoch 155/1000, Train Loss: 1.3419, Val Loss: 1.4549, Best Val Loss: 1.4549\n",
      "Epoch 157/1000, Train Loss: 1.3287, Val Loss: 1.4482, Best Val Loss: 1.4482\n",
      "Early stopping at epoch 207, Best Val Loss: 1.4482\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 137.9138, Val Loss: 5.3369, Best Val Loss: 5.3369\n",
      "Epoch 2/1000, Train Loss: 5.2919, Val Loss: 5.2575, Best Val Loss: 5.2575\n",
      "Epoch 3/1000, Train Loss: 5.2309, Val Loss: 5.2076, Best Val Loss: 5.2076\n",
      "Epoch 4/1000, Train Loss: 5.1676, Val Loss: 5.0853, Best Val Loss: 5.0853\n",
      "Epoch 5/1000, Train Loss: 3.7218, Val Loss: 2.7635, Best Val Loss: 2.7635\n",
      "Epoch 6/1000, Train Loss: 2.5735, Val Loss: 2.4113, Best Val Loss: 2.4113\n",
      "Epoch 7/1000, Train Loss: 2.2962, Val Loss: 2.1906, Best Val Loss: 2.1906\n",
      "Epoch 8/1000, Train Loss: 2.1175, Val Loss: 2.0537, Best Val Loss: 2.0537\n",
      "Epoch 9/1000, Train Loss: 2.0026, Val Loss: 1.9793, Best Val Loss: 1.9793\n",
      "Epoch 10/1000, Train Loss: 1.9314, Val Loss: 1.9160, Best Val Loss: 1.9160\n",
      "Epoch 11/1000, Train Loss: 1.8730, Val Loss: 1.8558, Best Val Loss: 1.8558\n",
      "Epoch 12/1000, Train Loss: 1.8299, Val Loss: 1.8287, Best Val Loss: 1.8287\n",
      "Epoch 13/1000, Train Loss: 1.8056, Val Loss: 1.8052, Best Val Loss: 1.8052\n",
      "Epoch 14/1000, Train Loss: 1.7735, Val Loss: 1.7907, Best Val Loss: 1.7907\n",
      "Epoch 15/1000, Train Loss: 1.7659, Val Loss: 1.7506, Best Val Loss: 1.7506\n",
      "Epoch 17/1000, Train Loss: 1.7261, Val Loss: 1.7351, Best Val Loss: 1.7351\n",
      "Epoch 18/1000, Train Loss: 1.7094, Val Loss: 1.7221, Best Val Loss: 1.7221\n",
      "Epoch 20/1000, Train Loss: 1.7008, Val Loss: 1.6895, Best Val Loss: 1.6895\n",
      "Epoch 21/1000, Train Loss: 1.6809, Val Loss: 1.6761, Best Val Loss: 1.6761\n",
      "Epoch 28/1000, Train Loss: 1.6329, Val Loss: 1.6535, Best Val Loss: 1.6535\n",
      "Epoch 29/1000, Train Loss: 1.6239, Val Loss: 1.6176, Best Val Loss: 1.6176\n",
      "Epoch 33/1000, Train Loss: 1.6099, Val Loss: 1.6069, Best Val Loss: 1.6069\n",
      "Epoch 34/1000, Train Loss: 1.5968, Val Loss: 1.5988, Best Val Loss: 1.5988\n",
      "Epoch 42/1000, Train Loss: 1.5598, Val Loss: 1.5967, Best Val Loss: 1.5967\n",
      "Epoch 44/1000, Train Loss: 1.5526, Val Loss: 1.5807, Best Val Loss: 1.5807\n",
      "Epoch 45/1000, Train Loss: 1.5523, Val Loss: 1.5695, Best Val Loss: 1.5695\n",
      "Epoch 48/1000, Train Loss: 1.5264, Val Loss: 1.5562, Best Val Loss: 1.5562\n",
      "Epoch 49/1000, Train Loss: 1.5164, Val Loss: 1.5454, Best Val Loss: 1.5454\n",
      "Epoch 55/1000, Train Loss: 1.5055, Val Loss: 1.5332, Best Val Loss: 1.5332\n",
      "Epoch 58/1000, Train Loss: 1.4919, Val Loss: 1.5269, Best Val Loss: 1.5269\n",
      "Epoch 69/1000, Train Loss: 1.4650, Val Loss: 1.4896, Best Val Loss: 1.4896\n",
      "Epoch 83/1000, Train Loss: 1.4329, Val Loss: 1.4792, Best Val Loss: 1.4792\n",
      "Epoch 90/1000, Train Loss: 1.4413, Val Loss: 1.4739, Best Val Loss: 1.4739\n",
      "Epoch 117/1000, Train Loss: 1.3764, Val Loss: 1.4727, Best Val Loss: 1.4727\n",
      "Epoch 154/1000, Train Loss: 1.3350, Val Loss: 1.4698, Best Val Loss: 1.4698\n",
      "Epoch 166/1000, Train Loss: 1.3299, Val Loss: 1.4684, Best Val Loss: 1.4684\n",
      "Early stopping at epoch 216, Best Val Loss: 1.4684\n",
      "Epistemic Variance: 0.838448\n",
      "Aleatoric Variance: 7.394613\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n",
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           1.966\n",
      "  RMSE          2.765\n",
      "  MDAE          1.457\n",
      "  MARPD         1.717\n",
      "  R2            0.866\n",
      "  Correlation   0.931\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.032\n",
      "  Mean-absolute Calibration Error       0.028\n",
      "  Miscalibration Area                   0.028\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.041\n",
      "     Group Size: 0.56 -- Calibration Error: 0.032\n",
      "     Group Size: 1.00 -- Calibration Error: 0.028\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.044\n",
      "     Group Size: 0.56 -- Calibration Error: 0.036\n",
      "     Group Size: 1.00 -- Calibration Error: 0.032\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   2.869\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   2.277\n",
      "  CRPS                      1.408\n",
      "  Check Score               0.711\n",
      "  Interval Score            7.054\n",
      "{'accuracy': {'mae': 1.9656434433546346, 'rmse': np.float64(2.7646984118294737), 'mdae': 1.4569797241210978, 'marpd': np.float64(1.7171301826578853), 'r2': 0.8661403454264198, 'corr': np.float64(0.9307628716902713)}, 'avg_calibration': {'rms_cal': np.float64(0.03211520848659491), 'ma_cal': np.float64(0.02773334615014896), 'miscal_area': np.float64(0.028009634176597128)}, 'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.3685303 , 0.04147754, 0.03369977, 0.03286661, 0.03272452,\n",
      "       0.03196072, 0.0307782 , 0.02949306, 0.02939045, 0.02773335]), 'adv_group_cali_stderr': array([0.0511452 , 0.00443823, 0.00400501, 0.00195548, 0.00182905,\n",
      "       0.00105899, 0.00092574, 0.00066818, 0.00041146, 0.        ])}, 'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.46302495, 0.04412527, 0.04081114, 0.03959221, 0.03822595,\n",
      "       0.0360851 , 0.0354079 , 0.03454436, 0.03357455, 0.03211521]), 'adv_group_cali_stderr': array([6.70526040e-02, 5.68014633e-03, 2.93480848e-03, 2.70676658e-03,\n",
      "       1.61982606e-03, 1.59419971e-03, 1.14100233e-03, 1.22097107e-03,\n",
      "       6.18357709e-04, 7.31423639e-18])}}, 'sharpness': {'sharp': np.float32(2.8693314)}, 'scoring_rule': {'nll': np.float64(2.277140061346625), 'crps': np.float64(1.4076962209774568), 'check': np.float64(0.7107720761913753), 'interval': np.float64(7.054087467163622)}}\n",
      "coverage: 0.9519369412669423, MPIW: 10.162967980396473\n",
      "Run 2 with seed 123\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 156.0092, Val Loss: 5.5488, Best Val Loss: 5.5488\n",
      "Epoch 2/1000, Train Loss: 5.4987, Val Loss: 5.4531, Best Val Loss: 5.4531\n",
      "Epoch 3/1000, Train Loss: 5.4138, Val Loss: 5.3746, Best Val Loss: 5.3746\n",
      "Epoch 4/1000, Train Loss: 5.3371, Val Loss: 5.2990, Best Val Loss: 5.2990\n",
      "Epoch 5/1000, Train Loss: 5.2558, Val Loss: 5.2038, Best Val Loss: 5.2038\n",
      "Epoch 6/1000, Train Loss: 5.0973, Val Loss: 4.9031, Best Val Loss: 4.9031\n",
      "Epoch 7/1000, Train Loss: 3.3912, Val Loss: 2.3764, Best Val Loss: 2.3764\n",
      "Epoch 8/1000, Train Loss: 2.2252, Val Loss: 2.1127, Best Val Loss: 2.1127\n",
      "Epoch 9/1000, Train Loss: 2.0256, Val Loss: 1.9596, Best Val Loss: 1.9596\n",
      "Epoch 10/1000, Train Loss: 1.9231, Val Loss: 1.8958, Best Val Loss: 1.8958\n",
      "Epoch 11/1000, Train Loss: 1.8593, Val Loss: 1.8351, Best Val Loss: 1.8351\n",
      "Epoch 12/1000, Train Loss: 1.8208, Val Loss: 1.8073, Best Val Loss: 1.8073\n",
      "Epoch 13/1000, Train Loss: 1.7874, Val Loss: 1.7906, Best Val Loss: 1.7906\n",
      "Epoch 14/1000, Train Loss: 1.7611, Val Loss: 1.7556, Best Val Loss: 1.7556\n",
      "Epoch 15/1000, Train Loss: 1.7491, Val Loss: 1.7326, Best Val Loss: 1.7326\n",
      "Epoch 17/1000, Train Loss: 1.7244, Val Loss: 1.7269, Best Val Loss: 1.7269\n",
      "Epoch 19/1000, Train Loss: 1.7037, Val Loss: 1.7209, Best Val Loss: 1.7209\n",
      "Epoch 20/1000, Train Loss: 1.6982, Val Loss: 1.6911, Best Val Loss: 1.6911\n",
      "Epoch 21/1000, Train Loss: 1.6896, Val Loss: 1.6830, Best Val Loss: 1.6830\n",
      "Epoch 24/1000, Train Loss: 1.6781, Val Loss: 1.6803, Best Val Loss: 1.6803\n",
      "Epoch 27/1000, Train Loss: 1.6467, Val Loss: 1.6417, Best Val Loss: 1.6417\n",
      "Epoch 30/1000, Train Loss: 1.6332, Val Loss: 1.6371, Best Val Loss: 1.6371\n",
      "Epoch 32/1000, Train Loss: 1.6167, Val Loss: 1.6172, Best Val Loss: 1.6172\n",
      "Epoch 35/1000, Train Loss: 1.5987, Val Loss: 1.6105, Best Val Loss: 1.6105\n",
      "Epoch 38/1000, Train Loss: 1.5764, Val Loss: 1.5967, Best Val Loss: 1.5967\n",
      "Epoch 41/1000, Train Loss: 1.5677, Val Loss: 1.5800, Best Val Loss: 1.5800\n",
      "Epoch 46/1000, Train Loss: 1.5347, Val Loss: 1.5590, Best Val Loss: 1.5590\n",
      "Epoch 49/1000, Train Loss: 1.5215, Val Loss: 1.5474, Best Val Loss: 1.5474\n",
      "Epoch 50/1000, Train Loss: 1.5272, Val Loss: 1.5458, Best Val Loss: 1.5458\n",
      "Epoch 51/1000, Train Loss: 1.5248, Val Loss: 1.5446, Best Val Loss: 1.5446\n",
      "Epoch 53/1000, Train Loss: 1.5123, Val Loss: 1.5375, Best Val Loss: 1.5375\n",
      "Epoch 57/1000, Train Loss: 1.5074, Val Loss: 1.5292, Best Val Loss: 1.5292\n",
      "Epoch 63/1000, Train Loss: 1.4830, Val Loss: 1.5261, Best Val Loss: 1.5261\n",
      "Epoch 66/1000, Train Loss: 1.4692, Val Loss: 1.5109, Best Val Loss: 1.5109\n",
      "Epoch 81/1000, Train Loss: 1.4397, Val Loss: 1.5041, Best Val Loss: 1.5041\n",
      "Epoch 84/1000, Train Loss: 1.4387, Val Loss: 1.5039, Best Val Loss: 1.5039\n",
      "Epoch 86/1000, Train Loss: 1.4291, Val Loss: 1.4929, Best Val Loss: 1.4929\n",
      "Epoch 89/1000, Train Loss: 1.4283, Val Loss: 1.4882, Best Val Loss: 1.4882\n",
      "Epoch 90/1000, Train Loss: 1.4256, Val Loss: 1.4859, Best Val Loss: 1.4859\n",
      "Epoch 91/1000, Train Loss: 1.4237, Val Loss: 1.4825, Best Val Loss: 1.4825\n",
      "Epoch 106/1000, Train Loss: 1.4072, Val Loss: 1.4676, Best Val Loss: 1.4676\n",
      "Epoch 119/1000, Train Loss: 1.3870, Val Loss: 1.4645, Best Val Loss: 1.4645\n",
      "Epoch 156/1000, Train Loss: 1.3546, Val Loss: 1.4576, Best Val Loss: 1.4576\n",
      "Early stopping at epoch 206, Best Val Loss: 1.4576\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 124.3247, Val Loss: 5.5396, Best Val Loss: 5.5396\n",
      "Epoch 2/1000, Train Loss: 5.4829, Val Loss: 5.4153, Best Val Loss: 5.4153\n",
      "Epoch 3/1000, Train Loss: 5.3672, Val Loss: 5.2928, Best Val Loss: 5.2928\n",
      "Epoch 4/1000, Train Loss: 5.1673, Val Loss: 4.9429, Best Val Loss: 4.9429\n",
      "Epoch 5/1000, Train Loss: 3.4837, Val Loss: 2.4825, Best Val Loss: 2.4825\n",
      "Epoch 6/1000, Train Loss: 2.3411, Val Loss: 2.2394, Best Val Loss: 2.2394\n",
      "Epoch 7/1000, Train Loss: 2.1342, Val Loss: 2.0633, Best Val Loss: 2.0633\n",
      "Epoch 8/1000, Train Loss: 1.9964, Val Loss: 1.9681, Best Val Loss: 1.9681\n",
      "Epoch 9/1000, Train Loss: 1.9062, Val Loss: 1.8739, Best Val Loss: 1.8739\n",
      "Epoch 10/1000, Train Loss: 1.8477, Val Loss: 1.8109, Best Val Loss: 1.8109\n",
      "Epoch 12/1000, Train Loss: 1.7802, Val Loss: 1.7506, Best Val Loss: 1.7506\n",
      "Epoch 16/1000, Train Loss: 1.7148, Val Loss: 1.7083, Best Val Loss: 1.7083\n",
      "Epoch 18/1000, Train Loss: 1.7025, Val Loss: 1.6649, Best Val Loss: 1.6649\n",
      "Epoch 21/1000, Train Loss: 1.6656, Val Loss: 1.6465, Best Val Loss: 1.6465\n",
      "Epoch 25/1000, Train Loss: 1.6477, Val Loss: 1.6406, Best Val Loss: 1.6406\n",
      "Epoch 29/1000, Train Loss: 1.6177, Val Loss: 1.6047, Best Val Loss: 1.6047\n",
      "Epoch 35/1000, Train Loss: 1.5874, Val Loss: 1.5982, Best Val Loss: 1.5982\n",
      "Epoch 37/1000, Train Loss: 1.5699, Val Loss: 1.5853, Best Val Loss: 1.5853\n",
      "Epoch 38/1000, Train Loss: 1.5561, Val Loss: 1.5567, Best Val Loss: 1.5567\n",
      "Epoch 44/1000, Train Loss: 1.5399, Val Loss: 1.5555, Best Val Loss: 1.5555\n",
      "Epoch 48/1000, Train Loss: 1.5120, Val Loss: 1.5540, Best Val Loss: 1.5540\n",
      "Epoch 49/1000, Train Loss: 1.5270, Val Loss: 1.5393, Best Val Loss: 1.5393\n",
      "Epoch 53/1000, Train Loss: 1.5017, Val Loss: 1.5374, Best Val Loss: 1.5374\n",
      "Epoch 59/1000, Train Loss: 1.4848, Val Loss: 1.5115, Best Val Loss: 1.5115\n",
      "Epoch 65/1000, Train Loss: 1.4720, Val Loss: 1.5035, Best Val Loss: 1.5035\n",
      "Epoch 74/1000, Train Loss: 1.4481, Val Loss: 1.4903, Best Val Loss: 1.4903\n",
      "Epoch 96/1000, Train Loss: 1.4065, Val Loss: 1.4878, Best Val Loss: 1.4878\n",
      "Epoch 98/1000, Train Loss: 1.3975, Val Loss: 1.4857, Best Val Loss: 1.4857\n",
      "Epoch 101/1000, Train Loss: 1.4062, Val Loss: 1.4732, Best Val Loss: 1.4732\n",
      "Epoch 119/1000, Train Loss: 1.3738, Val Loss: 1.4731, Best Val Loss: 1.4731\n",
      "Epoch 125/1000, Train Loss: 1.3687, Val Loss: 1.4665, Best Val Loss: 1.4665\n",
      "Epoch 165/1000, Train Loss: 1.3155, Val Loss: 1.4620, Best Val Loss: 1.4620\n",
      "Early stopping at epoch 215, Best Val Loss: 1.4620\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 135.6368, Val Loss: 5.5211, Best Val Loss: 5.5211\n",
      "Epoch 2/1000, Train Loss: 5.4772, Val Loss: 5.4293, Best Val Loss: 5.4293\n",
      "Epoch 3/1000, Train Loss: 5.3927, Val Loss: 5.3507, Best Val Loss: 5.3507\n",
      "Epoch 4/1000, Train Loss: 5.3132, Val Loss: 5.2742, Best Val Loss: 5.2742\n",
      "Epoch 5/1000, Train Loss: 5.2374, Val Loss: 5.2016, Best Val Loss: 5.2016\n",
      "Epoch 6/1000, Train Loss: 5.1494, Val Loss: 5.0771, Best Val Loss: 5.0771\n",
      "Epoch 7/1000, Train Loss: 4.1774, Val Loss: 2.4207, Best Val Loss: 2.4207\n",
      "Epoch 8/1000, Train Loss: 2.2352, Val Loss: 2.1472, Best Val Loss: 2.1472\n",
      "Epoch 9/1000, Train Loss: 2.0514, Val Loss: 2.0260, Best Val Loss: 2.0260\n",
      "Epoch 10/1000, Train Loss: 1.9465, Val Loss: 1.9300, Best Val Loss: 1.9300\n",
      "Epoch 11/1000, Train Loss: 1.8776, Val Loss: 1.8667, Best Val Loss: 1.8667\n",
      "Epoch 12/1000, Train Loss: 1.8312, Val Loss: 1.7993, Best Val Loss: 1.7993\n",
      "Epoch 13/1000, Train Loss: 1.7948, Val Loss: 1.7817, Best Val Loss: 1.7817\n",
      "Epoch 14/1000, Train Loss: 1.7670, Val Loss: 1.7674, Best Val Loss: 1.7674\n",
      "Epoch 15/1000, Train Loss: 1.7469, Val Loss: 1.7647, Best Val Loss: 1.7647\n",
      "Epoch 16/1000, Train Loss: 1.7299, Val Loss: 1.7525, Best Val Loss: 1.7525\n",
      "Epoch 17/1000, Train Loss: 1.7121, Val Loss: 1.7171, Best Val Loss: 1.7171\n",
      "Epoch 18/1000, Train Loss: 1.7042, Val Loss: 1.7028, Best Val Loss: 1.7028\n",
      "Epoch 21/1000, Train Loss: 1.6672, Val Loss: 1.7004, Best Val Loss: 1.7004\n",
      "Epoch 22/1000, Train Loss: 1.6639, Val Loss: 1.6759, Best Val Loss: 1.6759\n",
      "Epoch 23/1000, Train Loss: 1.6518, Val Loss: 1.6626, Best Val Loss: 1.6626\n",
      "Epoch 26/1000, Train Loss: 1.6260, Val Loss: 1.6229, Best Val Loss: 1.6229\n",
      "Epoch 30/1000, Train Loss: 1.5983, Val Loss: 1.6172, Best Val Loss: 1.6172\n",
      "Epoch 31/1000, Train Loss: 1.5894, Val Loss: 1.6040, Best Val Loss: 1.6040\n",
      "Epoch 33/1000, Train Loss: 1.5695, Val Loss: 1.5901, Best Val Loss: 1.5901\n",
      "Epoch 34/1000, Train Loss: 1.5655, Val Loss: 1.5893, Best Val Loss: 1.5893\n",
      "Epoch 35/1000, Train Loss: 1.5568, Val Loss: 1.5851, Best Val Loss: 1.5851\n",
      "Epoch 37/1000, Train Loss: 1.5510, Val Loss: 1.5583, Best Val Loss: 1.5583\n",
      "Epoch 41/1000, Train Loss: 1.5351, Val Loss: 1.5532, Best Val Loss: 1.5532\n",
      "Epoch 42/1000, Train Loss: 1.5150, Val Loss: 1.5470, Best Val Loss: 1.5470\n",
      "Epoch 43/1000, Train Loss: 1.5145, Val Loss: 1.5241, Best Val Loss: 1.5241\n",
      "Epoch 54/1000, Train Loss: 1.4649, Val Loss: 1.5033, Best Val Loss: 1.5033\n",
      "Epoch 57/1000, Train Loss: 1.4549, Val Loss: 1.4889, Best Val Loss: 1.4889\n",
      "Epoch 75/1000, Train Loss: 1.4179, Val Loss: 1.4686, Best Val Loss: 1.4686\n",
      "Early stopping at epoch 125, Best Val Loss: 1.4686\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 144.0583, Val Loss: 5.4603, Best Val Loss: 5.4603\n",
      "Epoch 2/1000, Train Loss: 5.3986, Val Loss: 5.3502, Best Val Loss: 5.3502\n",
      "Epoch 3/1000, Train Loss: 5.3189, Val Loss: 5.2849, Best Val Loss: 5.2849\n",
      "Epoch 4/1000, Train Loss: 5.2589, Val Loss: 5.2311, Best Val Loss: 5.2311\n",
      "Epoch 5/1000, Train Loss: 5.2054, Val Loss: 5.1768, Best Val Loss: 5.1768\n",
      "Epoch 6/1000, Train Loss: 5.1286, Val Loss: 5.0374, Best Val Loss: 5.0374\n",
      "Epoch 7/1000, Train Loss: 3.7724, Val Loss: 2.3232, Best Val Loss: 2.3232\n",
      "Epoch 8/1000, Train Loss: 2.1438, Val Loss: 2.0287, Best Val Loss: 2.0287\n",
      "Epoch 9/1000, Train Loss: 1.9592, Val Loss: 1.9275, Best Val Loss: 1.9275\n",
      "Epoch 10/1000, Train Loss: 1.8767, Val Loss: 1.8632, Best Val Loss: 1.8632\n",
      "Epoch 11/1000, Train Loss: 1.8211, Val Loss: 1.8007, Best Val Loss: 1.8007\n",
      "Epoch 12/1000, Train Loss: 1.8001, Val Loss: 1.7865, Best Val Loss: 1.7865\n",
      "Epoch 13/1000, Train Loss: 1.7544, Val Loss: 1.7453, Best Val Loss: 1.7453\n",
      "Epoch 14/1000, Train Loss: 1.7452, Val Loss: 1.7218, Best Val Loss: 1.7218\n",
      "Epoch 16/1000, Train Loss: 1.7163, Val Loss: 1.7015, Best Val Loss: 1.7015\n",
      "Epoch 17/1000, Train Loss: 1.6968, Val Loss: 1.6765, Best Val Loss: 1.6765\n",
      "Epoch 21/1000, Train Loss: 1.6674, Val Loss: 1.6650, Best Val Loss: 1.6650\n",
      "Epoch 22/1000, Train Loss: 1.6612, Val Loss: 1.6418, Best Val Loss: 1.6418\n",
      "Epoch 26/1000, Train Loss: 1.6170, Val Loss: 1.6254, Best Val Loss: 1.6254\n",
      "Epoch 28/1000, Train Loss: 1.6009, Val Loss: 1.6093, Best Val Loss: 1.6093\n",
      "Epoch 32/1000, Train Loss: 1.5836, Val Loss: 1.6021, Best Val Loss: 1.6021\n",
      "Epoch 35/1000, Train Loss: 1.5569, Val Loss: 1.5814, Best Val Loss: 1.5814\n",
      "Epoch 36/1000, Train Loss: 1.5477, Val Loss: 1.5792, Best Val Loss: 1.5792\n",
      "Epoch 37/1000, Train Loss: 1.5432, Val Loss: 1.5630, Best Val Loss: 1.5630\n",
      "Epoch 38/1000, Train Loss: 1.5394, Val Loss: 1.5599, Best Val Loss: 1.5599\n",
      "Epoch 39/1000, Train Loss: 1.5366, Val Loss: 1.5543, Best Val Loss: 1.5543\n",
      "Epoch 44/1000, Train Loss: 1.5070, Val Loss: 1.5501, Best Val Loss: 1.5501\n",
      "Epoch 47/1000, Train Loss: 1.5043, Val Loss: 1.5463, Best Val Loss: 1.5463\n",
      "Epoch 51/1000, Train Loss: 1.4815, Val Loss: 1.5368, Best Val Loss: 1.5368\n",
      "Epoch 54/1000, Train Loss: 1.4691, Val Loss: 1.5294, Best Val Loss: 1.5294\n",
      "Epoch 57/1000, Train Loss: 1.4639, Val Loss: 1.5059, Best Val Loss: 1.5059\n",
      "Epoch 61/1000, Train Loss: 1.4492, Val Loss: 1.5028, Best Val Loss: 1.5028\n",
      "Epoch 64/1000, Train Loss: 1.4404, Val Loss: 1.4761, Best Val Loss: 1.4761\n",
      "Early stopping at epoch 114, Best Val Loss: 1.4761\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 137.0009, Val Loss: 5.4440, Best Val Loss: 5.4440\n",
      "Epoch 2/1000, Train Loss: 5.3930, Val Loss: 5.3362, Best Val Loss: 5.3362\n",
      "Epoch 3/1000, Train Loss: 5.2828, Val Loss: 5.2041, Best Val Loss: 5.2041\n",
      "Epoch 4/1000, Train Loss: 5.0625, Val Loss: 4.8181, Best Val Loss: 4.8181\n",
      "Epoch 5/1000, Train Loss: 3.4332, Val Loss: 2.3831, Best Val Loss: 2.3831\n",
      "Epoch 6/1000, Train Loss: 2.2302, Val Loss: 2.1208, Best Val Loss: 2.1208\n",
      "Epoch 7/1000, Train Loss: 2.0411, Val Loss: 1.9834, Best Val Loss: 1.9834\n",
      "Epoch 8/1000, Train Loss: 1.9349, Val Loss: 1.9100, Best Val Loss: 1.9100\n",
      "Epoch 9/1000, Train Loss: 1.8741, Val Loss: 1.8404, Best Val Loss: 1.8404\n",
      "Epoch 11/1000, Train Loss: 1.8043, Val Loss: 1.7893, Best Val Loss: 1.7893\n",
      "Epoch 12/1000, Train Loss: 1.7915, Val Loss: 1.7598, Best Val Loss: 1.7598\n",
      "Epoch 14/1000, Train Loss: 1.7391, Val Loss: 1.7489, Best Val Loss: 1.7489\n",
      "Epoch 15/1000, Train Loss: 1.7355, Val Loss: 1.7308, Best Val Loss: 1.7308\n",
      "Epoch 16/1000, Train Loss: 1.7162, Val Loss: 1.7072, Best Val Loss: 1.7072\n",
      "Epoch 20/1000, Train Loss: 1.6841, Val Loss: 1.6503, Best Val Loss: 1.6503\n",
      "Epoch 29/1000, Train Loss: 1.6299, Val Loss: 1.6424, Best Val Loss: 1.6424\n",
      "Epoch 31/1000, Train Loss: 1.6089, Val Loss: 1.6174, Best Val Loss: 1.6174\n",
      "Epoch 34/1000, Train Loss: 1.5958, Val Loss: 1.5969, Best Val Loss: 1.5969\n",
      "Epoch 37/1000, Train Loss: 1.5790, Val Loss: 1.5845, Best Val Loss: 1.5845\n",
      "Epoch 40/1000, Train Loss: 1.5505, Val Loss: 1.5637, Best Val Loss: 1.5637\n",
      "Epoch 45/1000, Train Loss: 1.5277, Val Loss: 1.5411, Best Val Loss: 1.5411\n",
      "Epoch 47/1000, Train Loss: 1.5256, Val Loss: 1.5407, Best Val Loss: 1.5407\n",
      "Epoch 54/1000, Train Loss: 1.4968, Val Loss: 1.5280, Best Val Loss: 1.5280\n",
      "Epoch 57/1000, Train Loss: 1.4899, Val Loss: 1.5274, Best Val Loss: 1.5274\n",
      "Epoch 62/1000, Train Loss: 1.4730, Val Loss: 1.5138, Best Val Loss: 1.5138\n",
      "Epoch 71/1000, Train Loss: 1.4502, Val Loss: 1.5129, Best Val Loss: 1.5129\n",
      "Epoch 82/1000, Train Loss: 1.4291, Val Loss: 1.5002, Best Val Loss: 1.5002\n",
      "Epoch 88/1000, Train Loss: 1.4212, Val Loss: 1.4972, Best Val Loss: 1.4972\n",
      "Epoch 95/1000, Train Loss: 1.4011, Val Loss: 1.4871, Best Val Loss: 1.4871\n",
      "Epoch 96/1000, Train Loss: 1.4069, Val Loss: 1.4778, Best Val Loss: 1.4778\n",
      "Epoch 101/1000, Train Loss: 1.3993, Val Loss: 1.4774, Best Val Loss: 1.4774\n",
      "Epoch 112/1000, Train Loss: 1.3801, Val Loss: 1.4725, Best Val Loss: 1.4725\n",
      "Early stopping at epoch 162, Best Val Loss: 1.4725\n",
      "Epistemic Variance: 0.865886\n",
      "Aleatoric Variance: 10.355071\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n",
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           1.987\n",
      "  RMSE          2.817\n",
      "  MDAE          1.482\n",
      "  MARPD         1.735\n",
      "  R2            0.861\n",
      "  Correlation   0.928\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.035\n",
      "  Mean-absolute Calibration Error       0.032\n",
      "  Miscalibration Area                   0.032\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.043\n",
      "     Group Size: 0.56 -- Calibration Error: 0.036\n",
      "     Group Size: 1.00 -- Calibration Error: 0.032\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.047\n",
      "     Group Size: 0.56 -- Calibration Error: 0.039\n",
      "     Group Size: 1.00 -- Calibration Error: 0.035\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   3.350\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   2.294\n",
      "  CRPS                      1.433\n",
      "  Check Score               0.723\n",
      "  Interval Score            7.201\n",
      "{'accuracy': {'mae': 1.9872824606338202, 'rmse': np.float64(2.817089032082645), 'mdae': 1.4818587524414113, 'marpd': np.float64(1.7352368032359893), 'r2': 0.8610190360950551, 'corr': np.float64(0.9281527467035683)}, 'avg_calibration': {'rms_cal': np.float64(0.03495274442117548), 'ma_cal': np.float64(0.03169166431206223), 'miscal_area': np.float64(0.03200716062089398)}, 'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.35543434, 0.04336637, 0.03947367, 0.03723627, 0.0362691 ,\n",
      "       0.03558678, 0.03527994, 0.03413293, 0.03315483, 0.03169166]), 'adv_group_cali_stderr': array([0.06259313, 0.00512378, 0.00272976, 0.00279784, 0.00169522,\n",
      "       0.00148073, 0.00129154, 0.00082831, 0.00057673, 0.        ])}, 'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.45725644, 0.04748497, 0.0433621 , 0.04193364, 0.04042642,\n",
      "       0.03946564, 0.03832286, 0.03761076, 0.03682379, 0.03495274]), 'adv_group_cali_stderr': array([0.06913812, 0.00441717, 0.00354323, 0.00218433, 0.00239253,\n",
      "       0.00135503, 0.00197913, 0.00090076, 0.00074443, 0.        ])}}, 'sharpness': {'sharp': np.float32(3.3497698)}, 'scoring_rule': {'nll': np.float64(2.2943989222758376), 'crps': np.float64(1.4328701542026647), 'check': np.float64(0.7234761536416943), 'interval': np.float64(7.200831966081777)}}\n",
      "coverage: 0.953378833028934, MPIW: 10.459032960057037\n",
      "Run 3 with seed 777\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 165.9495, Val Loss: 5.5439, Best Val Loss: 5.5439\n",
      "Epoch 2/1000, Train Loss: 5.4886, Val Loss: 5.4278, Best Val Loss: 5.4278\n",
      "Epoch 3/1000, Train Loss: 5.3828, Val Loss: 5.3385, Best Val Loss: 5.3385\n",
      "Epoch 4/1000, Train Loss: 5.3066, Val Loss: 5.2757, Best Val Loss: 5.2757\n",
      "Epoch 5/1000, Train Loss: 5.2495, Val Loss: 5.2242, Best Val Loss: 5.2242\n",
      "Epoch 6/1000, Train Loss: 5.1947, Val Loss: 5.1600, Best Val Loss: 5.1600\n",
      "Epoch 7/1000, Train Loss: 4.9288, Val Loss: 3.0560, Best Val Loss: 3.0560\n",
      "Epoch 8/1000, Train Loss: 2.3194, Val Loss: 2.1279, Best Val Loss: 2.1279\n",
      "Epoch 9/1000, Train Loss: 2.0178, Val Loss: 1.9608, Best Val Loss: 1.9608\n",
      "Epoch 10/1000, Train Loss: 1.8956, Val Loss: 1.8828, Best Val Loss: 1.8828\n",
      "Epoch 11/1000, Train Loss: 1.8383, Val Loss: 1.8344, Best Val Loss: 1.8344\n",
      "Epoch 12/1000, Train Loss: 1.8031, Val Loss: 1.7966, Best Val Loss: 1.7966\n",
      "Epoch 14/1000, Train Loss: 1.7462, Val Loss: 1.7642, Best Val Loss: 1.7642\n",
      "Epoch 15/1000, Train Loss: 1.7306, Val Loss: 1.7274, Best Val Loss: 1.7274\n",
      "Epoch 16/1000, Train Loss: 1.7132, Val Loss: 1.7187, Best Val Loss: 1.7187\n",
      "Epoch 20/1000, Train Loss: 1.6595, Val Loss: 1.6736, Best Val Loss: 1.6736\n",
      "Epoch 22/1000, Train Loss: 1.6611, Val Loss: 1.6557, Best Val Loss: 1.6557\n",
      "Epoch 23/1000, Train Loss: 1.6391, Val Loss: 1.6495, Best Val Loss: 1.6495\n",
      "Epoch 27/1000, Train Loss: 1.6057, Val Loss: 1.6356, Best Val Loss: 1.6356\n",
      "Epoch 28/1000, Train Loss: 1.6005, Val Loss: 1.6196, Best Val Loss: 1.6196\n",
      "Epoch 30/1000, Train Loss: 1.5911, Val Loss: 1.6159, Best Val Loss: 1.6159\n",
      "Epoch 32/1000, Train Loss: 1.5831, Val Loss: 1.6063, Best Val Loss: 1.6063\n",
      "Epoch 34/1000, Train Loss: 1.5675, Val Loss: 1.5911, Best Val Loss: 1.5911\n",
      "Epoch 35/1000, Train Loss: 1.5629, Val Loss: 1.5584, Best Val Loss: 1.5584\n",
      "Epoch 42/1000, Train Loss: 1.5214, Val Loss: 1.5525, Best Val Loss: 1.5525\n",
      "Epoch 45/1000, Train Loss: 1.5116, Val Loss: 1.5445, Best Val Loss: 1.5445\n",
      "Epoch 46/1000, Train Loss: 1.5117, Val Loss: 1.5408, Best Val Loss: 1.5408\n",
      "Epoch 49/1000, Train Loss: 1.4967, Val Loss: 1.5354, Best Val Loss: 1.5354\n",
      "Epoch 53/1000, Train Loss: 1.4828, Val Loss: 1.5115, Best Val Loss: 1.5115\n",
      "Epoch 58/1000, Train Loss: 1.4797, Val Loss: 1.5067, Best Val Loss: 1.5067\n",
      "Epoch 59/1000, Train Loss: 1.4707, Val Loss: 1.4922, Best Val Loss: 1.4922\n",
      "Epoch 68/1000, Train Loss: 1.4432, Val Loss: 1.4890, Best Val Loss: 1.4890\n",
      "Epoch 71/1000, Train Loss: 1.4342, Val Loss: 1.4826, Best Val Loss: 1.4826\n",
      "Epoch 82/1000, Train Loss: 1.4162, Val Loss: 1.4722, Best Val Loss: 1.4722\n",
      "Epoch 113/1000, Train Loss: 1.3603, Val Loss: 1.4543, Best Val Loss: 1.4543\n",
      "Early stopping at epoch 163, Best Val Loss: 1.4543\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 140.9380, Val Loss: 5.5253, Best Val Loss: 5.5253\n",
      "Epoch 2/1000, Train Loss: 5.4861, Val Loss: 5.4468, Best Val Loss: 5.4468\n",
      "Epoch 3/1000, Train Loss: 5.4159, Val Loss: 5.3816, Best Val Loss: 5.3816\n",
      "Epoch 4/1000, Train Loss: 5.3526, Val Loss: 5.3201, Best Val Loss: 5.3201\n",
      "Epoch 5/1000, Train Loss: 5.2909, Val Loss: 5.2624, Best Val Loss: 5.2624\n",
      "Epoch 6/1000, Train Loss: 5.2364, Val Loss: 5.2090, Best Val Loss: 5.2090\n",
      "Epoch 7/1000, Train Loss: 5.1743, Val Loss: 5.1339, Best Val Loss: 5.1339\n",
      "Epoch 8/1000, Train Loss: 5.0580, Val Loss: 4.8983, Best Val Loss: 4.8983\n",
      "Epoch 9/1000, Train Loss: 2.7444, Val Loss: 2.1388, Best Val Loss: 2.1388\n",
      "Epoch 10/1000, Train Loss: 2.0420, Val Loss: 1.9827, Best Val Loss: 1.9827\n",
      "Epoch 11/1000, Train Loss: 1.9295, Val Loss: 1.8987, Best Val Loss: 1.8987\n",
      "Epoch 12/1000, Train Loss: 1.8602, Val Loss: 1.8506, Best Val Loss: 1.8506\n",
      "Epoch 13/1000, Train Loss: 1.8216, Val Loss: 1.7972, Best Val Loss: 1.7972\n",
      "Epoch 16/1000, Train Loss: 1.7515, Val Loss: 1.7617, Best Val Loss: 1.7617\n",
      "Epoch 17/1000, Train Loss: 1.7325, Val Loss: 1.7228, Best Val Loss: 1.7228\n",
      "Epoch 19/1000, Train Loss: 1.7085, Val Loss: 1.7172, Best Val Loss: 1.7172\n",
      "Epoch 21/1000, Train Loss: 1.6834, Val Loss: 1.7157, Best Val Loss: 1.7157\n",
      "Epoch 22/1000, Train Loss: 1.6813, Val Loss: 1.6924, Best Val Loss: 1.6924\n",
      "Epoch 24/1000, Train Loss: 1.6554, Val Loss: 1.6376, Best Val Loss: 1.6376\n",
      "Epoch 32/1000, Train Loss: 1.5981, Val Loss: 1.6093, Best Val Loss: 1.6093\n",
      "Epoch 34/1000, Train Loss: 1.5782, Val Loss: 1.5962, Best Val Loss: 1.5962\n",
      "Epoch 37/1000, Train Loss: 1.5624, Val Loss: 1.5782, Best Val Loss: 1.5782\n",
      "Epoch 39/1000, Train Loss: 1.5581, Val Loss: 1.5589, Best Val Loss: 1.5589\n",
      "Epoch 48/1000, Train Loss: 1.5109, Val Loss: 1.5447, Best Val Loss: 1.5447\n",
      "Epoch 50/1000, Train Loss: 1.4991, Val Loss: 1.5281, Best Val Loss: 1.5281\n",
      "Epoch 51/1000, Train Loss: 1.5019, Val Loss: 1.5265, Best Val Loss: 1.5265\n",
      "Epoch 54/1000, Train Loss: 1.4836, Val Loss: 1.5177, Best Val Loss: 1.5177\n",
      "Epoch 58/1000, Train Loss: 1.4756, Val Loss: 1.5101, Best Val Loss: 1.5101\n",
      "Epoch 60/1000, Train Loss: 1.4675, Val Loss: 1.5084, Best Val Loss: 1.5084\n",
      "Epoch 62/1000, Train Loss: 1.4638, Val Loss: 1.5051, Best Val Loss: 1.5051\n",
      "Epoch 70/1000, Train Loss: 1.4456, Val Loss: 1.5030, Best Val Loss: 1.5030\n",
      "Epoch 72/1000, Train Loss: 1.4397, Val Loss: 1.4798, Best Val Loss: 1.4798\n",
      "Epoch 77/1000, Train Loss: 1.4196, Val Loss: 1.4797, Best Val Loss: 1.4797\n",
      "Epoch 79/1000, Train Loss: 1.4159, Val Loss: 1.4644, Best Val Loss: 1.4644\n",
      "Epoch 90/1000, Train Loss: 1.3966, Val Loss: 1.4596, Best Val Loss: 1.4596\n",
      "Early stopping at epoch 140, Best Val Loss: 1.4596\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 159.8454, Val Loss: 5.5023, Best Val Loss: 5.5023\n",
      "Epoch 2/1000, Train Loss: 5.4474, Val Loss: 5.3902, Best Val Loss: 5.3902\n",
      "Epoch 3/1000, Train Loss: 5.3472, Val Loss: 5.3051, Best Val Loss: 5.3051\n",
      "Epoch 4/1000, Train Loss: 5.2702, Val Loss: 5.2364, Best Val Loss: 5.2364\n",
      "Epoch 5/1000, Train Loss: 5.2083, Val Loss: 5.1797, Best Val Loss: 5.1797\n",
      "Epoch 6/1000, Train Loss: 5.1446, Val Loss: 5.1021, Best Val Loss: 5.1021\n",
      "Epoch 7/1000, Train Loss: 4.4623, Val Loss: 2.4276, Best Val Loss: 2.4276\n",
      "Epoch 8/1000, Train Loss: 2.1749, Val Loss: 2.0584, Best Val Loss: 2.0584\n",
      "Epoch 9/1000, Train Loss: 1.9700, Val Loss: 1.9632, Best Val Loss: 1.9632\n",
      "Epoch 10/1000, Train Loss: 1.8812, Val Loss: 1.8559, Best Val Loss: 1.8559\n",
      "Epoch 11/1000, Train Loss: 1.8332, Val Loss: 1.8379, Best Val Loss: 1.8379\n",
      "Epoch 12/1000, Train Loss: 1.7926, Val Loss: 1.7850, Best Val Loss: 1.7850\n",
      "Epoch 14/1000, Train Loss: 1.7443, Val Loss: 1.7628, Best Val Loss: 1.7628\n",
      "Epoch 15/1000, Train Loss: 1.7308, Val Loss: 1.7467, Best Val Loss: 1.7467\n",
      "Epoch 16/1000, Train Loss: 1.7048, Val Loss: 1.7006, Best Val Loss: 1.7006\n",
      "Epoch 18/1000, Train Loss: 1.6729, Val Loss: 1.6910, Best Val Loss: 1.6910\n",
      "Epoch 19/1000, Train Loss: 1.6709, Val Loss: 1.6813, Best Val Loss: 1.6813\n",
      "Epoch 20/1000, Train Loss: 1.6583, Val Loss: 1.6664, Best Val Loss: 1.6664\n",
      "Epoch 22/1000, Train Loss: 1.6368, Val Loss: 1.6574, Best Val Loss: 1.6574\n",
      "Epoch 24/1000, Train Loss: 1.6191, Val Loss: 1.6307, Best Val Loss: 1.6307\n",
      "Epoch 28/1000, Train Loss: 1.5972, Val Loss: 1.6282, Best Val Loss: 1.6282\n",
      "Epoch 29/1000, Train Loss: 1.5892, Val Loss: 1.5795, Best Val Loss: 1.5795\n",
      "Epoch 38/1000, Train Loss: 1.5326, Val Loss: 1.5720, Best Val Loss: 1.5720\n",
      "Epoch 39/1000, Train Loss: 1.5280, Val Loss: 1.5657, Best Val Loss: 1.5657\n",
      "Epoch 40/1000, Train Loss: 1.5268, Val Loss: 1.5520, Best Val Loss: 1.5520\n",
      "Epoch 44/1000, Train Loss: 1.5113, Val Loss: 1.5264, Best Val Loss: 1.5264\n",
      "Epoch 55/1000, Train Loss: 1.4708, Val Loss: 1.5240, Best Val Loss: 1.5240\n",
      "Epoch 57/1000, Train Loss: 1.4606, Val Loss: 1.4990, Best Val Loss: 1.4990\n",
      "Epoch 71/1000, Train Loss: 1.4217, Val Loss: 1.4884, Best Val Loss: 1.4884\n",
      "Epoch 85/1000, Train Loss: 1.3988, Val Loss: 1.4747, Best Val Loss: 1.4747\n",
      "Epoch 91/1000, Train Loss: 1.3875, Val Loss: 1.4654, Best Val Loss: 1.4654\n",
      "Epoch 117/1000, Train Loss: 1.3420, Val Loss: 1.4653, Best Val Loss: 1.4653\n",
      "Epoch 132/1000, Train Loss: 1.3233, Val Loss: 1.4611, Best Val Loss: 1.4611\n",
      "Early stopping at epoch 182, Best Val Loss: 1.4611\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 144.7298, Val Loss: 5.5223, Best Val Loss: 5.5223\n",
      "Epoch 2/1000, Train Loss: 5.4815, Val Loss: 5.4417, Best Val Loss: 5.4417\n",
      "Epoch 3/1000, Train Loss: 5.4113, Val Loss: 5.3792, Best Val Loss: 5.3792\n",
      "Epoch 4/1000, Train Loss: 5.3524, Val Loss: 5.3264, Best Val Loss: 5.3264\n",
      "Epoch 5/1000, Train Loss: 5.3036, Val Loss: 5.2822, Best Val Loss: 5.2822\n",
      "Epoch 6/1000, Train Loss: 5.2586, Val Loss: 5.2344, Best Val Loss: 5.2344\n",
      "Epoch 7/1000, Train Loss: 5.2035, Val Loss: 5.1680, Best Val Loss: 5.1680\n",
      "Epoch 8/1000, Train Loss: 5.1073, Val Loss: 5.0173, Best Val Loss: 5.0173\n",
      "Epoch 9/1000, Train Loss: 4.0123, Val Loss: 2.3603, Best Val Loss: 2.3603\n",
      "Epoch 10/1000, Train Loss: 2.2001, Val Loss: 2.0960, Best Val Loss: 2.0960\n",
      "Epoch 11/1000, Train Loss: 2.0172, Val Loss: 1.9627, Best Val Loss: 1.9627\n",
      "Epoch 12/1000, Train Loss: 1.9275, Val Loss: 1.8961, Best Val Loss: 1.8961\n",
      "Epoch 13/1000, Train Loss: 1.8644, Val Loss: 1.8491, Best Val Loss: 1.8491\n",
      "Epoch 14/1000, Train Loss: 1.8212, Val Loss: 1.8130, Best Val Loss: 1.8130\n",
      "Epoch 15/1000, Train Loss: 1.7960, Val Loss: 1.8069, Best Val Loss: 1.8069\n",
      "Epoch 16/1000, Train Loss: 1.7658, Val Loss: 1.7509, Best Val Loss: 1.7509\n",
      "Epoch 20/1000, Train Loss: 1.7036, Val Loss: 1.6939, Best Val Loss: 1.6939\n",
      "Epoch 22/1000, Train Loss: 1.6869, Val Loss: 1.6852, Best Val Loss: 1.6852\n",
      "Epoch 24/1000, Train Loss: 1.6605, Val Loss: 1.6652, Best Val Loss: 1.6652\n",
      "Epoch 25/1000, Train Loss: 1.6620, Val Loss: 1.6632, Best Val Loss: 1.6632\n",
      "Epoch 26/1000, Train Loss: 1.6399, Val Loss: 1.6491, Best Val Loss: 1.6491\n",
      "Epoch 27/1000, Train Loss: 1.6353, Val Loss: 1.6410, Best Val Loss: 1.6410\n",
      "Epoch 30/1000, Train Loss: 1.6174, Val Loss: 1.6023, Best Val Loss: 1.6023\n",
      "Epoch 33/1000, Train Loss: 1.5810, Val Loss: 1.5961, Best Val Loss: 1.5961\n",
      "Epoch 37/1000, Train Loss: 1.5568, Val Loss: 1.5914, Best Val Loss: 1.5914\n",
      "Epoch 40/1000, Train Loss: 1.5322, Val Loss: 1.5523, Best Val Loss: 1.5523\n",
      "Epoch 48/1000, Train Loss: 1.5016, Val Loss: 1.5460, Best Val Loss: 1.5460\n",
      "Epoch 52/1000, Train Loss: 1.4833, Val Loss: 1.5379, Best Val Loss: 1.5379\n",
      "Epoch 53/1000, Train Loss: 1.4916, Val Loss: 1.5214, Best Val Loss: 1.5214\n",
      "Epoch 54/1000, Train Loss: 1.4837, Val Loss: 1.5189, Best Val Loss: 1.5189\n",
      "Epoch 56/1000, Train Loss: 1.4778, Val Loss: 1.5060, Best Val Loss: 1.5060\n",
      "Epoch 67/1000, Train Loss: 1.4507, Val Loss: 1.4951, Best Val Loss: 1.4951\n",
      "Epoch 71/1000, Train Loss: 1.4432, Val Loss: 1.4847, Best Val Loss: 1.4847\n",
      "Epoch 77/1000, Train Loss: 1.4238, Val Loss: 1.4836, Best Val Loss: 1.4836\n",
      "Epoch 106/1000, Train Loss: 1.3771, Val Loss: 1.4832, Best Val Loss: 1.4832\n",
      "Epoch 118/1000, Train Loss: 1.3593, Val Loss: 1.4705, Best Val Loss: 1.4705\n",
      "Early stopping at epoch 168, Best Val Loss: 1.4705\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 137.3998, Val Loss: 5.5874, Best Val Loss: 5.5874\n",
      "Epoch 2/1000, Train Loss: 5.5199, Val Loss: 5.4521, Best Val Loss: 5.4521\n",
      "Epoch 3/1000, Train Loss: 5.3947, Val Loss: 5.3279, Best Val Loss: 5.3279\n",
      "Epoch 4/1000, Train Loss: 5.2286, Val Loss: 5.0765, Best Val Loss: 5.0765\n",
      "Epoch 5/1000, Train Loss: 4.1129, Val Loss: 2.5671, Best Val Loss: 2.5671\n",
      "Epoch 6/1000, Train Loss: 2.4027, Val Loss: 2.2824, Best Val Loss: 2.2824\n",
      "Epoch 7/1000, Train Loss: 2.1739, Val Loss: 2.1084, Best Val Loss: 2.1084\n",
      "Epoch 8/1000, Train Loss: 2.0239, Val Loss: 1.9882, Best Val Loss: 1.9882\n",
      "Epoch 9/1000, Train Loss: 1.9324, Val Loss: 1.9033, Best Val Loss: 1.9033\n",
      "Epoch 11/1000, Train Loss: 1.8378, Val Loss: 1.8140, Best Val Loss: 1.8140\n",
      "Epoch 13/1000, Train Loss: 1.7786, Val Loss: 1.7869, Best Val Loss: 1.7869\n",
      "Epoch 14/1000, Train Loss: 1.7553, Val Loss: 1.7596, Best Val Loss: 1.7596\n",
      "Epoch 16/1000, Train Loss: 1.7296, Val Loss: 1.7238, Best Val Loss: 1.7238\n",
      "Epoch 18/1000, Train Loss: 1.6990, Val Loss: 1.7137, Best Val Loss: 1.7137\n",
      "Epoch 19/1000, Train Loss: 1.7080, Val Loss: 1.6796, Best Val Loss: 1.6796\n",
      "Epoch 23/1000, Train Loss: 1.6644, Val Loss: 1.6499, Best Val Loss: 1.6499\n",
      "Epoch 25/1000, Train Loss: 1.6491, Val Loss: 1.6392, Best Val Loss: 1.6392\n",
      "Epoch 26/1000, Train Loss: 1.6385, Val Loss: 1.6324, Best Val Loss: 1.6324\n",
      "Epoch 34/1000, Train Loss: 1.5979, Val Loss: 1.6036, Best Val Loss: 1.6036\n",
      "Epoch 35/1000, Train Loss: 1.5915, Val Loss: 1.5735, Best Val Loss: 1.5735\n",
      "Epoch 41/1000, Train Loss: 1.5439, Val Loss: 1.5657, Best Val Loss: 1.5657\n",
      "Epoch 45/1000, Train Loss: 1.5332, Val Loss: 1.5542, Best Val Loss: 1.5542\n",
      "Epoch 47/1000, Train Loss: 1.5256, Val Loss: 1.5436, Best Val Loss: 1.5436\n",
      "Epoch 52/1000, Train Loss: 1.5090, Val Loss: 1.5228, Best Val Loss: 1.5228\n",
      "Epoch 57/1000, Train Loss: 1.4954, Val Loss: 1.5055, Best Val Loss: 1.5055\n",
      "Epoch 69/1000, Train Loss: 1.4506, Val Loss: 1.4992, Best Val Loss: 1.4992\n",
      "Epoch 70/1000, Train Loss: 1.4521, Val Loss: 1.4873, Best Val Loss: 1.4873\n",
      "Epoch 97/1000, Train Loss: 1.3947, Val Loss: 1.4866, Best Val Loss: 1.4866\n",
      "Epoch 100/1000, Train Loss: 1.4006, Val Loss: 1.4815, Best Val Loss: 1.4815\n",
      "Epoch 102/1000, Train Loss: 1.3907, Val Loss: 1.4698, Best Val Loss: 1.4698\n",
      "Epoch 128/1000, Train Loss: 1.3621, Val Loss: 1.4653, Best Val Loss: 1.4653\n",
      "Epoch 156/1000, Train Loss: 1.3247, Val Loss: 1.4626, Best Val Loss: 1.4626\n",
      "Early stopping at epoch 206, Best Val Loss: 1.4626\n",
      "Epistemic Variance: 0.950219\n",
      "Aleatoric Variance: 8.131834\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n",
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           1.955\n",
      "  RMSE          2.772\n",
      "  MDAE          1.437\n",
      "  MARPD         1.707\n",
      "  R2            0.865\n",
      "  Correlation   0.930\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.038\n",
      "  Mean-absolute Calibration Error       0.035\n",
      "  Miscalibration Area                   0.035\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.047\n",
      "     Group Size: 0.56 -- Calibration Error: 0.038\n",
      "     Group Size: 1.00 -- Calibration Error: 0.035\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.052\n",
      "     Group Size: 0.56 -- Calibration Error: 0.041\n",
      "     Group Size: 1.00 -- Calibration Error: 0.038\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   3.014\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   2.275\n",
      "  CRPS                      1.407\n",
      "  Check Score               0.710\n",
      "  Interval Score            7.071\n",
      "{'accuracy': {'mae': 1.9553234257196221, 'rmse': np.float64(2.7723194239936166), 'mdae': 1.4367778320312539, 'marpd': np.float64(1.7074280226481813), 'r2': 0.8654013482974411, 'corr': np.float64(0.9303692146981556)}, 'avg_calibration': {'rms_cal': np.float64(0.038198241820928736), 'ma_cal': np.float64(0.03466934072047977), 'miscal_area': np.float64(0.035017906729243146)}, 'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.35806566, 0.0468882 , 0.0431441 , 0.03978249, 0.03973291,\n",
      "       0.03792473, 0.0372925 , 0.03697093, 0.03621653, 0.03466934]), 'adv_group_cali_stderr': array([0.03815362, 0.00549001, 0.00223598, 0.00230389, 0.00181237,\n",
      "       0.0016834 , 0.00082792, 0.0007906 , 0.00095906, 0.        ])}, 'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.42927098, 0.0515571 , 0.0489745 , 0.04698195, 0.04286063,\n",
      "       0.04134018, 0.04137244, 0.04077623, 0.04009723, 0.03819824]), 'adv_group_cali_stderr': array([0.06359357, 0.0050217 , 0.00551743, 0.00270776, 0.00204423,\n",
      "       0.00116332, 0.00119303, 0.00106979, 0.00057727, 0.        ])}}, 'sharpness': {'sharp': np.float32(3.0136445)}, 'scoring_rule': {'nll': np.float64(2.2752694691825286), 'crps': np.float64(1.4068051165002082), 'check': np.float64(0.7103174672683034), 'interval': np.float64(7.070962110682711)}}\n",
      "coverage: 0.9543400942035951, MPIW: 10.323772811019042\n",
      "Run 4 with seed 2024\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 150.6342, Val Loss: 5.4654, Best Val Loss: 5.4654\n",
      "Epoch 2/1000, Train Loss: 5.4191, Val Loss: 5.3673, Best Val Loss: 5.3673\n",
      "Epoch 3/1000, Train Loss: 5.3242, Val Loss: 5.2774, Best Val Loss: 5.2774\n",
      "Epoch 4/1000, Train Loss: 5.2248, Val Loss: 5.1557, Best Val Loss: 5.1557\n",
      "Epoch 5/1000, Train Loss: 4.4915, Val Loss: 2.5514, Best Val Loss: 2.5514\n",
      "Epoch 6/1000, Train Loss: 2.3217, Val Loss: 2.1962, Best Val Loss: 2.1962\n",
      "Epoch 7/1000, Train Loss: 2.0974, Val Loss: 2.0475, Best Val Loss: 2.0475\n",
      "Epoch 8/1000, Train Loss: 1.9775, Val Loss: 1.9392, Best Val Loss: 1.9392\n",
      "Epoch 9/1000, Train Loss: 1.8946, Val Loss: 1.9035, Best Val Loss: 1.9035\n",
      "Epoch 10/1000, Train Loss: 1.8441, Val Loss: 1.8176, Best Val Loss: 1.8176\n",
      "Epoch 11/1000, Train Loss: 1.8061, Val Loss: 1.8072, Best Val Loss: 1.8072\n",
      "Epoch 12/1000, Train Loss: 1.7847, Val Loss: 1.7571, Best Val Loss: 1.7571\n",
      "Epoch 13/1000, Train Loss: 1.7607, Val Loss: 1.7462, Best Val Loss: 1.7462\n",
      "Epoch 16/1000, Train Loss: 1.7141, Val Loss: 1.7051, Best Val Loss: 1.7051\n",
      "Epoch 19/1000, Train Loss: 1.6753, Val Loss: 1.6919, Best Val Loss: 1.6919\n",
      "Epoch 21/1000, Train Loss: 1.6528, Val Loss: 1.6644, Best Val Loss: 1.6644\n",
      "Epoch 22/1000, Train Loss: 1.6552, Val Loss: 1.6294, Best Val Loss: 1.6294\n",
      "Epoch 26/1000, Train Loss: 1.6210, Val Loss: 1.6228, Best Val Loss: 1.6228\n",
      "Epoch 29/1000, Train Loss: 1.5985, Val Loss: 1.6109, Best Val Loss: 1.6109\n",
      "Epoch 32/1000, Train Loss: 1.5826, Val Loss: 1.5949, Best Val Loss: 1.5949\n",
      "Epoch 35/1000, Train Loss: 1.5629, Val Loss: 1.5918, Best Val Loss: 1.5918\n",
      "Epoch 36/1000, Train Loss: 1.5550, Val Loss: 1.5579, Best Val Loss: 1.5579\n",
      "Epoch 41/1000, Train Loss: 1.5224, Val Loss: 1.5448, Best Val Loss: 1.5448\n",
      "Epoch 44/1000, Train Loss: 1.5106, Val Loss: 1.5277, Best Val Loss: 1.5277\n",
      "Epoch 47/1000, Train Loss: 1.4980, Val Loss: 1.5258, Best Val Loss: 1.5258\n",
      "Epoch 48/1000, Train Loss: 1.4998, Val Loss: 1.5237, Best Val Loss: 1.5237\n",
      "Epoch 54/1000, Train Loss: 1.4734, Val Loss: 1.4920, Best Val Loss: 1.4920\n",
      "Epoch 69/1000, Train Loss: 1.4344, Val Loss: 1.4778, Best Val Loss: 1.4778\n",
      "Epoch 77/1000, Train Loss: 1.4109, Val Loss: 1.4566, Best Val Loss: 1.4566\n",
      "Early stopping at epoch 127, Best Val Loss: 1.4566\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 113.1164, Val Loss: 5.4901, Best Val Loss: 5.4901\n",
      "Epoch 2/1000, Train Loss: 5.4280, Val Loss: 5.3713, Best Val Loss: 5.3713\n",
      "Epoch 3/1000, Train Loss: 5.3279, Val Loss: 5.2860, Best Val Loss: 5.2860\n",
      "Epoch 4/1000, Train Loss: 5.2420, Val Loss: 5.1920, Best Val Loss: 5.1920\n",
      "Epoch 5/1000, Train Loss: 5.0568, Val Loss: 4.5372, Best Val Loss: 4.5372\n",
      "Epoch 6/1000, Train Loss: 2.5784, Val Loss: 2.2451, Best Val Loss: 2.2451\n",
      "Epoch 7/1000, Train Loss: 2.1350, Val Loss: 2.0592, Best Val Loss: 2.0592\n",
      "Epoch 8/1000, Train Loss: 1.9884, Val Loss: 1.9537, Best Val Loss: 1.9537\n",
      "Epoch 9/1000, Train Loss: 1.8966, Val Loss: 1.8796, Best Val Loss: 1.8796\n",
      "Epoch 10/1000, Train Loss: 1.8463, Val Loss: 1.8391, Best Val Loss: 1.8391\n",
      "Epoch 11/1000, Train Loss: 1.8048, Val Loss: 1.8237, Best Val Loss: 1.8237\n",
      "Epoch 12/1000, Train Loss: 1.7775, Val Loss: 1.7650, Best Val Loss: 1.7650\n",
      "Epoch 14/1000, Train Loss: 1.7376, Val Loss: 1.7472, Best Val Loss: 1.7472\n",
      "Epoch 17/1000, Train Loss: 1.6944, Val Loss: 1.7151, Best Val Loss: 1.7151\n",
      "Epoch 19/1000, Train Loss: 1.6682, Val Loss: 1.6765, Best Val Loss: 1.6765\n",
      "Epoch 22/1000, Train Loss: 1.6411, Val Loss: 1.6718, Best Val Loss: 1.6718\n",
      "Epoch 23/1000, Train Loss: 1.6407, Val Loss: 1.6619, Best Val Loss: 1.6619\n",
      "Epoch 25/1000, Train Loss: 1.6222, Val Loss: 1.6319, Best Val Loss: 1.6319\n",
      "Epoch 29/1000, Train Loss: 1.5912, Val Loss: 1.6183, Best Val Loss: 1.6183\n",
      "Epoch 30/1000, Train Loss: 1.5752, Val Loss: 1.5920, Best Val Loss: 1.5920\n",
      "Epoch 31/1000, Train Loss: 1.5761, Val Loss: 1.5771, Best Val Loss: 1.5771\n",
      "Epoch 34/1000, Train Loss: 1.5445, Val Loss: 1.5717, Best Val Loss: 1.5717\n",
      "Epoch 36/1000, Train Loss: 1.5465, Val Loss: 1.5580, Best Val Loss: 1.5580\n",
      "Epoch 38/1000, Train Loss: 1.5284, Val Loss: 1.5389, Best Val Loss: 1.5389\n",
      "Epoch 41/1000, Train Loss: 1.5036, Val Loss: 1.5215, Best Val Loss: 1.5215\n",
      "Epoch 52/1000, Train Loss: 1.4708, Val Loss: 1.5122, Best Val Loss: 1.5122\n",
      "Epoch 53/1000, Train Loss: 1.4697, Val Loss: 1.5052, Best Val Loss: 1.5052\n",
      "Epoch 64/1000, Train Loss: 1.4384, Val Loss: 1.4969, Best Val Loss: 1.4969\n",
      "Epoch 69/1000, Train Loss: 1.4328, Val Loss: 1.4964, Best Val Loss: 1.4964\n",
      "Epoch 70/1000, Train Loss: 1.4205, Val Loss: 1.4909, Best Val Loss: 1.4909\n",
      "Epoch 75/1000, Train Loss: 1.4121, Val Loss: 1.4817, Best Val Loss: 1.4817\n",
      "Epoch 84/1000, Train Loss: 1.3945, Val Loss: 1.4722, Best Val Loss: 1.4722\n",
      "Early stopping at epoch 134, Best Val Loss: 1.4722\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 142.4738, Val Loss: 5.4270, Best Val Loss: 5.4270\n",
      "Epoch 2/1000, Train Loss: 5.3498, Val Loss: 5.2544, Best Val Loss: 5.2544\n",
      "Epoch 3/1000, Train Loss: 5.1777, Val Loss: 5.0805, Best Val Loss: 5.0805\n",
      "Epoch 4/1000, Train Loss: 4.7374, Val Loss: 3.7028, Best Val Loss: 3.7028\n",
      "Epoch 5/1000, Train Loss: 2.9066, Val Loss: 2.5962, Best Val Loss: 2.5962\n",
      "Epoch 6/1000, Train Loss: 2.4005, Val Loss: 2.2637, Best Val Loss: 2.2637\n",
      "Epoch 7/1000, Train Loss: 2.1623, Val Loss: 2.0933, Best Val Loss: 2.0933\n",
      "Epoch 8/1000, Train Loss: 2.0151, Val Loss: 1.9943, Best Val Loss: 1.9943\n",
      "Epoch 9/1000, Train Loss: 1.9229, Val Loss: 1.9299, Best Val Loss: 1.9299\n",
      "Epoch 10/1000, Train Loss: 1.8649, Val Loss: 1.8684, Best Val Loss: 1.8684\n",
      "Epoch 11/1000, Train Loss: 1.8185, Val Loss: 1.8127, Best Val Loss: 1.8127\n",
      "Epoch 12/1000, Train Loss: 1.7852, Val Loss: 1.7869, Best Val Loss: 1.7869\n",
      "Epoch 13/1000, Train Loss: 1.7619, Val Loss: 1.7791, Best Val Loss: 1.7791\n",
      "Epoch 15/1000, Train Loss: 1.7220, Val Loss: 1.7311, Best Val Loss: 1.7311\n",
      "Epoch 16/1000, Train Loss: 1.7193, Val Loss: 1.7192, Best Val Loss: 1.7192\n",
      "Epoch 17/1000, Train Loss: 1.6891, Val Loss: 1.6823, Best Val Loss: 1.6823\n",
      "Epoch 19/1000, Train Loss: 1.6808, Val Loss: 1.6803, Best Val Loss: 1.6803\n",
      "Epoch 23/1000, Train Loss: 1.6424, Val Loss: 1.6469, Best Val Loss: 1.6469\n",
      "Epoch 24/1000, Train Loss: 1.6407, Val Loss: 1.6281, Best Val Loss: 1.6281\n",
      "Epoch 30/1000, Train Loss: 1.5979, Val Loss: 1.6065, Best Val Loss: 1.6065\n",
      "Epoch 34/1000, Train Loss: 1.5801, Val Loss: 1.5967, Best Val Loss: 1.5967\n",
      "Epoch 37/1000, Train Loss: 1.5756, Val Loss: 1.5896, Best Val Loss: 1.5896\n",
      "Epoch 42/1000, Train Loss: 1.5467, Val Loss: 1.5659, Best Val Loss: 1.5659\n",
      "Epoch 44/1000, Train Loss: 1.5319, Val Loss: 1.5622, Best Val Loss: 1.5622\n",
      "Epoch 50/1000, Train Loss: 1.5138, Val Loss: 1.5303, Best Val Loss: 1.5303\n",
      "Epoch 55/1000, Train Loss: 1.5009, Val Loss: 1.5291, Best Val Loss: 1.5291\n",
      "Epoch 61/1000, Train Loss: 1.4861, Val Loss: 1.5188, Best Val Loss: 1.5188\n",
      "Epoch 63/1000, Train Loss: 1.4790, Val Loss: 1.5144, Best Val Loss: 1.5144\n",
      "Epoch 75/1000, Train Loss: 1.4483, Val Loss: 1.5118, Best Val Loss: 1.5118\n",
      "Epoch 81/1000, Train Loss: 1.4410, Val Loss: 1.4977, Best Val Loss: 1.4977\n",
      "Epoch 90/1000, Train Loss: 1.4229, Val Loss: 1.4897, Best Val Loss: 1.4897\n",
      "Epoch 100/1000, Train Loss: 1.4049, Val Loss: 1.4843, Best Val Loss: 1.4843\n",
      "Epoch 101/1000, Train Loss: 1.4134, Val Loss: 1.4782, Best Val Loss: 1.4782\n",
      "Epoch 122/1000, Train Loss: 1.3676, Val Loss: 1.4712, Best Val Loss: 1.4712\n",
      "Early stopping at epoch 172, Best Val Loss: 1.4712\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 150.6478, Val Loss: 5.5429, Best Val Loss: 5.5429\n",
      "Epoch 2/1000, Train Loss: 5.4851, Val Loss: 5.4372, Best Val Loss: 5.4372\n",
      "Epoch 3/1000, Train Loss: 5.3939, Val Loss: 5.3538, Best Val Loss: 5.3538\n",
      "Epoch 4/1000, Train Loss: 5.3134, Val Loss: 5.2771, Best Val Loss: 5.2771\n",
      "Epoch 5/1000, Train Loss: 5.2372, Val Loss: 5.1982, Best Val Loss: 5.1982\n",
      "Epoch 6/1000, Train Loss: 5.1437, Val Loss: 5.0790, Best Val Loss: 5.0790\n",
      "Epoch 7/1000, Train Loss: 4.6836, Val Loss: 2.6373, Best Val Loss: 2.6373\n",
      "Epoch 8/1000, Train Loss: 2.2637, Val Loss: 2.1394, Best Val Loss: 2.1394\n",
      "Epoch 9/1000, Train Loss: 2.0257, Val Loss: 1.9831, Best Val Loss: 1.9831\n",
      "Epoch 10/1000, Train Loss: 1.9131, Val Loss: 1.8724, Best Val Loss: 1.8724\n",
      "Epoch 11/1000, Train Loss: 1.8566, Val Loss: 1.8527, Best Val Loss: 1.8527\n",
      "Epoch 12/1000, Train Loss: 1.8223, Val Loss: 1.8130, Best Val Loss: 1.8130\n",
      "Epoch 13/1000, Train Loss: 1.7878, Val Loss: 1.7709, Best Val Loss: 1.7709\n",
      "Epoch 14/1000, Train Loss: 1.7550, Val Loss: 1.7382, Best Val Loss: 1.7382\n",
      "Epoch 16/1000, Train Loss: 1.7238, Val Loss: 1.7276, Best Val Loss: 1.7276\n",
      "Epoch 19/1000, Train Loss: 1.6905, Val Loss: 1.7037, Best Val Loss: 1.7037\n",
      "Epoch 20/1000, Train Loss: 1.6773, Val Loss: 1.6936, Best Val Loss: 1.6936\n",
      "Epoch 22/1000, Train Loss: 1.6699, Val Loss: 1.6741, Best Val Loss: 1.6741\n",
      "Epoch 23/1000, Train Loss: 1.6549, Val Loss: 1.6488, Best Val Loss: 1.6488\n",
      "Epoch 25/1000, Train Loss: 1.6371, Val Loss: 1.6443, Best Val Loss: 1.6443\n",
      "Epoch 27/1000, Train Loss: 1.6232, Val Loss: 1.6378, Best Val Loss: 1.6378\n",
      "Epoch 31/1000, Train Loss: 1.5945, Val Loss: 1.6362, Best Val Loss: 1.6362\n",
      "Epoch 33/1000, Train Loss: 1.5795, Val Loss: 1.5919, Best Val Loss: 1.5919\n",
      "Epoch 35/1000, Train Loss: 1.5695, Val Loss: 1.5800, Best Val Loss: 1.5800\n",
      "Epoch 38/1000, Train Loss: 1.5434, Val Loss: 1.5760, Best Val Loss: 1.5760\n",
      "Epoch 39/1000, Train Loss: 1.5399, Val Loss: 1.5432, Best Val Loss: 1.5432\n",
      "Epoch 48/1000, Train Loss: 1.5017, Val Loss: 1.5382, Best Val Loss: 1.5382\n",
      "Epoch 49/1000, Train Loss: 1.4853, Val Loss: 1.5186, Best Val Loss: 1.5186\n",
      "Epoch 64/1000, Train Loss: 1.4441, Val Loss: 1.5139, Best Val Loss: 1.5139\n",
      "Epoch 66/1000, Train Loss: 1.4333, Val Loss: 1.5078, Best Val Loss: 1.5078\n",
      "Epoch 70/1000, Train Loss: 1.4333, Val Loss: 1.4997, Best Val Loss: 1.4997\n",
      "Epoch 74/1000, Train Loss: 1.4228, Val Loss: 1.4942, Best Val Loss: 1.4942\n",
      "Epoch 76/1000, Train Loss: 1.4239, Val Loss: 1.4852, Best Val Loss: 1.4852\n",
      "Epoch 83/1000, Train Loss: 1.4043, Val Loss: 1.4740, Best Val Loss: 1.4740\n",
      "Epoch 96/1000, Train Loss: 1.3753, Val Loss: 1.4736, Best Val Loss: 1.4736\n",
      "Early stopping at epoch 146, Best Val Loss: 1.4736\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 117.1112, Val Loss: 5.5366, Best Val Loss: 5.5366\n",
      "Epoch 2/1000, Train Loss: 5.4747, Val Loss: 5.4159, Best Val Loss: 5.4159\n",
      "Epoch 3/1000, Train Loss: 5.3696, Val Loss: 5.3267, Best Val Loss: 5.3267\n",
      "Epoch 4/1000, Train Loss: 5.2936, Val Loss: 5.2630, Best Val Loss: 5.2630\n",
      "Epoch 5/1000, Train Loss: 5.2348, Val Loss: 5.2061, Best Val Loss: 5.2061\n",
      "Epoch 6/1000, Train Loss: 5.1684, Val Loss: 5.1250, Best Val Loss: 5.1250\n",
      "Epoch 7/1000, Train Loss: 5.0235, Val Loss: 4.7719, Best Val Loss: 4.7719\n",
      "Epoch 8/1000, Train Loss: 2.6401, Val Loss: 2.1969, Best Val Loss: 2.1969\n",
      "Epoch 9/1000, Train Loss: 2.0637, Val Loss: 2.0038, Best Val Loss: 2.0038\n",
      "Epoch 10/1000, Train Loss: 1.9332, Val Loss: 1.9173, Best Val Loss: 1.9173\n",
      "Epoch 11/1000, Train Loss: 1.8610, Val Loss: 1.8691, Best Val Loss: 1.8691\n",
      "Epoch 12/1000, Train Loss: 1.8187, Val Loss: 1.8216, Best Val Loss: 1.8216\n",
      "Epoch 14/1000, Train Loss: 1.7634, Val Loss: 1.7932, Best Val Loss: 1.7932\n",
      "Epoch 15/1000, Train Loss: 1.7431, Val Loss: 1.7426, Best Val Loss: 1.7426\n",
      "Epoch 16/1000, Train Loss: 1.7265, Val Loss: 1.7343, Best Val Loss: 1.7343\n",
      "Epoch 17/1000, Train Loss: 1.7181, Val Loss: 1.7057, Best Val Loss: 1.7057\n",
      "Epoch 21/1000, Train Loss: 1.6592, Val Loss: 1.6776, Best Val Loss: 1.6776\n",
      "Epoch 23/1000, Train Loss: 1.6411, Val Loss: 1.6448, Best Val Loss: 1.6448\n",
      "Epoch 29/1000, Train Loss: 1.5975, Val Loss: 1.6168, Best Val Loss: 1.6168\n",
      "Epoch 32/1000, Train Loss: 1.5749, Val Loss: 1.5968, Best Val Loss: 1.5968\n",
      "Epoch 33/1000, Train Loss: 1.5630, Val Loss: 1.5964, Best Val Loss: 1.5964\n",
      "Epoch 34/1000, Train Loss: 1.5664, Val Loss: 1.5793, Best Val Loss: 1.5793\n",
      "Epoch 40/1000, Train Loss: 1.5278, Val Loss: 1.5717, Best Val Loss: 1.5717\n",
      "Epoch 41/1000, Train Loss: 1.5154, Val Loss: 1.5417, Best Val Loss: 1.5417\n",
      "Epoch 49/1000, Train Loss: 1.4889, Val Loss: 1.5383, Best Val Loss: 1.5383\n",
      "Epoch 51/1000, Train Loss: 1.4772, Val Loss: 1.5207, Best Val Loss: 1.5207\n",
      "Epoch 54/1000, Train Loss: 1.4683, Val Loss: 1.4991, Best Val Loss: 1.4991\n",
      "Epoch 73/1000, Train Loss: 1.4239, Val Loss: 1.4689, Best Val Loss: 1.4689\n",
      "Epoch 94/1000, Train Loss: 1.3802, Val Loss: 1.4661, Best Val Loss: 1.4661\n",
      "Epoch 96/1000, Train Loss: 1.3868, Val Loss: 1.4555, Best Val Loss: 1.4555\n",
      "Early stopping at epoch 146, Best Val Loss: 1.4555\n",
      "Epistemic Variance: 0.919660\n",
      "Aleatoric Variance: 9.633337\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n",
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           1.980\n",
      "  RMSE          2.797\n",
      "  MDAE          1.465\n",
      "  MARPD         1.729\n",
      "  R2            0.863\n",
      "  Correlation   0.929\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.030\n",
      "  Mean-absolute Calibration Error       0.026\n",
      "  Miscalibration Area                   0.027\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.038\n",
      "     Group Size: 0.56 -- Calibration Error: 0.030\n",
      "     Group Size: 1.00 -- Calibration Error: 0.026\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.046\n",
      "     Group Size: 0.56 -- Calibration Error: 0.035\n",
      "     Group Size: 1.00 -- Calibration Error: 0.030\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   3.249\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   2.282\n",
      "  CRPS                      1.419\n",
      "  Check Score               0.716\n",
      "  Interval Score            7.119\n",
      "{'accuracy': {'mae': 1.9795458945023374, 'rmse': np.float64(2.796578451216053), 'mdae': 1.4651736938476603, 'marpd': np.float64(1.7287886101981287), 'r2': 0.8630354459814187, 'corr': np.float64(0.9290851418744073)}, 'avg_calibration': {'rms_cal': np.float64(0.030280775472968487), 'ma_cal': np.float64(0.02636934567243129), 'miscal_area': np.float64(0.026633819097807248)}, 'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.37578283, 0.03803215, 0.03331795, 0.03412928, 0.03103681,\n",
      "       0.03003018, 0.02981068, 0.028727  , 0.02831702, 0.02636935]), 'adv_group_cali_stderr': array([7.65193842e-02, 4.83792836e-03, 2.16962495e-03, 2.55129793e-03,\n",
      "       2.28900217e-03, 2.29867703e-03, 9.32618287e-04, 1.17883258e-03,\n",
      "       9.45889376e-04, 3.65711820e-18])}, 'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.42494029, 0.04582846, 0.0398267 , 0.03728014, 0.03574031,\n",
      "       0.0349037 , 0.03322847, 0.03275167, 0.0319394 , 0.03028078]), 'adv_group_cali_stderr': array([0.05538515, 0.00374151, 0.00248791, 0.00295331, 0.00095433,\n",
      "       0.00163896, 0.00119803, 0.0009051 , 0.0008428 , 0.        ])}}, 'sharpness': {'sharp': np.float32(3.2485378)}, 'scoring_rule': {'nll': np.float64(2.2816881388503742), 'crps': np.float64(1.41870340841148), 'check': np.float64(0.7163267761963674), 'interval': np.float64(7.11897319945172)}}\n",
      "coverage: 0.9540517158511967, MPIW: 10.255678692738515\n",
      "Run 5 with seed 5250\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 136.2163, Val Loss: 5.5831, Best Val Loss: 5.5831\n",
      "Epoch 2/1000, Train Loss: 5.5236, Val Loss: 5.4598, Best Val Loss: 5.4598\n",
      "Epoch 3/1000, Train Loss: 5.4162, Val Loss: 5.3626, Best Val Loss: 5.3626\n",
      "Epoch 4/1000, Train Loss: 5.3135, Val Loss: 5.2508, Best Val Loss: 5.2508\n",
      "Epoch 5/1000, Train Loss: 5.1513, Val Loss: 4.9922, Best Val Loss: 4.9922\n",
      "Epoch 6/1000, Train Loss: 3.8083, Val Loss: 2.4888, Best Val Loss: 2.4888\n",
      "Epoch 7/1000, Train Loss: 2.3390, Val Loss: 2.2224, Best Val Loss: 2.2224\n",
      "Epoch 8/1000, Train Loss: 2.1188, Val Loss: 2.0572, Best Val Loss: 2.0572\n",
      "Epoch 9/1000, Train Loss: 1.9779, Val Loss: 1.9454, Best Val Loss: 1.9454\n",
      "Epoch 10/1000, Train Loss: 1.8899, Val Loss: 1.8798, Best Val Loss: 1.8798\n",
      "Epoch 11/1000, Train Loss: 1.8370, Val Loss: 1.8454, Best Val Loss: 1.8454\n",
      "Epoch 12/1000, Train Loss: 1.7949, Val Loss: 1.8046, Best Val Loss: 1.8046\n",
      "Epoch 13/1000, Train Loss: 1.7762, Val Loss: 1.7527, Best Val Loss: 1.7527\n",
      "Epoch 16/1000, Train Loss: 1.7300, Val Loss: 1.7182, Best Val Loss: 1.7182\n",
      "Epoch 18/1000, Train Loss: 1.7091, Val Loss: 1.6967, Best Val Loss: 1.6967\n",
      "Epoch 20/1000, Train Loss: 1.6957, Val Loss: 1.6885, Best Val Loss: 1.6885\n",
      "Epoch 25/1000, Train Loss: 1.6572, Val Loss: 1.6521, Best Val Loss: 1.6521\n",
      "Epoch 27/1000, Train Loss: 1.6389, Val Loss: 1.6319, Best Val Loss: 1.6319\n",
      "Epoch 29/1000, Train Loss: 1.6313, Val Loss: 1.6056, Best Val Loss: 1.6056\n",
      "Epoch 35/1000, Train Loss: 1.5984, Val Loss: 1.5924, Best Val Loss: 1.5924\n",
      "Epoch 36/1000, Train Loss: 1.5940, Val Loss: 1.5862, Best Val Loss: 1.5862\n",
      "Epoch 38/1000, Train Loss: 1.5792, Val Loss: 1.5705, Best Val Loss: 1.5705\n",
      "Epoch 43/1000, Train Loss: 1.5539, Val Loss: 1.5594, Best Val Loss: 1.5594\n",
      "Epoch 44/1000, Train Loss: 1.5433, Val Loss: 1.5516, Best Val Loss: 1.5516\n",
      "Epoch 54/1000, Train Loss: 1.5051, Val Loss: 1.5396, Best Val Loss: 1.5396\n",
      "Epoch 55/1000, Train Loss: 1.4954, Val Loss: 1.5297, Best Val Loss: 1.5297\n",
      "Epoch 56/1000, Train Loss: 1.5040, Val Loss: 1.5169, Best Val Loss: 1.5169\n",
      "Epoch 65/1000, Train Loss: 1.4684, Val Loss: 1.5105, Best Val Loss: 1.5105\n",
      "Epoch 71/1000, Train Loss: 1.4622, Val Loss: 1.5098, Best Val Loss: 1.5098\n",
      "Epoch 81/1000, Train Loss: 1.4464, Val Loss: 1.5088, Best Val Loss: 1.5088\n",
      "Epoch 84/1000, Train Loss: 1.4305, Val Loss: 1.4944, Best Val Loss: 1.4944\n",
      "Epoch 94/1000, Train Loss: 1.4156, Val Loss: 1.4910, Best Val Loss: 1.4910\n",
      "Epoch 100/1000, Train Loss: 1.4165, Val Loss: 1.4780, Best Val Loss: 1.4780\n",
      "Epoch 114/1000, Train Loss: 1.3833, Val Loss: 1.4777, Best Val Loss: 1.4777\n",
      "Early stopping at epoch 164, Best Val Loss: 1.4777\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 139.7724, Val Loss: 5.5034, Best Val Loss: 5.5034\n",
      "Epoch 2/1000, Train Loss: 5.4299, Val Loss: 5.3693, Best Val Loss: 5.3693\n",
      "Epoch 3/1000, Train Loss: 5.3373, Val Loss: 5.2991, Best Val Loss: 5.2991\n",
      "Epoch 4/1000, Train Loss: 5.2703, Val Loss: 5.2366, Best Val Loss: 5.2366\n",
      "Epoch 5/1000, Train Loss: 5.2054, Val Loss: 5.1686, Best Val Loss: 5.1686\n",
      "Epoch 6/1000, Train Loss: 5.1045, Val Loss: 4.9714, Best Val Loss: 4.9714\n",
      "Epoch 7/1000, Train Loss: 3.1545, Val Loss: 2.2373, Best Val Loss: 2.2373\n",
      "Epoch 8/1000, Train Loss: 2.1140, Val Loss: 2.0205, Best Val Loss: 2.0205\n",
      "Epoch 9/1000, Train Loss: 1.9634, Val Loss: 1.9282, Best Val Loss: 1.9282\n",
      "Epoch 11/1000, Train Loss: 1.8233, Val Loss: 1.8059, Best Val Loss: 1.8059\n",
      "Epoch 13/1000, Train Loss: 1.7692, Val Loss: 1.7892, Best Val Loss: 1.7892\n",
      "Epoch 15/1000, Train Loss: 1.7302, Val Loss: 1.7551, Best Val Loss: 1.7551\n",
      "Epoch 16/1000, Train Loss: 1.7203, Val Loss: 1.7294, Best Val Loss: 1.7294\n",
      "Epoch 17/1000, Train Loss: 1.7051, Val Loss: 1.7050, Best Val Loss: 1.7050\n",
      "Epoch 20/1000, Train Loss: 1.6869, Val Loss: 1.6997, Best Val Loss: 1.6997\n",
      "Epoch 22/1000, Train Loss: 1.6525, Val Loss: 1.6503, Best Val Loss: 1.6503\n",
      "Epoch 25/1000, Train Loss: 1.6303, Val Loss: 1.6107, Best Val Loss: 1.6107\n",
      "Epoch 30/1000, Train Loss: 1.5823, Val Loss: 1.5907, Best Val Loss: 1.5907\n",
      "Epoch 34/1000, Train Loss: 1.5559, Val Loss: 1.5621, Best Val Loss: 1.5621\n",
      "Epoch 35/1000, Train Loss: 1.5603, Val Loss: 1.5563, Best Val Loss: 1.5563\n",
      "Epoch 40/1000, Train Loss: 1.5142, Val Loss: 1.5484, Best Val Loss: 1.5484\n",
      "Epoch 45/1000, Train Loss: 1.4932, Val Loss: 1.5145, Best Val Loss: 1.5145\n",
      "Epoch 57/1000, Train Loss: 1.4535, Val Loss: 1.5138, Best Val Loss: 1.5138\n",
      "Epoch 59/1000, Train Loss: 1.4583, Val Loss: 1.5015, Best Val Loss: 1.5015\n",
      "Epoch 64/1000, Train Loss: 1.4382, Val Loss: 1.4893, Best Val Loss: 1.4893\n",
      "Epoch 73/1000, Train Loss: 1.4142, Val Loss: 1.4886, Best Val Loss: 1.4886\n",
      "Epoch 98/1000, Train Loss: 1.3738, Val Loss: 1.4832, Best Val Loss: 1.4832\n",
      "Early stopping at epoch 148, Best Val Loss: 1.4832\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 127.9004, Val Loss: 5.4402, Best Val Loss: 5.4402\n",
      "Epoch 2/1000, Train Loss: 5.3908, Val Loss: 5.3413, Best Val Loss: 5.3413\n",
      "Epoch 3/1000, Train Loss: 5.3115, Val Loss: 5.2777, Best Val Loss: 5.2777\n",
      "Epoch 4/1000, Train Loss: 5.2538, Val Loss: 5.2271, Best Val Loss: 5.2271\n",
      "Epoch 5/1000, Train Loss: 5.2030, Val Loss: 5.1727, Best Val Loss: 5.1727\n",
      "Epoch 6/1000, Train Loss: 4.9935, Val Loss: 3.6216, Best Val Loss: 3.6216\n",
      "Epoch 7/1000, Train Loss: 2.4757, Val Loss: 2.1982, Best Val Loss: 2.1982\n",
      "Epoch 8/1000, Train Loss: 2.0522, Val Loss: 1.9890, Best Val Loss: 1.9890\n",
      "Epoch 9/1000, Train Loss: 1.9290, Val Loss: 1.8905, Best Val Loss: 1.8905\n",
      "Epoch 10/1000, Train Loss: 1.8583, Val Loss: 1.8660, Best Val Loss: 1.8660\n",
      "Epoch 12/1000, Train Loss: 1.7723, Val Loss: 1.8465, Best Val Loss: 1.8465\n",
      "Epoch 13/1000, Train Loss: 1.7598, Val Loss: 1.7591, Best Val Loss: 1.7591\n",
      "Epoch 16/1000, Train Loss: 1.7064, Val Loss: 1.7430, Best Val Loss: 1.7430\n",
      "Epoch 17/1000, Train Loss: 1.6792, Val Loss: 1.7029, Best Val Loss: 1.7029\n",
      "Epoch 19/1000, Train Loss: 1.6641, Val Loss: 1.6820, Best Val Loss: 1.6820\n",
      "Epoch 22/1000, Train Loss: 1.6306, Val Loss: 1.6633, Best Val Loss: 1.6633\n",
      "Epoch 23/1000, Train Loss: 1.6274, Val Loss: 1.6570, Best Val Loss: 1.6570\n",
      "Epoch 24/1000, Train Loss: 1.6218, Val Loss: 1.6489, Best Val Loss: 1.6489\n",
      "Epoch 25/1000, Train Loss: 1.6153, Val Loss: 1.6383, Best Val Loss: 1.6383\n",
      "Epoch 29/1000, Train Loss: 1.5899, Val Loss: 1.6075, Best Val Loss: 1.6075\n",
      "Epoch 30/1000, Train Loss: 1.5759, Val Loss: 1.6024, Best Val Loss: 1.6024\n",
      "Epoch 34/1000, Train Loss: 1.5567, Val Loss: 1.5732, Best Val Loss: 1.5732\n",
      "Epoch 35/1000, Train Loss: 1.5484, Val Loss: 1.5665, Best Val Loss: 1.5665\n",
      "Epoch 37/1000, Train Loss: 1.5317, Val Loss: 1.5529, Best Val Loss: 1.5529\n",
      "Epoch 39/1000, Train Loss: 1.5200, Val Loss: 1.5413, Best Val Loss: 1.5413\n",
      "Epoch 43/1000, Train Loss: 1.5058, Val Loss: 1.5359, Best Val Loss: 1.5359\n",
      "Epoch 48/1000, Train Loss: 1.4814, Val Loss: 1.5345, Best Val Loss: 1.5345\n",
      "Epoch 50/1000, Train Loss: 1.4792, Val Loss: 1.5173, Best Val Loss: 1.5173\n",
      "Epoch 55/1000, Train Loss: 1.4589, Val Loss: 1.5063, Best Val Loss: 1.5063\n",
      "Epoch 63/1000, Train Loss: 1.4467, Val Loss: 1.5013, Best Val Loss: 1.5013\n",
      "Epoch 68/1000, Train Loss: 1.4274, Val Loss: 1.4907, Best Val Loss: 1.4907\n",
      "Epoch 83/1000, Train Loss: 1.4088, Val Loss: 1.4816, Best Val Loss: 1.4816\n",
      "Epoch 86/1000, Train Loss: 1.3828, Val Loss: 1.4774, Best Val Loss: 1.4774\n",
      "Epoch 116/1000, Train Loss: 1.3347, Val Loss: 1.4768, Best Val Loss: 1.4768\n",
      "Epoch 117/1000, Train Loss: 1.3332, Val Loss: 1.4683, Best Val Loss: 1.4683\n",
      "Early stopping at epoch 167, Best Val Loss: 1.4683\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 143.6859, Val Loss: 5.5210, Best Val Loss: 5.5210\n",
      "Epoch 2/1000, Train Loss: 5.4611, Val Loss: 5.3948, Best Val Loss: 5.3948\n",
      "Epoch 3/1000, Train Loss: 5.2937, Val Loss: 5.1258, Best Val Loss: 5.1258\n",
      "Epoch 4/1000, Train Loss: 4.1154, Val Loss: 2.6976, Best Val Loss: 2.6976\n",
      "Epoch 5/1000, Train Loss: 2.5360, Val Loss: 2.4201, Best Val Loss: 2.4201\n",
      "Epoch 6/1000, Train Loss: 2.3044, Val Loss: 2.2240, Best Val Loss: 2.2240\n",
      "Epoch 7/1000, Train Loss: 2.1291, Val Loss: 2.0827, Best Val Loss: 2.0827\n",
      "Epoch 8/1000, Train Loss: 2.0081, Val Loss: 1.9962, Best Val Loss: 1.9962\n",
      "Epoch 9/1000, Train Loss: 1.9083, Val Loss: 1.8962, Best Val Loss: 1.8962\n",
      "Epoch 10/1000, Train Loss: 1.8423, Val Loss: 1.8267, Best Val Loss: 1.8267\n",
      "Epoch 11/1000, Train Loss: 1.8086, Val Loss: 1.7824, Best Val Loss: 1.7824\n",
      "Epoch 12/1000, Train Loss: 1.7708, Val Loss: 1.7717, Best Val Loss: 1.7717\n",
      "Epoch 14/1000, Train Loss: 1.7438, Val Loss: 1.7360, Best Val Loss: 1.7360\n",
      "Epoch 17/1000, Train Loss: 1.7019, Val Loss: 1.6854, Best Val Loss: 1.6854\n",
      "Epoch 19/1000, Train Loss: 1.6766, Val Loss: 1.6703, Best Val Loss: 1.6703\n",
      "Epoch 21/1000, Train Loss: 1.6721, Val Loss: 1.6562, Best Val Loss: 1.6562\n",
      "Epoch 23/1000, Train Loss: 1.6439, Val Loss: 1.6459, Best Val Loss: 1.6459\n",
      "Epoch 25/1000, Train Loss: 1.6533, Val Loss: 1.6290, Best Val Loss: 1.6290\n",
      "Epoch 26/1000, Train Loss: 1.6357, Val Loss: 1.6079, Best Val Loss: 1.6079\n",
      "Epoch 32/1000, Train Loss: 1.5970, Val Loss: 1.6001, Best Val Loss: 1.6001\n",
      "Epoch 36/1000, Train Loss: 1.5879, Val Loss: 1.5883, Best Val Loss: 1.5883\n",
      "Epoch 38/1000, Train Loss: 1.5612, Val Loss: 1.5847, Best Val Loss: 1.5847\n",
      "Epoch 39/1000, Train Loss: 1.5602, Val Loss: 1.5796, Best Val Loss: 1.5796\n",
      "Epoch 42/1000, Train Loss: 1.5480, Val Loss: 1.5666, Best Val Loss: 1.5666\n",
      "Epoch 47/1000, Train Loss: 1.5286, Val Loss: 1.5660, Best Val Loss: 1.5660\n",
      "Epoch 49/1000, Train Loss: 1.5176, Val Loss: 1.5651, Best Val Loss: 1.5651\n",
      "Epoch 50/1000, Train Loss: 1.5168, Val Loss: 1.5633, Best Val Loss: 1.5633\n",
      "Epoch 51/1000, Train Loss: 1.5167, Val Loss: 1.5388, Best Val Loss: 1.5388\n",
      "Epoch 54/1000, Train Loss: 1.4997, Val Loss: 1.5331, Best Val Loss: 1.5331\n",
      "Epoch 56/1000, Train Loss: 1.5103, Val Loss: 1.5271, Best Val Loss: 1.5271\n",
      "Epoch 61/1000, Train Loss: 1.4888, Val Loss: 1.5181, Best Val Loss: 1.5181\n",
      "Epoch 63/1000, Train Loss: 1.4895, Val Loss: 1.5054, Best Val Loss: 1.5054\n",
      "Epoch 65/1000, Train Loss: 1.4662, Val Loss: 1.4996, Best Val Loss: 1.4996\n",
      "Epoch 74/1000, Train Loss: 1.4570, Val Loss: 1.4966, Best Val Loss: 1.4966\n",
      "Epoch 81/1000, Train Loss: 1.4396, Val Loss: 1.4803, Best Val Loss: 1.4803\n",
      "Epoch 101/1000, Train Loss: 1.4000, Val Loss: 1.4763, Best Val Loss: 1.4763\n",
      "Epoch 106/1000, Train Loss: 1.3996, Val Loss: 1.4762, Best Val Loss: 1.4762\n",
      "Epoch 107/1000, Train Loss: 1.3885, Val Loss: 1.4675, Best Val Loss: 1.4675\n",
      "Epoch 135/1000, Train Loss: 1.3520, Val Loss: 1.4674, Best Val Loss: 1.4674\n",
      "Epoch 163/1000, Train Loss: 1.3212, Val Loss: 1.4674, Best Val Loss: 1.4674\n",
      "Epoch 195/1000, Train Loss: 1.2865, Val Loss: 1.4660, Best Val Loss: 1.4660\n",
      "Early stopping at epoch 245, Best Val Loss: 1.4660\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 126.3599, Val Loss: 5.5340, Best Val Loss: 5.5340\n",
      "Epoch 2/1000, Train Loss: 5.4846, Val Loss: 5.4271, Best Val Loss: 5.4271\n",
      "Epoch 3/1000, Train Loss: 5.3918, Val Loss: 5.3476, Best Val Loss: 5.3476\n",
      "Epoch 4/1000, Train Loss: 5.3157, Val Loss: 5.2774, Best Val Loss: 5.2774\n",
      "Epoch 5/1000, Train Loss: 5.2421, Val Loss: 5.1972, Best Val Loss: 5.1972\n",
      "Epoch 6/1000, Train Loss: 5.0579, Val Loss: 4.2504, Best Val Loss: 4.2504\n",
      "Epoch 7/1000, Train Loss: 2.4458, Val Loss: 2.2119, Best Val Loss: 2.2119\n",
      "Epoch 8/1000, Train Loss: 2.0963, Val Loss: 2.0334, Best Val Loss: 2.0334\n",
      "Epoch 9/1000, Train Loss: 1.9671, Val Loss: 1.9235, Best Val Loss: 1.9235\n",
      "Epoch 10/1000, Train Loss: 1.8899, Val Loss: 1.8962, Best Val Loss: 1.8962\n",
      "Epoch 11/1000, Train Loss: 1.8481, Val Loss: 1.8379, Best Val Loss: 1.8379\n",
      "Epoch 12/1000, Train Loss: 1.8138, Val Loss: 1.8113, Best Val Loss: 1.8113\n",
      "Epoch 14/1000, Train Loss: 1.7605, Val Loss: 1.7580, Best Val Loss: 1.7580\n",
      "Epoch 15/1000, Train Loss: 1.7471, Val Loss: 1.7331, Best Val Loss: 1.7331\n",
      "Epoch 18/1000, Train Loss: 1.6925, Val Loss: 1.6807, Best Val Loss: 1.6807\n",
      "Epoch 21/1000, Train Loss: 1.6680, Val Loss: 1.6799, Best Val Loss: 1.6799\n",
      "Epoch 23/1000, Train Loss: 1.6462, Val Loss: 1.6503, Best Val Loss: 1.6503\n",
      "Epoch 25/1000, Train Loss: 1.6308, Val Loss: 1.6354, Best Val Loss: 1.6354\n",
      "Epoch 26/1000, Train Loss: 1.6253, Val Loss: 1.6293, Best Val Loss: 1.6293\n",
      "Epoch 28/1000, Train Loss: 1.6061, Val Loss: 1.6011, Best Val Loss: 1.6011\n",
      "Epoch 38/1000, Train Loss: 1.5375, Val Loss: 1.5954, Best Val Loss: 1.5954\n",
      "Epoch 39/1000, Train Loss: 1.5397, Val Loss: 1.5586, Best Val Loss: 1.5586\n",
      "Epoch 46/1000, Train Loss: 1.4985, Val Loss: 1.5441, Best Val Loss: 1.5441\n",
      "Epoch 51/1000, Train Loss: 1.4862, Val Loss: 1.5196, Best Val Loss: 1.5196\n",
      "Epoch 54/1000, Train Loss: 1.4800, Val Loss: 1.5095, Best Val Loss: 1.5095\n",
      "Epoch 63/1000, Train Loss: 1.4566, Val Loss: 1.4993, Best Val Loss: 1.4993\n",
      "Epoch 72/1000, Train Loss: 1.4368, Val Loss: 1.4867, Best Val Loss: 1.4867\n",
      "Epoch 86/1000, Train Loss: 1.4083, Val Loss: 1.4815, Best Val Loss: 1.4815\n",
      "Epoch 93/1000, Train Loss: 1.3930, Val Loss: 1.4799, Best Val Loss: 1.4799\n",
      "Epoch 105/1000, Train Loss: 1.3656, Val Loss: 1.4789, Best Val Loss: 1.4789\n",
      "Epoch 122/1000, Train Loss: 1.3333, Val Loss: 1.4695, Best Val Loss: 1.4695\n",
      "Early stopping at epoch 172, Best Val Loss: 1.4695\n",
      "Epistemic Variance: 0.945110\n",
      "Aleatoric Variance: 12.521072\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n",
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           1.968\n",
      "  RMSE          2.774\n",
      "  MDAE          1.463\n",
      "  MARPD         1.719\n",
      "  R2            0.865\n",
      "  Correlation   0.931\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.028\n",
      "  Mean-absolute Calibration Error       0.025\n",
      "  Miscalibration Area                   0.025\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.035\n",
      "     Group Size: 0.56 -- Calibration Error: 0.029\n",
      "     Group Size: 1.00 -- Calibration Error: 0.025\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.039\n",
      "     Group Size: 0.56 -- Calibration Error: 0.031\n",
      "     Group Size: 1.00 -- Calibration Error: 0.028\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   3.670\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   2.289\n",
      "  CRPS                      1.424\n",
      "  Check Score               0.719\n",
      "  Interval Score            7.181\n",
      "{'accuracy': {'mae': 1.9676792836238646, 'rmse': np.float64(2.773567785734614), 'mdae': 1.4627147094726496, 'marpd': np.float64(1.7185099377019313), 'r2': 0.8652801027896064, 'corr': np.float64(0.9306695091136319)}, 'avg_calibration': {'rms_cal': np.float64(0.02780828360860038), 'ma_cal': np.float64(0.02499123698777641), 'miscal_area': np.float64(0.02523749543941734)}, 'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.34844444, 0.03539358, 0.03304696, 0.03232788, 0.0296994 ,\n",
      "       0.02891188, 0.02793847, 0.02747195, 0.02687606, 0.02499124]), 'adv_group_cali_stderr': array([0.04767901, 0.00314161, 0.00279926, 0.00207692, 0.00170819,\n",
      "       0.00157316, 0.00102544, 0.00093234, 0.0006677 , 0.        ])}, 'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.41463673, 0.03900413, 0.03656138, 0.0345026 , 0.03312302,\n",
      "       0.03130553, 0.03145238, 0.03002402, 0.02943856, 0.02780828]), 'adv_group_cali_stderr': array([0.0702711 , 0.00335498, 0.0020204 , 0.00269697, 0.00206363,\n",
      "       0.00172595, 0.00055567, 0.00055192, 0.00061252, 0.        ])}}, 'sharpness': {'sharp': np.float32(3.66963)}, 'scoring_rule': {'nll': np.float64(2.2885620125794), 'crps': np.float64(1.4242802756825077), 'check': np.float64(0.7191308256532791), 'interval': np.float64(7.18081122878677)}}\n",
      "coverage: 0.9508795539748149, MPIW: 10.326637509591155\n",
      "Run 6 with seed 8888\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 130.9563, Val Loss: 5.3921, Best Val Loss: 5.3921\n",
      "Epoch 2/1000, Train Loss: 5.3172, Val Loss: 5.2745, Best Val Loss: 5.2745\n",
      "Epoch 3/1000, Train Loss: 5.2490, Val Loss: 5.2304, Best Val Loss: 5.2304\n",
      "Epoch 4/1000, Train Loss: 5.2070, Val Loss: 5.1834, Best Val Loss: 5.1834\n",
      "Epoch 5/1000, Train Loss: 5.1395, Val Loss: 5.0758, Best Val Loss: 5.0758\n",
      "Epoch 6/1000, Train Loss: 4.6667, Val Loss: 3.6272, Best Val Loss: 3.6272\n",
      "Epoch 7/1000, Train Loss: 2.8213, Val Loss: 2.4416, Best Val Loss: 2.4416\n",
      "Epoch 8/1000, Train Loss: 2.2597, Val Loss: 2.1655, Best Val Loss: 2.1655\n",
      "Epoch 9/1000, Train Loss: 2.0632, Val Loss: 1.9960, Best Val Loss: 1.9960\n",
      "Epoch 10/1000, Train Loss: 1.9467, Val Loss: 1.9075, Best Val Loss: 1.9075\n",
      "Epoch 11/1000, Train Loss: 1.8778, Val Loss: 1.8486, Best Val Loss: 1.8486\n",
      "Epoch 13/1000, Train Loss: 1.7924, Val Loss: 1.8032, Best Val Loss: 1.8032\n",
      "Epoch 14/1000, Train Loss: 1.7779, Val Loss: 1.7778, Best Val Loss: 1.7778\n",
      "Epoch 16/1000, Train Loss: 1.7334, Val Loss: 1.7396, Best Val Loss: 1.7396\n",
      "Epoch 17/1000, Train Loss: 1.7080, Val Loss: 1.7125, Best Val Loss: 1.7125\n",
      "Epoch 19/1000, Train Loss: 1.6872, Val Loss: 1.7089, Best Val Loss: 1.7089\n",
      "Epoch 20/1000, Train Loss: 1.6736, Val Loss: 1.6545, Best Val Loss: 1.6545\n",
      "Epoch 21/1000, Train Loss: 1.6644, Val Loss: 1.6420, Best Val Loss: 1.6420\n",
      "Epoch 24/1000, Train Loss: 1.6418, Val Loss: 1.6372, Best Val Loss: 1.6372\n",
      "Epoch 26/1000, Train Loss: 1.6211, Val Loss: 1.6241, Best Val Loss: 1.6241\n",
      "Epoch 27/1000, Train Loss: 1.6113, Val Loss: 1.6206, Best Val Loss: 1.6206\n",
      "Epoch 30/1000, Train Loss: 1.5866, Val Loss: 1.6004, Best Val Loss: 1.6004\n",
      "Epoch 33/1000, Train Loss: 1.5666, Val Loss: 1.5927, Best Val Loss: 1.5927\n",
      "Epoch 35/1000, Train Loss: 1.5499, Val Loss: 1.5784, Best Val Loss: 1.5784\n",
      "Epoch 36/1000, Train Loss: 1.5454, Val Loss: 1.5682, Best Val Loss: 1.5682\n",
      "Epoch 37/1000, Train Loss: 1.5457, Val Loss: 1.5637, Best Val Loss: 1.5637\n",
      "Epoch 39/1000, Train Loss: 1.5217, Val Loss: 1.5487, Best Val Loss: 1.5487\n",
      "Epoch 45/1000, Train Loss: 1.4936, Val Loss: 1.5296, Best Val Loss: 1.5296\n",
      "Epoch 48/1000, Train Loss: 1.4795, Val Loss: 1.5264, Best Val Loss: 1.5264\n",
      "Epoch 52/1000, Train Loss: 1.4651, Val Loss: 1.5112, Best Val Loss: 1.5112\n",
      "Epoch 54/1000, Train Loss: 1.4545, Val Loss: 1.5049, Best Val Loss: 1.5049\n",
      "Epoch 59/1000, Train Loss: 1.4429, Val Loss: 1.4953, Best Val Loss: 1.4953\n",
      "Epoch 62/1000, Train Loss: 1.4340, Val Loss: 1.4939, Best Val Loss: 1.4939\n",
      "Epoch 70/1000, Train Loss: 1.4120, Val Loss: 1.4867, Best Val Loss: 1.4867\n",
      "Epoch 73/1000, Train Loss: 1.4089, Val Loss: 1.4694, Best Val Loss: 1.4694\n",
      "Epoch 98/1000, Train Loss: 1.3532, Val Loss: 1.4583, Best Val Loss: 1.4583\n",
      "Early stopping at epoch 148, Best Val Loss: 1.4583\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 121.9328, Val Loss: 5.5523, Best Val Loss: 5.5523\n",
      "Epoch 2/1000, Train Loss: 5.4669, Val Loss: 5.3833, Best Val Loss: 5.3833\n",
      "Epoch 3/1000, Train Loss: 5.3293, Val Loss: 5.2783, Best Val Loss: 5.2783\n",
      "Epoch 4/1000, Train Loss: 5.2535, Val Loss: 5.2298, Best Val Loss: 5.2298\n",
      "Epoch 5/1000, Train Loss: 5.2051, Val Loss: 5.1810, Best Val Loss: 5.1810\n",
      "Epoch 6/1000, Train Loss: 5.1404, Val Loss: 5.0834, Best Val Loss: 5.0834\n",
      "Epoch 7/1000, Train Loss: 4.6444, Val Loss: 3.1329, Best Val Loss: 3.1329\n",
      "Epoch 8/1000, Train Loss: 2.5378, Val Loss: 2.2402, Best Val Loss: 2.2402\n",
      "Epoch 9/1000, Train Loss: 2.1238, Val Loss: 2.0480, Best Val Loss: 2.0480\n",
      "Epoch 10/1000, Train Loss: 1.9814, Val Loss: 1.9512, Best Val Loss: 1.9512\n",
      "Epoch 11/1000, Train Loss: 1.8984, Val Loss: 1.8825, Best Val Loss: 1.8825\n",
      "Epoch 12/1000, Train Loss: 1.8493, Val Loss: 1.8323, Best Val Loss: 1.8323\n",
      "Epoch 13/1000, Train Loss: 1.8140, Val Loss: 1.8135, Best Val Loss: 1.8135\n",
      "Epoch 15/1000, Train Loss: 1.7608, Val Loss: 1.8091, Best Val Loss: 1.8091\n",
      "Epoch 16/1000, Train Loss: 1.7413, Val Loss: 1.7721, Best Val Loss: 1.7721\n",
      "Epoch 17/1000, Train Loss: 1.7338, Val Loss: 1.7257, Best Val Loss: 1.7257\n",
      "Epoch 19/1000, Train Loss: 1.7002, Val Loss: 1.7123, Best Val Loss: 1.7123\n",
      "Epoch 20/1000, Train Loss: 1.6850, Val Loss: 1.6972, Best Val Loss: 1.6972\n",
      "Epoch 22/1000, Train Loss: 1.6645, Val Loss: 1.6620, Best Val Loss: 1.6620\n",
      "Epoch 23/1000, Train Loss: 1.6510, Val Loss: 1.6516, Best Val Loss: 1.6516\n",
      "Epoch 24/1000, Train Loss: 1.6501, Val Loss: 1.6323, Best Val Loss: 1.6323\n",
      "Epoch 31/1000, Train Loss: 1.5969, Val Loss: 1.6089, Best Val Loss: 1.6089\n",
      "Epoch 32/1000, Train Loss: 1.5810, Val Loss: 1.5782, Best Val Loss: 1.5782\n",
      "Epoch 38/1000, Train Loss: 1.5423, Val Loss: 1.5665, Best Val Loss: 1.5665\n",
      "Epoch 39/1000, Train Loss: 1.5408, Val Loss: 1.5571, Best Val Loss: 1.5571\n",
      "Epoch 48/1000, Train Loss: 1.4931, Val Loss: 1.5428, Best Val Loss: 1.5428\n",
      "Epoch 49/1000, Train Loss: 1.4943, Val Loss: 1.5380, Best Val Loss: 1.5380\n",
      "Epoch 50/1000, Train Loss: 1.4961, Val Loss: 1.5277, Best Val Loss: 1.5277\n",
      "Epoch 51/1000, Train Loss: 1.4818, Val Loss: 1.5221, Best Val Loss: 1.5221\n",
      "Epoch 56/1000, Train Loss: 1.4701, Val Loss: 1.5057, Best Val Loss: 1.5057\n",
      "Epoch 63/1000, Train Loss: 1.4452, Val Loss: 1.5025, Best Val Loss: 1.5025\n",
      "Epoch 66/1000, Train Loss: 1.4429, Val Loss: 1.4939, Best Val Loss: 1.4939\n",
      "Epoch 67/1000, Train Loss: 1.4330, Val Loss: 1.4827, Best Val Loss: 1.4827\n",
      "Epoch 77/1000, Train Loss: 1.4156, Val Loss: 1.4689, Best Val Loss: 1.4689\n",
      "Epoch 90/1000, Train Loss: 1.3849, Val Loss: 1.4538, Best Val Loss: 1.4538\n",
      "Early stopping at epoch 140, Best Val Loss: 1.4538\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 137.8711, Val Loss: 5.4876, Best Val Loss: 5.4876\n",
      "Epoch 2/1000, Train Loss: 5.4447, Val Loss: 5.3947, Best Val Loss: 5.3947\n",
      "Epoch 3/1000, Train Loss: 5.3567, Val Loss: 5.3021, Best Val Loss: 5.3021\n",
      "Epoch 4/1000, Train Loss: 5.2270, Val Loss: 5.1138, Best Val Loss: 5.1138\n",
      "Epoch 5/1000, Train Loss: 4.5396, Val Loss: 2.6069, Best Val Loss: 2.6069\n",
      "Epoch 6/1000, Train Loss: 2.4074, Val Loss: 2.2747, Best Val Loss: 2.2747\n",
      "Epoch 7/1000, Train Loss: 2.1686, Val Loss: 2.0968, Best Val Loss: 2.0968\n",
      "Epoch 8/1000, Train Loss: 2.0197, Val Loss: 1.9631, Best Val Loss: 1.9631\n",
      "Epoch 9/1000, Train Loss: 1.9215, Val Loss: 1.8950, Best Val Loss: 1.8950\n",
      "Epoch 10/1000, Train Loss: 1.8567, Val Loss: 1.8647, Best Val Loss: 1.8647\n",
      "Epoch 11/1000, Train Loss: 1.8205, Val Loss: 1.8220, Best Val Loss: 1.8220\n",
      "Epoch 12/1000, Train Loss: 1.7794, Val Loss: 1.8046, Best Val Loss: 1.8046\n",
      "Epoch 14/1000, Train Loss: 1.7446, Val Loss: 1.7724, Best Val Loss: 1.7724\n",
      "Epoch 15/1000, Train Loss: 1.7194, Val Loss: 1.7349, Best Val Loss: 1.7349\n",
      "Epoch 16/1000, Train Loss: 1.7096, Val Loss: 1.7203, Best Val Loss: 1.7203\n",
      "Epoch 17/1000, Train Loss: 1.6972, Val Loss: 1.7059, Best Val Loss: 1.7059\n",
      "Epoch 18/1000, Train Loss: 1.6859, Val Loss: 1.6805, Best Val Loss: 1.6805\n",
      "Epoch 23/1000, Train Loss: 1.6471, Val Loss: 1.6504, Best Val Loss: 1.6504\n",
      "Epoch 24/1000, Train Loss: 1.6352, Val Loss: 1.6490, Best Val Loss: 1.6490\n",
      "Epoch 25/1000, Train Loss: 1.6424, Val Loss: 1.6442, Best Val Loss: 1.6442\n",
      "Epoch 26/1000, Train Loss: 1.6229, Val Loss: 1.6336, Best Val Loss: 1.6336\n",
      "Epoch 28/1000, Train Loss: 1.6091, Val Loss: 1.6127, Best Val Loss: 1.6127\n",
      "Epoch 29/1000, Train Loss: 1.6081, Val Loss: 1.6118, Best Val Loss: 1.6118\n",
      "Epoch 31/1000, Train Loss: 1.5910, Val Loss: 1.5955, Best Val Loss: 1.5955\n",
      "Epoch 36/1000, Train Loss: 1.5729, Val Loss: 1.5919, Best Val Loss: 1.5919\n",
      "Epoch 37/1000, Train Loss: 1.5691, Val Loss: 1.5717, Best Val Loss: 1.5717\n",
      "Epoch 45/1000, Train Loss: 1.5359, Val Loss: 1.5477, Best Val Loss: 1.5477\n",
      "Epoch 47/1000, Train Loss: 1.5271, Val Loss: 1.5385, Best Val Loss: 1.5385\n",
      "Epoch 50/1000, Train Loss: 1.5117, Val Loss: 1.5206, Best Val Loss: 1.5206\n",
      "Epoch 67/1000, Train Loss: 1.4687, Val Loss: 1.5159, Best Val Loss: 1.5159\n",
      "Epoch 69/1000, Train Loss: 1.4690, Val Loss: 1.5003, Best Val Loss: 1.5003\n",
      "Epoch 75/1000, Train Loss: 1.4486, Val Loss: 1.4857, Best Val Loss: 1.4857\n",
      "Epoch 80/1000, Train Loss: 1.4356, Val Loss: 1.4787, Best Val Loss: 1.4787\n",
      "Epoch 84/1000, Train Loss: 1.4249, Val Loss: 1.4748, Best Val Loss: 1.4748\n",
      "Epoch 101/1000, Train Loss: 1.3993, Val Loss: 1.4677, Best Val Loss: 1.4677\n",
      "Early stopping at epoch 151, Best Val Loss: 1.4677\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 129.1054, Val Loss: 5.4260, Best Val Loss: 5.4260\n",
      "Epoch 2/1000, Train Loss: 5.3810, Val Loss: 5.3378, Best Val Loss: 5.3378\n",
      "Epoch 3/1000, Train Loss: 5.3079, Val Loss: 5.2741, Best Val Loss: 5.2741\n",
      "Epoch 4/1000, Train Loss: 5.2514, Val Loss: 5.2299, Best Val Loss: 5.2299\n",
      "Epoch 5/1000, Train Loss: 5.2106, Val Loss: 5.1916, Best Val Loss: 5.1916\n",
      "Epoch 6/1000, Train Loss: 5.1692, Val Loss: 5.1464, Best Val Loss: 5.1464\n",
      "Epoch 7/1000, Train Loss: 5.1095, Val Loss: 5.0589, Best Val Loss: 5.0589\n",
      "Epoch 8/1000, Train Loss: 4.0899, Val Loss: 2.2137, Best Val Loss: 2.2137\n",
      "Epoch 9/1000, Train Loss: 2.0650, Val Loss: 2.0064, Best Val Loss: 2.0064\n",
      "Epoch 10/1000, Train Loss: 1.9241, Val Loss: 1.9232, Best Val Loss: 1.9232\n",
      "Epoch 11/1000, Train Loss: 1.8530, Val Loss: 1.8588, Best Val Loss: 1.8588\n",
      "Epoch 12/1000, Train Loss: 1.8065, Val Loss: 1.8130, Best Val Loss: 1.8130\n",
      "Epoch 14/1000, Train Loss: 1.7515, Val Loss: 1.7608, Best Val Loss: 1.7608\n",
      "Epoch 15/1000, Train Loss: 1.7335, Val Loss: 1.7498, Best Val Loss: 1.7498\n",
      "Epoch 16/1000, Train Loss: 1.7278, Val Loss: 1.7214, Best Val Loss: 1.7214\n",
      "Epoch 19/1000, Train Loss: 1.6914, Val Loss: 1.6831, Best Val Loss: 1.6831\n",
      "Epoch 21/1000, Train Loss: 1.6576, Val Loss: 1.6542, Best Val Loss: 1.6542\n",
      "Epoch 23/1000, Train Loss: 1.6427, Val Loss: 1.6403, Best Val Loss: 1.6403\n",
      "Epoch 27/1000, Train Loss: 1.6172, Val Loss: 1.6166, Best Val Loss: 1.6166\n",
      "Epoch 29/1000, Train Loss: 1.5956, Val Loss: 1.5940, Best Val Loss: 1.5940\n",
      "Epoch 34/1000, Train Loss: 1.5571, Val Loss: 1.5862, Best Val Loss: 1.5862\n",
      "Epoch 35/1000, Train Loss: 1.5557, Val Loss: 1.5625, Best Val Loss: 1.5625\n",
      "Epoch 47/1000, Train Loss: 1.5047, Val Loss: 1.5446, Best Val Loss: 1.5446\n",
      "Epoch 49/1000, Train Loss: 1.4917, Val Loss: 1.5444, Best Val Loss: 1.5444\n",
      "Epoch 52/1000, Train Loss: 1.4766, Val Loss: 1.5440, Best Val Loss: 1.5440\n",
      "Epoch 53/1000, Train Loss: 1.4820, Val Loss: 1.5272, Best Val Loss: 1.5272\n",
      "Epoch 54/1000, Train Loss: 1.4708, Val Loss: 1.5175, Best Val Loss: 1.5175\n",
      "Epoch 59/1000, Train Loss: 1.4632, Val Loss: 1.5163, Best Val Loss: 1.5163\n",
      "Epoch 65/1000, Train Loss: 1.4477, Val Loss: 1.4986, Best Val Loss: 1.4986\n",
      "Epoch 70/1000, Train Loss: 1.4265, Val Loss: 1.4906, Best Val Loss: 1.4906\n",
      "Epoch 91/1000, Train Loss: 1.3852, Val Loss: 1.4679, Best Val Loss: 1.4679\n",
      "Early stopping at epoch 141, Best Val Loss: 1.4679\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 143.9459, Val Loss: 5.5488, Best Val Loss: 5.5488\n",
      "Epoch 2/1000, Train Loss: 5.4986, Val Loss: 5.4424, Best Val Loss: 5.4424\n",
      "Epoch 3/1000, Train Loss: 5.4105, Val Loss: 5.3685, Best Val Loss: 5.3685\n",
      "Epoch 4/1000, Train Loss: 5.3378, Val Loss: 5.3007, Best Val Loss: 5.3007\n",
      "Epoch 5/1000, Train Loss: 5.2681, Val Loss: 5.2329, Best Val Loss: 5.2329\n",
      "Epoch 6/1000, Train Loss: 5.1850, Val Loss: 5.1184, Best Val Loss: 5.1184\n",
      "Epoch 7/1000, Train Loss: 4.4237, Val Loss: 2.5921, Best Val Loss: 2.5921\n",
      "Epoch 8/1000, Train Loss: 2.3290, Val Loss: 2.1888, Best Val Loss: 2.1888\n",
      "Epoch 9/1000, Train Loss: 2.0934, Val Loss: 2.0539, Best Val Loss: 2.0539\n",
      "Epoch 10/1000, Train Loss: 1.9712, Val Loss: 1.9312, Best Val Loss: 1.9312\n",
      "Epoch 11/1000, Train Loss: 1.8915, Val Loss: 1.8758, Best Val Loss: 1.8758\n",
      "Epoch 12/1000, Train Loss: 1.8462, Val Loss: 1.8244, Best Val Loss: 1.8244\n",
      "Epoch 13/1000, Train Loss: 1.8099, Val Loss: 1.8218, Best Val Loss: 1.8218\n",
      "Epoch 14/1000, Train Loss: 1.7835, Val Loss: 1.8049, Best Val Loss: 1.8049\n",
      "Epoch 15/1000, Train Loss: 1.7635, Val Loss: 1.7649, Best Val Loss: 1.7649\n",
      "Epoch 16/1000, Train Loss: 1.7410, Val Loss: 1.7649, Best Val Loss: 1.7649\n",
      "Epoch 17/1000, Train Loss: 1.7270, Val Loss: 1.7171, Best Val Loss: 1.7171\n",
      "Epoch 20/1000, Train Loss: 1.7036, Val Loss: 1.6858, Best Val Loss: 1.6858\n",
      "Epoch 21/1000, Train Loss: 1.6941, Val Loss: 1.6829, Best Val Loss: 1.6829\n",
      "Epoch 22/1000, Train Loss: 1.6648, Val Loss: 1.6481, Best Val Loss: 1.6481\n",
      "Epoch 28/1000, Train Loss: 1.6258, Val Loss: 1.6124, Best Val Loss: 1.6124\n",
      "Epoch 31/1000, Train Loss: 1.6014, Val Loss: 1.6018, Best Val Loss: 1.6018\n",
      "Epoch 34/1000, Train Loss: 1.5741, Val Loss: 1.5770, Best Val Loss: 1.5770\n",
      "Epoch 43/1000, Train Loss: 1.5280, Val Loss: 1.5737, Best Val Loss: 1.5737\n",
      "Epoch 45/1000, Train Loss: 1.5250, Val Loss: 1.5623, Best Val Loss: 1.5623\n",
      "Epoch 48/1000, Train Loss: 1.5115, Val Loss: 1.5429, Best Val Loss: 1.5429\n",
      "Epoch 60/1000, Train Loss: 1.4688, Val Loss: 1.5378, Best Val Loss: 1.5378\n",
      "Epoch 61/1000, Train Loss: 1.4700, Val Loss: 1.5270, Best Val Loss: 1.5270\n",
      "Epoch 64/1000, Train Loss: 1.4544, Val Loss: 1.5001, Best Val Loss: 1.5001\n",
      "Epoch 75/1000, Train Loss: 1.4360, Val Loss: 1.5001, Best Val Loss: 1.5001\n",
      "Epoch 80/1000, Train Loss: 1.4228, Val Loss: 1.4930, Best Val Loss: 1.4930\n",
      "Epoch 81/1000, Train Loss: 1.4244, Val Loss: 1.4743, Best Val Loss: 1.4743\n",
      "Epoch 90/1000, Train Loss: 1.3987, Val Loss: 1.4740, Best Val Loss: 1.4740\n",
      "Epoch 99/1000, Train Loss: 1.3873, Val Loss: 1.4739, Best Val Loss: 1.4739\n",
      "Epoch 106/1000, Train Loss: 1.3772, Val Loss: 1.4733, Best Val Loss: 1.4733\n",
      "Early stopping at epoch 156, Best Val Loss: 1.4733\n",
      "Epistemic Variance: 1.046975\n",
      "Aleatoric Variance: 8.330327\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n",
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           1.989\n",
      "  RMSE          2.786\n",
      "  MDAE          1.473\n",
      "  MARPD         1.736\n",
      "  R2            0.864\n",
      "  Correlation   0.930\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.039\n",
      "  Mean-absolute Calibration Error       0.034\n",
      "  Miscalibration Area                   0.035\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.047\n",
      "     Group Size: 0.56 -- Calibration Error: 0.038\n",
      "     Group Size: 1.00 -- Calibration Error: 0.034\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.051\n",
      "     Group Size: 0.56 -- Calibration Error: 0.043\n",
      "     Group Size: 1.00 -- Calibration Error: 0.039\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   3.062\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   2.291\n",
      "  CRPS                      1.424\n",
      "  Check Score               0.719\n",
      "  Interval Score            7.138\n",
      "{'accuracy': {'mae': 1.9887849853722148, 'rmse': np.float64(2.7856598516269733), 'mdae': 1.4726665283203175, 'marpd': np.float64(1.7362085949570398), 'r2': 0.864102851603441, 'corr': np.float64(0.9299197517497786)}, 'avg_calibration': {'rms_cal': np.float64(0.039199323337588646), 'ma_cal': np.float64(0.034438948749243824), 'miscal_area': np.float64(0.034780794527152596)}, 'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.372     , 0.04672286, 0.04229955, 0.04033566, 0.0397087 ,\n",
      "       0.03753159, 0.03677531, 0.03704518, 0.03586168, 0.03443895]), 'adv_group_cali_stderr': array([6.07719307e-02, 4.56522268e-03, 3.39528173e-03, 2.61546894e-03,\n",
      "       2.01198367e-03, 1.33043666e-03, 8.45798741e-04, 8.65613338e-04,\n",
      "       4.43836227e-04, 7.31423639e-18])}, 'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.42595252, 0.05109502, 0.04740434, 0.04555205, 0.04323545,\n",
      "       0.04309516, 0.04191612, 0.04136559, 0.04080661, 0.03919932]), 'adv_group_cali_stderr': array([0.09682369, 0.00640968, 0.00180529, 0.00208276, 0.00226645,\n",
      "       0.00168938, 0.00097766, 0.0008678 , 0.00046597, 0.        ])}}, 'sharpness': {'sharp': np.float32(3.062238)}, 'scoring_rule': {'nll': np.float64(2.291441464938911), 'crps': np.float64(1.4238628354623886), 'check': np.float64(0.7189421667238983), 'interval': np.float64(7.137835663359023)}}\n",
      "coverage: 0.9598192828991637, MPIW: 10.735786063378074\n",
      "Run 7 with seed 9876\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 136.0894, Val Loss: 5.3993, Best Val Loss: 5.3993\n",
      "Epoch 2/1000, Train Loss: 5.2945, Val Loss: 5.2397, Best Val Loss: 5.2397\n",
      "Epoch 3/1000, Train Loss: 5.2019, Val Loss: 5.1625, Best Val Loss: 5.1625\n",
      "Epoch 4/1000, Train Loss: 5.0778, Val Loss: 4.9039, Best Val Loss: 4.9039\n",
      "Epoch 5/1000, Train Loss: 3.6410, Val Loss: 2.8047, Best Val Loss: 2.8047\n",
      "Epoch 6/1000, Train Loss: 2.5437, Val Loss: 2.3333, Best Val Loss: 2.3333\n",
      "Epoch 7/1000, Train Loss: 2.2099, Val Loss: 2.1288, Best Val Loss: 2.1288\n",
      "Epoch 8/1000, Train Loss: 2.0483, Val Loss: 2.0200, Best Val Loss: 2.0200\n",
      "Epoch 9/1000, Train Loss: 1.9501, Val Loss: 1.9537, Best Val Loss: 1.9537\n",
      "Epoch 10/1000, Train Loss: 1.8939, Val Loss: 1.9033, Best Val Loss: 1.9033\n",
      "Epoch 11/1000, Train Loss: 1.8465, Val Loss: 1.8470, Best Val Loss: 1.8470\n",
      "Epoch 12/1000, Train Loss: 1.8158, Val Loss: 1.8103, Best Val Loss: 1.8103\n",
      "Epoch 13/1000, Train Loss: 1.7845, Val Loss: 1.7844, Best Val Loss: 1.7844\n",
      "Epoch 15/1000, Train Loss: 1.7408, Val Loss: 1.7541, Best Val Loss: 1.7541\n",
      "Epoch 16/1000, Train Loss: 1.7309, Val Loss: 1.7240, Best Val Loss: 1.7240\n",
      "Epoch 17/1000, Train Loss: 1.7146, Val Loss: 1.7108, Best Val Loss: 1.7108\n",
      "Epoch 22/1000, Train Loss: 1.6570, Val Loss: 1.6541, Best Val Loss: 1.6541\n",
      "Epoch 26/1000, Train Loss: 1.6398, Val Loss: 1.6412, Best Val Loss: 1.6412\n",
      "Epoch 29/1000, Train Loss: 1.6067, Val Loss: 1.6403, Best Val Loss: 1.6403\n",
      "Epoch 30/1000, Train Loss: 1.6268, Val Loss: 1.6365, Best Val Loss: 1.6365\n",
      "Epoch 31/1000, Train Loss: 1.5926, Val Loss: 1.6239, Best Val Loss: 1.6239\n",
      "Epoch 33/1000, Train Loss: 1.5896, Val Loss: 1.6050, Best Val Loss: 1.6050\n",
      "Epoch 38/1000, Train Loss: 1.5552, Val Loss: 1.5578, Best Val Loss: 1.5578\n",
      "Epoch 48/1000, Train Loss: 1.5094, Val Loss: 1.5498, Best Val Loss: 1.5498\n",
      "Epoch 49/1000, Train Loss: 1.5082, Val Loss: 1.5453, Best Val Loss: 1.5453\n",
      "Epoch 51/1000, Train Loss: 1.5103, Val Loss: 1.5280, Best Val Loss: 1.5280\n",
      "Epoch 59/1000, Train Loss: 1.4844, Val Loss: 1.5199, Best Val Loss: 1.5199\n",
      "Epoch 62/1000, Train Loss: 1.4720, Val Loss: 1.5183, Best Val Loss: 1.5183\n",
      "Epoch 64/1000, Train Loss: 1.4644, Val Loss: 1.5022, Best Val Loss: 1.5022\n",
      "Epoch 65/1000, Train Loss: 1.4654, Val Loss: 1.5006, Best Val Loss: 1.5006\n",
      "Epoch 77/1000, Train Loss: 1.4398, Val Loss: 1.4877, Best Val Loss: 1.4877\n",
      "Epoch 88/1000, Train Loss: 1.4130, Val Loss: 1.4601, Best Val Loss: 1.4601\n",
      "Epoch 106/1000, Train Loss: 1.3814, Val Loss: 1.4536, Best Val Loss: 1.4536\n",
      "Early stopping at epoch 156, Best Val Loss: 1.4536\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 139.8915, Val Loss: 5.5170, Best Val Loss: 5.5170\n",
      "Epoch 2/1000, Train Loss: 5.4768, Val Loss: 5.4337, Best Val Loss: 5.4337\n",
      "Epoch 3/1000, Train Loss: 5.3975, Val Loss: 5.3556, Best Val Loss: 5.3556\n",
      "Epoch 4/1000, Train Loss: 5.3196, Val Loss: 5.2813, Best Val Loss: 5.2813\n",
      "Epoch 5/1000, Train Loss: 5.2453, Val Loss: 5.2091, Best Val Loss: 5.2091\n",
      "Epoch 6/1000, Train Loss: 5.1712, Val Loss: 5.1289, Best Val Loss: 5.1289\n",
      "Epoch 7/1000, Train Loss: 5.0576, Val Loss: 4.9383, Best Val Loss: 4.9383\n",
      "Epoch 8/1000, Train Loss: 3.2347, Val Loss: 2.2289, Best Val Loss: 2.2289\n",
      "Epoch 9/1000, Train Loss: 2.0982, Val Loss: 2.0327, Best Val Loss: 2.0327\n",
      "Epoch 10/1000, Train Loss: 1.9666, Val Loss: 1.9180, Best Val Loss: 1.9180\n",
      "Epoch 11/1000, Train Loss: 1.8835, Val Loss: 1.8495, Best Val Loss: 1.8495\n",
      "Epoch 13/1000, Train Loss: 1.7989, Val Loss: 1.8024, Best Val Loss: 1.8024\n",
      "Epoch 14/1000, Train Loss: 1.7697, Val Loss: 1.7754, Best Val Loss: 1.7754\n",
      "Epoch 16/1000, Train Loss: 1.7326, Val Loss: 1.7323, Best Val Loss: 1.7323\n",
      "Epoch 17/1000, Train Loss: 1.7233, Val Loss: 1.7276, Best Val Loss: 1.7276\n",
      "Epoch 19/1000, Train Loss: 1.7000, Val Loss: 1.6945, Best Val Loss: 1.6945\n",
      "Epoch 22/1000, Train Loss: 1.6700, Val Loss: 1.6843, Best Val Loss: 1.6843\n",
      "Epoch 25/1000, Train Loss: 1.6361, Val Loss: 1.6522, Best Val Loss: 1.6522\n",
      "Epoch 27/1000, Train Loss: 1.6322, Val Loss: 1.6368, Best Val Loss: 1.6368\n",
      "Epoch 29/1000, Train Loss: 1.6031, Val Loss: 1.5957, Best Val Loss: 1.5957\n",
      "Epoch 30/1000, Train Loss: 1.5993, Val Loss: 1.5881, Best Val Loss: 1.5881\n",
      "Epoch 34/1000, Train Loss: 1.5691, Val Loss: 1.5747, Best Val Loss: 1.5747\n",
      "Epoch 41/1000, Train Loss: 1.5308, Val Loss: 1.5607, Best Val Loss: 1.5607\n",
      "Epoch 42/1000, Train Loss: 1.5243, Val Loss: 1.5402, Best Val Loss: 1.5402\n",
      "Epoch 46/1000, Train Loss: 1.5057, Val Loss: 1.5332, Best Val Loss: 1.5332\n",
      "Epoch 48/1000, Train Loss: 1.5019, Val Loss: 1.5174, Best Val Loss: 1.5174\n",
      "Epoch 58/1000, Train Loss: 1.4738, Val Loss: 1.5137, Best Val Loss: 1.5137\n",
      "Epoch 64/1000, Train Loss: 1.4525, Val Loss: 1.5016, Best Val Loss: 1.5016\n",
      "Epoch 66/1000, Train Loss: 1.4507, Val Loss: 1.4826, Best Val Loss: 1.4826\n",
      "Epoch 80/1000, Train Loss: 1.4084, Val Loss: 1.4679, Best Val Loss: 1.4679\n",
      "Epoch 120/1000, Train Loss: 1.3385, Val Loss: 1.4630, Best Val Loss: 1.4630\n",
      "Early stopping at epoch 170, Best Val Loss: 1.4630\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 135.7790, Val Loss: 5.5394, Best Val Loss: 5.5394\n",
      "Epoch 2/1000, Train Loss: 5.4544, Val Loss: 5.3730, Best Val Loss: 5.3730\n",
      "Epoch 3/1000, Train Loss: 5.3297, Val Loss: 5.2918, Best Val Loss: 5.2918\n",
      "Epoch 4/1000, Train Loss: 5.2671, Val Loss: 5.2417, Best Val Loss: 5.2417\n",
      "Epoch 5/1000, Train Loss: 5.2183, Val Loss: 5.1922, Best Val Loss: 5.1922\n",
      "Epoch 6/1000, Train Loss: 5.1580, Val Loss: 5.1143, Best Val Loss: 5.1143\n",
      "Epoch 7/1000, Train Loss: 4.6695, Val Loss: 2.4105, Best Val Loss: 2.4105\n",
      "Epoch 8/1000, Train Loss: 2.1617, Val Loss: 2.0404, Best Val Loss: 2.0404\n",
      "Epoch 9/1000, Train Loss: 1.9641, Val Loss: 1.9397, Best Val Loss: 1.9397\n",
      "Epoch 10/1000, Train Loss: 1.8856, Val Loss: 1.8863, Best Val Loss: 1.8863\n",
      "Epoch 11/1000, Train Loss: 1.8236, Val Loss: 1.8299, Best Val Loss: 1.8299\n",
      "Epoch 12/1000, Train Loss: 1.7963, Val Loss: 1.7907, Best Val Loss: 1.7907\n",
      "Epoch 13/1000, Train Loss: 1.7543, Val Loss: 1.7622, Best Val Loss: 1.7622\n",
      "Epoch 15/1000, Train Loss: 1.7128, Val Loss: 1.7222, Best Val Loss: 1.7222\n",
      "Epoch 16/1000, Train Loss: 1.7074, Val Loss: 1.7181, Best Val Loss: 1.7181\n",
      "Epoch 17/1000, Train Loss: 1.6892, Val Loss: 1.6936, Best Val Loss: 1.6936\n",
      "Epoch 18/1000, Train Loss: 1.6818, Val Loss: 1.6692, Best Val Loss: 1.6692\n",
      "Epoch 20/1000, Train Loss: 1.6594, Val Loss: 1.6521, Best Val Loss: 1.6521\n",
      "Epoch 23/1000, Train Loss: 1.6316, Val Loss: 1.6456, Best Val Loss: 1.6456\n",
      "Epoch 30/1000, Train Loss: 1.5807, Val Loss: 1.5992, Best Val Loss: 1.5992\n",
      "Epoch 33/1000, Train Loss: 1.5642, Val Loss: 1.5793, Best Val Loss: 1.5793\n",
      "Epoch 34/1000, Train Loss: 1.5509, Val Loss: 1.5631, Best Val Loss: 1.5631\n",
      "Epoch 40/1000, Train Loss: 1.5315, Val Loss: 1.5547, Best Val Loss: 1.5547\n",
      "Epoch 43/1000, Train Loss: 1.5254, Val Loss: 1.5512, Best Val Loss: 1.5512\n",
      "Epoch 46/1000, Train Loss: 1.5050, Val Loss: 1.5451, Best Val Loss: 1.5451\n",
      "Epoch 49/1000, Train Loss: 1.4964, Val Loss: 1.5370, Best Val Loss: 1.5370\n",
      "Epoch 51/1000, Train Loss: 1.4893, Val Loss: 1.5310, Best Val Loss: 1.5310\n",
      "Epoch 53/1000, Train Loss: 1.4833, Val Loss: 1.5271, Best Val Loss: 1.5271\n",
      "Epoch 54/1000, Train Loss: 1.4831, Val Loss: 1.5192, Best Val Loss: 1.5192\n",
      "Epoch 56/1000, Train Loss: 1.4721, Val Loss: 1.5099, Best Val Loss: 1.5099\n",
      "Epoch 64/1000, Train Loss: 1.4586, Val Loss: 1.4933, Best Val Loss: 1.4933\n",
      "Epoch 66/1000, Train Loss: 1.4493, Val Loss: 1.4825, Best Val Loss: 1.4825\n",
      "Epoch 79/1000, Train Loss: 1.4087, Val Loss: 1.4798, Best Val Loss: 1.4798\n",
      "Epoch 88/1000, Train Loss: 1.3960, Val Loss: 1.4593, Best Val Loss: 1.4593\n",
      "Epoch 115/1000, Train Loss: 1.3465, Val Loss: 1.4542, Best Val Loss: 1.4542\n",
      "Early stopping at epoch 165, Best Val Loss: 1.4542\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 154.2416, Val Loss: 5.5701, Best Val Loss: 5.5701\n",
      "Epoch 2/1000, Train Loss: 5.4883, Val Loss: 5.4120, Best Val Loss: 5.4120\n",
      "Epoch 3/1000, Train Loss: 5.3446, Val Loss: 5.2431, Best Val Loss: 5.2431\n",
      "Epoch 4/1000, Train Loss: 4.9841, Val Loss: 4.3988, Best Val Loss: 4.3988\n",
      "Epoch 5/1000, Train Loss: 2.8749, Val Loss: 2.5235, Best Val Loss: 2.5235\n",
      "Epoch 6/1000, Train Loss: 2.3862, Val Loss: 2.2899, Best Val Loss: 2.2899\n",
      "Epoch 7/1000, Train Loss: 2.1863, Val Loss: 2.1215, Best Val Loss: 2.1215\n",
      "Epoch 8/1000, Train Loss: 2.0470, Val Loss: 2.0001, Best Val Loss: 2.0001\n",
      "Epoch 9/1000, Train Loss: 1.9547, Val Loss: 1.9385, Best Val Loss: 1.9385\n",
      "Epoch 10/1000, Train Loss: 1.8953, Val Loss: 1.9078, Best Val Loss: 1.9078\n",
      "Epoch 11/1000, Train Loss: 1.8544, Val Loss: 1.8593, Best Val Loss: 1.8593\n",
      "Epoch 12/1000, Train Loss: 1.8164, Val Loss: 1.8333, Best Val Loss: 1.8333\n",
      "Epoch 14/1000, Train Loss: 1.7677, Val Loss: 1.7668, Best Val Loss: 1.7668\n",
      "Epoch 15/1000, Train Loss: 1.7578, Val Loss: 1.7578, Best Val Loss: 1.7578\n",
      "Epoch 16/1000, Train Loss: 1.7367, Val Loss: 1.7265, Best Val Loss: 1.7265\n",
      "Epoch 19/1000, Train Loss: 1.7044, Val Loss: 1.7029, Best Val Loss: 1.7029\n",
      "Epoch 23/1000, Train Loss: 1.6707, Val Loss: 1.6652, Best Val Loss: 1.6652\n",
      "Epoch 25/1000, Train Loss: 1.6629, Val Loss: 1.6131, Best Val Loss: 1.6131\n",
      "Epoch 31/1000, Train Loss: 1.6107, Val Loss: 1.5971, Best Val Loss: 1.5971\n",
      "Epoch 35/1000, Train Loss: 1.5830, Val Loss: 1.5757, Best Val Loss: 1.5757\n",
      "Epoch 38/1000, Train Loss: 1.5737, Val Loss: 1.5728, Best Val Loss: 1.5728\n",
      "Epoch 40/1000, Train Loss: 1.5665, Val Loss: 1.5686, Best Val Loss: 1.5686\n",
      "Epoch 50/1000, Train Loss: 1.5242, Val Loss: 1.5506, Best Val Loss: 1.5506\n",
      "Epoch 51/1000, Train Loss: 1.5140, Val Loss: 1.5445, Best Val Loss: 1.5445\n",
      "Epoch 56/1000, Train Loss: 1.5215, Val Loss: 1.5389, Best Val Loss: 1.5389\n",
      "Epoch 60/1000, Train Loss: 1.4929, Val Loss: 1.5228, Best Val Loss: 1.5228\n",
      "Epoch 62/1000, Train Loss: 1.4905, Val Loss: 1.4739, Best Val Loss: 1.4739\n",
      "Epoch 101/1000, Train Loss: 1.4017, Val Loss: 1.4633, Best Val Loss: 1.4633\n",
      "Epoch 108/1000, Train Loss: 1.3877, Val Loss: 1.4616, Best Val Loss: 1.4616\n",
      "Epoch 135/1000, Train Loss: 1.3486, Val Loss: 1.4570, Best Val Loss: 1.4570\n",
      "Epoch 143/1000, Train Loss: 1.3407, Val Loss: 1.4537, Best Val Loss: 1.4537\n",
      "Early stopping at epoch 193, Best Val Loss: 1.4537\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 152.2323, Val Loss: 5.4664, Best Val Loss: 5.4664\n",
      "Epoch 2/1000, Train Loss: 5.4153, Val Loss: 5.3621, Best Val Loss: 5.3621\n",
      "Epoch 3/1000, Train Loss: 5.3275, Val Loss: 5.2895, Best Val Loss: 5.2895\n",
      "Epoch 4/1000, Train Loss: 5.2593, Val Loss: 5.2257, Best Val Loss: 5.2257\n",
      "Epoch 5/1000, Train Loss: 5.1834, Val Loss: 5.1261, Best Val Loss: 5.1261\n",
      "Epoch 6/1000, Train Loss: 4.6279, Val Loss: 2.5967, Best Val Loss: 2.5967\n",
      "Epoch 7/1000, Train Loss: 2.2606, Val Loss: 2.1222, Best Val Loss: 2.1222\n",
      "Epoch 8/1000, Train Loss: 2.0200, Val Loss: 1.9634, Best Val Loss: 1.9634\n",
      "Epoch 9/1000, Train Loss: 1.9129, Val Loss: 1.8853, Best Val Loss: 1.8853\n",
      "Epoch 10/1000, Train Loss: 1.8530, Val Loss: 1.8453, Best Val Loss: 1.8453\n",
      "Epoch 11/1000, Train Loss: 1.8135, Val Loss: 1.8189, Best Val Loss: 1.8189\n",
      "Epoch 12/1000, Train Loss: 1.7826, Val Loss: 1.7895, Best Val Loss: 1.7895\n",
      "Epoch 13/1000, Train Loss: 1.7639, Val Loss: 1.7628, Best Val Loss: 1.7628\n",
      "Epoch 14/1000, Train Loss: 1.7417, Val Loss: 1.7456, Best Val Loss: 1.7456\n",
      "Epoch 18/1000, Train Loss: 1.6921, Val Loss: 1.6958, Best Val Loss: 1.6958\n",
      "Epoch 19/1000, Train Loss: 1.6813, Val Loss: 1.6694, Best Val Loss: 1.6694\n",
      "Epoch 20/1000, Train Loss: 1.6699, Val Loss: 1.6657, Best Val Loss: 1.6657\n",
      "Epoch 25/1000, Train Loss: 1.6355, Val Loss: 1.6612, Best Val Loss: 1.6612\n",
      "Epoch 26/1000, Train Loss: 1.6340, Val Loss: 1.6346, Best Val Loss: 1.6346\n",
      "Epoch 32/1000, Train Loss: 1.5907, Val Loss: 1.6081, Best Val Loss: 1.6081\n",
      "Epoch 36/1000, Train Loss: 1.5611, Val Loss: 1.5911, Best Val Loss: 1.5911\n",
      "Epoch 37/1000, Train Loss: 1.5544, Val Loss: 1.5718, Best Val Loss: 1.5718\n",
      "Epoch 38/1000, Train Loss: 1.5476, Val Loss: 1.5681, Best Val Loss: 1.5681\n",
      "Epoch 47/1000, Train Loss: 1.5073, Val Loss: 1.5357, Best Val Loss: 1.5357\n",
      "Epoch 50/1000, Train Loss: 1.5027, Val Loss: 1.5231, Best Val Loss: 1.5231\n",
      "Epoch 51/1000, Train Loss: 1.4911, Val Loss: 1.5150, Best Val Loss: 1.5150\n",
      "Epoch 65/1000, Train Loss: 1.4493, Val Loss: 1.4843, Best Val Loss: 1.4843\n",
      "Epoch 68/1000, Train Loss: 1.4441, Val Loss: 1.4817, Best Val Loss: 1.4817\n",
      "Epoch 85/1000, Train Loss: 1.4029, Val Loss: 1.4764, Best Val Loss: 1.4764\n",
      "Epoch 96/1000, Train Loss: 1.3853, Val Loss: 1.4737, Best Val Loss: 1.4737\n",
      "Epoch 100/1000, Train Loss: 1.3748, Val Loss: 1.4677, Best Val Loss: 1.4677\n",
      "Epoch 111/1000, Train Loss: 1.3584, Val Loss: 1.4643, Best Val Loss: 1.4643\n",
      "Early stopping at epoch 161, Best Val Loss: 1.4643\n",
      "Epistemic Variance: 0.904564\n",
      "Aleatoric Variance: 7.032503\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n",
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           1.981\n",
      "  RMSE          2.793\n",
      "  MDAE          1.491\n",
      "  MARPD         1.730\n",
      "  R2            0.863\n",
      "  Correlation   0.929\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.024\n",
      "  Mean-absolute Calibration Error       0.021\n",
      "  Miscalibration Area                   0.021\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.033\n",
      "     Group Size: 0.56 -- Calibration Error: 0.025\n",
      "     Group Size: 1.00 -- Calibration Error: 0.021\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.037\n",
      "     Group Size: 0.56 -- Calibration Error: 0.030\n",
      "     Group Size: 1.00 -- Calibration Error: 0.024\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   2.817\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   2.288\n",
      "  CRPS                      1.417\n",
      "  Check Score               0.715\n",
      "  Interval Score            7.105\n",
      "{'accuracy': {'mae': 1.9811315987059397, 'rmse': np.float64(2.7934507600083656), 'mdae': 1.4908690673828175, 'marpd': np.float64(1.7298635990513775), 'r2': 0.8633416367694272, 'corr': np.float64(0.9294616515779985)}, 'avg_calibration': {'rms_cal': np.float64(0.024027894061398655), 'ma_cal': np.float64(0.020936015931690227), 'miscal_area': np.float64(0.021145424462017635)}, 'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.38348485, 0.03292291, 0.02905929, 0.02650623, 0.02629196,\n",
      "       0.02490192, 0.02383617, 0.02366002, 0.02292055, 0.02093602]), 'adv_group_cali_stderr': array([0.07483481, 0.00520541, 0.00395479, 0.00171945, 0.00195642,\n",
      "       0.00167356, 0.00090114, 0.00088606, 0.00054908, 0.        ])}, 'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.42043661, 0.03736986, 0.03512278, 0.03182604, 0.02880253,\n",
      "       0.02983046, 0.02775798, 0.02680575, 0.02543702, 0.02402789]), 'adv_group_cali_stderr': array([0.08106263, 0.00385057, 0.00404214, 0.00247361, 0.00185265,\n",
      "       0.00202626, 0.00135855, 0.00093511, 0.00067294, 0.        ])}}, 'sharpness': {'sharp': np.float32(2.81728)}, 'scoring_rule': {'nll': np.float64(2.2879378146076124), 'crps': np.float64(1.4169408178311655), 'check': np.float64(0.7154332051362339), 'interval': np.float64(7.105245636721301)}}\n",
      "coverage: 0.9499182928001538, MPIW: 10.040916990379985\n",
      "Run 8 with seed 10001\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 160.8476, Val Loss: 5.5751, Best Val Loss: 5.5751\n",
      "Epoch 2/1000, Train Loss: 5.5141, Val Loss: 5.4497, Best Val Loss: 5.4497\n",
      "Epoch 3/1000, Train Loss: 5.3996, Val Loss: 5.3368, Best Val Loss: 5.3368\n",
      "Epoch 4/1000, Train Loss: 5.2552, Val Loss: 5.1282, Best Val Loss: 5.1282\n",
      "Epoch 5/1000, Train Loss: 4.2093, Val Loss: 2.5839, Best Val Loss: 2.5839\n",
      "Epoch 6/1000, Train Loss: 2.4078, Val Loss: 2.2705, Best Val Loss: 2.2705\n",
      "Epoch 7/1000, Train Loss: 2.1572, Val Loss: 2.0804, Best Val Loss: 2.0804\n",
      "Epoch 8/1000, Train Loss: 1.9992, Val Loss: 1.9444, Best Val Loss: 1.9444\n",
      "Epoch 9/1000, Train Loss: 1.9068, Val Loss: 1.8751, Best Val Loss: 1.8751\n",
      "Epoch 10/1000, Train Loss: 1.8449, Val Loss: 1.8598, Best Val Loss: 1.8598\n",
      "Epoch 11/1000, Train Loss: 1.8034, Val Loss: 1.7839, Best Val Loss: 1.7839\n",
      "Epoch 12/1000, Train Loss: 1.7868, Val Loss: 1.7672, Best Val Loss: 1.7672\n",
      "Epoch 13/1000, Train Loss: 1.7626, Val Loss: 1.7591, Best Val Loss: 1.7591\n",
      "Epoch 15/1000, Train Loss: 1.7336, Val Loss: 1.7191, Best Val Loss: 1.7191\n",
      "Epoch 20/1000, Train Loss: 1.6796, Val Loss: 1.6768, Best Val Loss: 1.6768\n",
      "Epoch 23/1000, Train Loss: 1.6534, Val Loss: 1.6753, Best Val Loss: 1.6753\n",
      "Epoch 24/1000, Train Loss: 1.6482, Val Loss: 1.6648, Best Val Loss: 1.6648\n",
      "Epoch 25/1000, Train Loss: 1.6404, Val Loss: 1.6498, Best Val Loss: 1.6498\n",
      "Epoch 26/1000, Train Loss: 1.6420, Val Loss: 1.6328, Best Val Loss: 1.6328\n",
      "Epoch 28/1000, Train Loss: 1.6236, Val Loss: 1.6315, Best Val Loss: 1.6315\n",
      "Epoch 29/1000, Train Loss: 1.6081, Val Loss: 1.6042, Best Val Loss: 1.6042\n",
      "Epoch 31/1000, Train Loss: 1.6020, Val Loss: 1.5785, Best Val Loss: 1.5785\n",
      "Epoch 37/1000, Train Loss: 1.5712, Val Loss: 1.5612, Best Val Loss: 1.5612\n",
      "Epoch 48/1000, Train Loss: 1.5171, Val Loss: 1.5330, Best Val Loss: 1.5330\n",
      "Epoch 53/1000, Train Loss: 1.5013, Val Loss: 1.5298, Best Val Loss: 1.5298\n",
      "Epoch 55/1000, Train Loss: 1.4952, Val Loss: 1.5151, Best Val Loss: 1.5151\n",
      "Epoch 57/1000, Train Loss: 1.4838, Val Loss: 1.5104, Best Val Loss: 1.5104\n",
      "Epoch 62/1000, Train Loss: 1.4720, Val Loss: 1.5072, Best Val Loss: 1.5072\n",
      "Epoch 71/1000, Train Loss: 1.4446, Val Loss: 1.4839, Best Val Loss: 1.4839\n",
      "Epoch 91/1000, Train Loss: 1.4048, Val Loss: 1.4831, Best Val Loss: 1.4831\n",
      "Epoch 93/1000, Train Loss: 1.4113, Val Loss: 1.4804, Best Val Loss: 1.4804\n",
      "Epoch 121/1000, Train Loss: 1.3645, Val Loss: 1.4775, Best Val Loss: 1.4775\n",
      "Epoch 130/1000, Train Loss: 1.3527, Val Loss: 1.4738, Best Val Loss: 1.4738\n",
      "Epoch 132/1000, Train Loss: 1.3492, Val Loss: 1.4709, Best Val Loss: 1.4709\n",
      "Epoch 153/1000, Train Loss: 1.3172, Val Loss: 1.4704, Best Val Loss: 1.4704\n",
      "Early stopping at epoch 203, Best Val Loss: 1.4704\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 131.0774, Val Loss: 5.5103, Best Val Loss: 5.5103\n",
      "Epoch 2/1000, Train Loss: 5.4333, Val Loss: 5.3677, Best Val Loss: 5.3677\n",
      "Epoch 3/1000, Train Loss: 5.3265, Val Loss: 5.2864, Best Val Loss: 5.2864\n",
      "Epoch 4/1000, Train Loss: 5.2598, Val Loss: 5.2329, Best Val Loss: 5.2329\n",
      "Epoch 5/1000, Train Loss: 5.2094, Val Loss: 5.1829, Best Val Loss: 5.1829\n",
      "Epoch 6/1000, Train Loss: 5.1493, Val Loss: 5.1043, Best Val Loss: 5.1043\n",
      "Epoch 7/1000, Train Loss: 4.8726, Val Loss: 3.0414, Best Val Loss: 3.0414\n",
      "Epoch 8/1000, Train Loss: 2.2519, Val Loss: 2.0822, Best Val Loss: 2.0822\n",
      "Epoch 9/1000, Train Loss: 1.9726, Val Loss: 1.9232, Best Val Loss: 1.9232\n",
      "Epoch 10/1000, Train Loss: 1.8769, Val Loss: 1.8726, Best Val Loss: 1.8726\n",
      "Epoch 11/1000, Train Loss: 1.8302, Val Loss: 1.7941, Best Val Loss: 1.7941\n",
      "Epoch 12/1000, Train Loss: 1.7920, Val Loss: 1.7766, Best Val Loss: 1.7766\n",
      "Epoch 13/1000, Train Loss: 1.7589, Val Loss: 1.7737, Best Val Loss: 1.7737\n",
      "Epoch 14/1000, Train Loss: 1.7402, Val Loss: 1.7542, Best Val Loss: 1.7542\n",
      "Epoch 15/1000, Train Loss: 1.7253, Val Loss: 1.7268, Best Val Loss: 1.7268\n",
      "Epoch 20/1000, Train Loss: 1.6659, Val Loss: 1.7090, Best Val Loss: 1.7090\n",
      "Epoch 22/1000, Train Loss: 1.6619, Val Loss: 1.6784, Best Val Loss: 1.6784\n",
      "Epoch 24/1000, Train Loss: 1.6342, Val Loss: 1.6671, Best Val Loss: 1.6671\n",
      "Epoch 25/1000, Train Loss: 1.6329, Val Loss: 1.6610, Best Val Loss: 1.6610\n",
      "Epoch 27/1000, Train Loss: 1.6192, Val Loss: 1.6195, Best Val Loss: 1.6195\n",
      "Epoch 28/1000, Train Loss: 1.6075, Val Loss: 1.6192, Best Val Loss: 1.6192\n",
      "Epoch 29/1000, Train Loss: 1.5983, Val Loss: 1.6048, Best Val Loss: 1.6048\n",
      "Epoch 31/1000, Train Loss: 1.5908, Val Loss: 1.6024, Best Val Loss: 1.6024\n",
      "Epoch 34/1000, Train Loss: 1.5672, Val Loss: 1.5694, Best Val Loss: 1.5694\n",
      "Epoch 42/1000, Train Loss: 1.5214, Val Loss: 1.5534, Best Val Loss: 1.5534\n",
      "Epoch 44/1000, Train Loss: 1.5050, Val Loss: 1.5452, Best Val Loss: 1.5452\n",
      "Epoch 47/1000, Train Loss: 1.5069, Val Loss: 1.5348, Best Val Loss: 1.5348\n",
      "Epoch 50/1000, Train Loss: 1.4786, Val Loss: 1.5285, Best Val Loss: 1.5285\n",
      "Epoch 54/1000, Train Loss: 1.4750, Val Loss: 1.5250, Best Val Loss: 1.5250\n",
      "Epoch 60/1000, Train Loss: 1.4608, Val Loss: 1.4965, Best Val Loss: 1.4965\n",
      "Epoch 70/1000, Train Loss: 1.4212, Val Loss: 1.4913, Best Val Loss: 1.4913\n",
      "Epoch 76/1000, Train Loss: 1.4140, Val Loss: 1.4892, Best Val Loss: 1.4892\n",
      "Epoch 112/1000, Train Loss: 1.3371, Val Loss: 1.4849, Best Val Loss: 1.4849\n",
      "Early stopping at epoch 162, Best Val Loss: 1.4849\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 146.7584, Val Loss: 5.6231, Best Val Loss: 5.6231\n",
      "Epoch 2/1000, Train Loss: 5.5323, Val Loss: 5.4484, Best Val Loss: 5.4484\n",
      "Epoch 3/1000, Train Loss: 5.4001, Val Loss: 5.3457, Best Val Loss: 5.3457\n",
      "Epoch 4/1000, Train Loss: 5.3033, Val Loss: 5.2559, Best Val Loss: 5.2559\n",
      "Epoch 5/1000, Train Loss: 5.2047, Val Loss: 5.1377, Best Val Loss: 5.1377\n",
      "Epoch 6/1000, Train Loss: 4.3799, Val Loss: 2.4959, Best Val Loss: 2.4959\n",
      "Epoch 7/1000, Train Loss: 2.3126, Val Loss: 2.1879, Best Val Loss: 2.1879\n",
      "Epoch 8/1000, Train Loss: 2.1022, Val Loss: 2.0326, Best Val Loss: 2.0326\n",
      "Epoch 9/1000, Train Loss: 1.9739, Val Loss: 1.9509, Best Val Loss: 1.9509\n",
      "Epoch 10/1000, Train Loss: 1.8918, Val Loss: 1.8827, Best Val Loss: 1.8827\n",
      "Epoch 11/1000, Train Loss: 1.8408, Val Loss: 1.8192, Best Val Loss: 1.8192\n",
      "Epoch 14/1000, Train Loss: 1.7517, Val Loss: 1.7482, Best Val Loss: 1.7482\n",
      "Epoch 15/1000, Train Loss: 1.7270, Val Loss: 1.7167, Best Val Loss: 1.7167\n",
      "Epoch 16/1000, Train Loss: 1.7031, Val Loss: 1.6965, Best Val Loss: 1.6965\n",
      "Epoch 17/1000, Train Loss: 1.6940, Val Loss: 1.6870, Best Val Loss: 1.6870\n",
      "Epoch 19/1000, Train Loss: 1.6665, Val Loss: 1.6660, Best Val Loss: 1.6660\n",
      "Epoch 21/1000, Train Loss: 1.6558, Val Loss: 1.6515, Best Val Loss: 1.6515\n",
      "Epoch 23/1000, Train Loss: 1.6381, Val Loss: 1.6485, Best Val Loss: 1.6485\n",
      "Epoch 24/1000, Train Loss: 1.6371, Val Loss: 1.6344, Best Val Loss: 1.6344\n",
      "Epoch 28/1000, Train Loss: 1.6046, Val Loss: 1.6093, Best Val Loss: 1.6093\n",
      "Epoch 32/1000, Train Loss: 1.5743, Val Loss: 1.5768, Best Val Loss: 1.5768\n",
      "Epoch 37/1000, Train Loss: 1.5481, Val Loss: 1.5750, Best Val Loss: 1.5750\n",
      "Epoch 38/1000, Train Loss: 1.5456, Val Loss: 1.5724, Best Val Loss: 1.5724\n",
      "Epoch 40/1000, Train Loss: 1.5251, Val Loss: 1.5670, Best Val Loss: 1.5670\n",
      "Epoch 43/1000, Train Loss: 1.5194, Val Loss: 1.5449, Best Val Loss: 1.5449\n",
      "Epoch 46/1000, Train Loss: 1.4980, Val Loss: 1.5394, Best Val Loss: 1.5394\n",
      "Epoch 49/1000, Train Loss: 1.4932, Val Loss: 1.5348, Best Val Loss: 1.5348\n",
      "Epoch 54/1000, Train Loss: 1.4824, Val Loss: 1.5284, Best Val Loss: 1.5284\n",
      "Epoch 55/1000, Train Loss: 1.4706, Val Loss: 1.5147, Best Val Loss: 1.5147\n",
      "Epoch 57/1000, Train Loss: 1.4607, Val Loss: 1.5121, Best Val Loss: 1.5121\n",
      "Epoch 64/1000, Train Loss: 1.4477, Val Loss: 1.4993, Best Val Loss: 1.4993\n",
      "Epoch 68/1000, Train Loss: 1.4394, Val Loss: 1.4902, Best Val Loss: 1.4902\n",
      "Epoch 87/1000, Train Loss: 1.3996, Val Loss: 1.4899, Best Val Loss: 1.4899\n",
      "Epoch 90/1000, Train Loss: 1.3923, Val Loss: 1.4850, Best Val Loss: 1.4850\n",
      "Epoch 91/1000, Train Loss: 1.3991, Val Loss: 1.4641, Best Val Loss: 1.4641\n",
      "Epoch 129/1000, Train Loss: 1.3338, Val Loss: 1.4560, Best Val Loss: 1.4560\n",
      "Early stopping at epoch 179, Best Val Loss: 1.4560\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 149.8877, Val Loss: 5.5648, Best Val Loss: 5.5648\n",
      "Epoch 2/1000, Train Loss: 5.5079, Val Loss: 5.4378, Best Val Loss: 5.4378\n",
      "Epoch 3/1000, Train Loss: 5.3974, Val Loss: 5.3394, Best Val Loss: 5.3394\n",
      "Epoch 4/1000, Train Loss: 5.2820, Val Loss: 5.1965, Best Val Loss: 5.1965\n",
      "Epoch 5/1000, Train Loss: 5.0035, Val Loss: 4.5672, Best Val Loss: 4.5672\n",
      "Epoch 6/1000, Train Loss: 2.8816, Val Loss: 2.4177, Best Val Loss: 2.4177\n",
      "Epoch 7/1000, Train Loss: 2.2914, Val Loss: 2.2002, Best Val Loss: 2.2002\n",
      "Epoch 8/1000, Train Loss: 2.1109, Val Loss: 2.0790, Best Val Loss: 2.0790\n",
      "Epoch 9/1000, Train Loss: 1.9925, Val Loss: 1.9653, Best Val Loss: 1.9653\n",
      "Epoch 10/1000, Train Loss: 1.9142, Val Loss: 1.8975, Best Val Loss: 1.8975\n",
      "Epoch 12/1000, Train Loss: 1.8054, Val Loss: 1.8225, Best Val Loss: 1.8225\n",
      "Epoch 13/1000, Train Loss: 1.7862, Val Loss: 1.7731, Best Val Loss: 1.7731\n",
      "Epoch 14/1000, Train Loss: 1.7599, Val Loss: 1.7518, Best Val Loss: 1.7518\n",
      "Epoch 16/1000, Train Loss: 1.7440, Val Loss: 1.7228, Best Val Loss: 1.7228\n",
      "Epoch 17/1000, Train Loss: 1.7156, Val Loss: 1.7049, Best Val Loss: 1.7049\n",
      "Epoch 21/1000, Train Loss: 1.6927, Val Loss: 1.6631, Best Val Loss: 1.6631\n",
      "Epoch 24/1000, Train Loss: 1.6567, Val Loss: 1.6497, Best Val Loss: 1.6497\n",
      "Epoch 28/1000, Train Loss: 1.6256, Val Loss: 1.6306, Best Val Loss: 1.6306\n",
      "Epoch 32/1000, Train Loss: 1.5941, Val Loss: 1.6246, Best Val Loss: 1.6246\n",
      "Epoch 35/1000, Train Loss: 1.5917, Val Loss: 1.5838, Best Val Loss: 1.5838\n",
      "Epoch 40/1000, Train Loss: 1.5579, Val Loss: 1.5836, Best Val Loss: 1.5836\n",
      "Epoch 41/1000, Train Loss: 1.5438, Val Loss: 1.5618, Best Val Loss: 1.5618\n",
      "Epoch 44/1000, Train Loss: 1.5369, Val Loss: 1.5521, Best Val Loss: 1.5521\n",
      "Epoch 51/1000, Train Loss: 1.5142, Val Loss: 1.5421, Best Val Loss: 1.5421\n",
      "Epoch 53/1000, Train Loss: 1.5023, Val Loss: 1.5412, Best Val Loss: 1.5412\n",
      "Epoch 54/1000, Train Loss: 1.5109, Val Loss: 1.5286, Best Val Loss: 1.5286\n",
      "Epoch 60/1000, Train Loss: 1.4875, Val Loss: 1.5200, Best Val Loss: 1.5200\n",
      "Epoch 67/1000, Train Loss: 1.4773, Val Loss: 1.5165, Best Val Loss: 1.5165\n",
      "Epoch 68/1000, Train Loss: 1.4670, Val Loss: 1.5022, Best Val Loss: 1.5022\n",
      "Epoch 74/1000, Train Loss: 1.4517, Val Loss: 1.5012, Best Val Loss: 1.5012\n",
      "Epoch 77/1000, Train Loss: 1.4539, Val Loss: 1.4989, Best Val Loss: 1.4989\n",
      "Epoch 97/1000, Train Loss: 1.4228, Val Loss: 1.4985, Best Val Loss: 1.4985\n",
      "Epoch 103/1000, Train Loss: 1.3998, Val Loss: 1.4920, Best Val Loss: 1.4920\n",
      "Epoch 111/1000, Train Loss: 1.3819, Val Loss: 1.4613, Best Val Loss: 1.4613\n",
      "Early stopping at epoch 161, Best Val Loss: 1.4613\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 148.2153, Val Loss: 5.5114, Best Val Loss: 5.5114\n",
      "Epoch 2/1000, Train Loss: 5.4648, Val Loss: 5.4210, Best Val Loss: 5.4210\n",
      "Epoch 3/1000, Train Loss: 5.3845, Val Loss: 5.3461, Best Val Loss: 5.3461\n",
      "Epoch 4/1000, Train Loss: 5.3083, Val Loss: 5.2685, Best Val Loss: 5.2685\n",
      "Epoch 5/1000, Train Loss: 5.2153, Val Loss: 5.1456, Best Val Loss: 5.1456\n",
      "Epoch 6/1000, Train Loss: 4.5161, Val Loss: 2.6108, Best Val Loss: 2.6108\n",
      "Epoch 7/1000, Train Loss: 2.3699, Val Loss: 2.2298, Best Val Loss: 2.2298\n",
      "Epoch 8/1000, Train Loss: 2.1164, Val Loss: 2.0382, Best Val Loss: 2.0382\n",
      "Epoch 9/1000, Train Loss: 1.9786, Val Loss: 1.9398, Best Val Loss: 1.9398\n",
      "Epoch 10/1000, Train Loss: 1.8952, Val Loss: 1.8710, Best Val Loss: 1.8710\n",
      "Epoch 11/1000, Train Loss: 1.8497, Val Loss: 1.8040, Best Val Loss: 1.8040\n",
      "Epoch 13/1000, Train Loss: 1.7829, Val Loss: 1.7917, Best Val Loss: 1.7917\n",
      "Epoch 14/1000, Train Loss: 1.7549, Val Loss: 1.7252, Best Val Loss: 1.7252\n",
      "Epoch 16/1000, Train Loss: 1.7278, Val Loss: 1.7228, Best Val Loss: 1.7228\n",
      "Epoch 18/1000, Train Loss: 1.7030, Val Loss: 1.7094, Best Val Loss: 1.7094\n",
      "Epoch 20/1000, Train Loss: 1.6792, Val Loss: 1.6840, Best Val Loss: 1.6840\n",
      "Epoch 21/1000, Train Loss: 1.6626, Val Loss: 1.6825, Best Val Loss: 1.6825\n",
      "Epoch 22/1000, Train Loss: 1.6635, Val Loss: 1.6488, Best Val Loss: 1.6488\n",
      "Epoch 24/1000, Train Loss: 1.6466, Val Loss: 1.6465, Best Val Loss: 1.6465\n",
      "Epoch 25/1000, Train Loss: 1.6434, Val Loss: 1.6378, Best Val Loss: 1.6378\n",
      "Epoch 30/1000, Train Loss: 1.6187, Val Loss: 1.6375, Best Val Loss: 1.6375\n",
      "Epoch 31/1000, Train Loss: 1.6022, Val Loss: 1.6244, Best Val Loss: 1.6244\n",
      "Epoch 36/1000, Train Loss: 1.5641, Val Loss: 1.6196, Best Val Loss: 1.6196\n",
      "Epoch 37/1000, Train Loss: 1.5666, Val Loss: 1.6076, Best Val Loss: 1.6076\n",
      "Epoch 39/1000, Train Loss: 1.5482, Val Loss: 1.5770, Best Val Loss: 1.5770\n",
      "Epoch 42/1000, Train Loss: 1.5410, Val Loss: 1.5712, Best Val Loss: 1.5712\n",
      "Epoch 43/1000, Train Loss: 1.5338, Val Loss: 1.5636, Best Val Loss: 1.5636\n",
      "Epoch 45/1000, Train Loss: 1.5230, Val Loss: 1.5310, Best Val Loss: 1.5310\n",
      "Epoch 57/1000, Train Loss: 1.4850, Val Loss: 1.5199, Best Val Loss: 1.5199\n",
      "Epoch 60/1000, Train Loss: 1.4800, Val Loss: 1.5110, Best Val Loss: 1.5110\n",
      "Epoch 72/1000, Train Loss: 1.4466, Val Loss: 1.4901, Best Val Loss: 1.4901\n",
      "Epoch 87/1000, Train Loss: 1.4232, Val Loss: 1.4791, Best Val Loss: 1.4791\n",
      "Epoch 100/1000, Train Loss: 1.3999, Val Loss: 1.4779, Best Val Loss: 1.4779\n",
      "Epoch 133/1000, Train Loss: 1.3478, Val Loss: 1.4633, Best Val Loss: 1.4633\n",
      "Early stopping at epoch 183, Best Val Loss: 1.4633\n",
      "Epistemic Variance: 0.846175\n",
      "Aleatoric Variance: 9.700246\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n",
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           1.977\n",
      "  RMSE          2.766\n",
      "  MDAE          1.457\n",
      "  MARPD         1.727\n",
      "  R2            0.866\n",
      "  Correlation   0.931\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.033\n",
      "  Mean-absolute Calibration Error       0.028\n",
      "  Miscalibration Area                   0.028\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.044\n",
      "     Group Size: 0.56 -- Calibration Error: 0.031\n",
      "     Group Size: 1.00 -- Calibration Error: 0.028\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.046\n",
      "     Group Size: 0.56 -- Calibration Error: 0.036\n",
      "     Group Size: 1.00 -- Calibration Error: 0.033\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   3.248\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   2.285\n",
      "  CRPS                      1.416\n",
      "  Check Score               0.715\n",
      "  Interval Score            7.113\n",
      "{'accuracy': {'mae': 1.9769504351692235, 'rmse': np.float64(2.7656761334682893), 'mdae': 1.456980859374994, 'marpd': np.float64(1.7273390924960756), 'r2': 0.8660456511029008, 'corr': np.float64(0.9308998206087401)}, 'avg_calibration': {'rms_cal': np.float64(0.03272556653746979), 'ma_cal': np.float64(0.027570271590265785), 'miscal_area': np.float64(0.027842480022774067)}, 'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.3765    , 0.04385574, 0.03658344, 0.0329801 , 0.03292622,\n",
      "       0.03061827, 0.03001819, 0.02990591, 0.02898535, 0.02757027]), 'adv_group_cali_stderr': array([4.85116254e-02, 5.33625144e-03, 2.74209424e-03, 2.67441057e-03,\n",
      "       1.24382257e-03, 1.06019866e-03, 8.17001018e-04, 1.50698485e-03,\n",
      "       6.03178484e-04, 3.65711820e-18])}, 'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.45902213, 0.04647776, 0.04171573, 0.03837041, 0.03817333,\n",
      "       0.03614727, 0.03614667, 0.03508266, 0.0347758 , 0.03272557]), 'adv_group_cali_stderr': array([0.06303107, 0.00304413, 0.00398055, 0.00217238, 0.00139203,\n",
      "       0.00124782, 0.00106814, 0.00083866, 0.00059912, 0.        ])}}, 'sharpness': {'sharp': np.float32(3.2475255)}, 'scoring_rule': {'nll': np.float64(2.2847498734998273), 'crps': np.float64(1.4156780276093872), 'check': np.float64(0.7147932856780381), 'interval': np.float64(7.112986387773111)}}\n",
      "coverage: 0.9536672113813323, MPIW: 10.37957672555416\n",
      "Run 9 with seed 31415\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 146.3950, Val Loss: 5.5080, Best Val Loss: 5.5080\n",
      "Epoch 2/1000, Train Loss: 5.4657, Val Loss: 5.4078, Best Val Loss: 5.4078\n",
      "Epoch 3/1000, Train Loss: 5.3500, Val Loss: 5.2605, Best Val Loss: 5.2605\n",
      "Epoch 4/1000, Train Loss: 5.0628, Val Loss: 4.6854, Best Val Loss: 4.6854\n",
      "Epoch 5/1000, Train Loss: 3.1199, Val Loss: 2.4725, Best Val Loss: 2.4725\n",
      "Epoch 6/1000, Train Loss: 2.3445, Val Loss: 2.2443, Best Val Loss: 2.2443\n",
      "Epoch 7/1000, Train Loss: 2.1589, Val Loss: 2.0915, Best Val Loss: 2.0915\n",
      "Epoch 8/1000, Train Loss: 2.0318, Val Loss: 1.9906, Best Val Loss: 1.9906\n",
      "Epoch 9/1000, Train Loss: 1.9414, Val Loss: 1.9191, Best Val Loss: 1.9191\n",
      "Epoch 10/1000, Train Loss: 1.8705, Val Loss: 1.8780, Best Val Loss: 1.8780\n",
      "Epoch 11/1000, Train Loss: 1.8320, Val Loss: 1.8079, Best Val Loss: 1.8079\n",
      "Epoch 13/1000, Train Loss: 1.7638, Val Loss: 1.7690, Best Val Loss: 1.7690\n",
      "Epoch 15/1000, Train Loss: 1.7378, Val Loss: 1.7550, Best Val Loss: 1.7550\n",
      "Epoch 16/1000, Train Loss: 1.7203, Val Loss: 1.7239, Best Val Loss: 1.7239\n",
      "Epoch 17/1000, Train Loss: 1.7121, Val Loss: 1.7062, Best Val Loss: 1.7062\n",
      "Epoch 19/1000, Train Loss: 1.6980, Val Loss: 1.6798, Best Val Loss: 1.6798\n",
      "Epoch 23/1000, Train Loss: 1.6637, Val Loss: 1.6559, Best Val Loss: 1.6559\n",
      "Epoch 26/1000, Train Loss: 1.6462, Val Loss: 1.6511, Best Val Loss: 1.6511\n",
      "Epoch 28/1000, Train Loss: 1.6332, Val Loss: 1.6197, Best Val Loss: 1.6197\n",
      "Epoch 36/1000, Train Loss: 1.5967, Val Loss: 1.6065, Best Val Loss: 1.6065\n",
      "Epoch 40/1000, Train Loss: 1.5746, Val Loss: 1.5730, Best Val Loss: 1.5730\n",
      "Epoch 46/1000, Train Loss: 1.5566, Val Loss: 1.5480, Best Val Loss: 1.5480\n",
      "Epoch 55/1000, Train Loss: 1.5330, Val Loss: 1.5393, Best Val Loss: 1.5393\n",
      "Epoch 64/1000, Train Loss: 1.5046, Val Loss: 1.5158, Best Val Loss: 1.5158\n",
      "Epoch 83/1000, Train Loss: 1.4543, Val Loss: 1.5082, Best Val Loss: 1.5082\n",
      "Epoch 87/1000, Train Loss: 1.4568, Val Loss: 1.4996, Best Val Loss: 1.4996\n",
      "Epoch 94/1000, Train Loss: 1.4395, Val Loss: 1.4994, Best Val Loss: 1.4994\n",
      "Epoch 101/1000, Train Loss: 1.4270, Val Loss: 1.4898, Best Val Loss: 1.4898\n",
      "Epoch 107/1000, Train Loss: 1.4123, Val Loss: 1.4775, Best Val Loss: 1.4775\n",
      "Epoch 108/1000, Train Loss: 1.4174, Val Loss: 1.4762, Best Val Loss: 1.4762\n",
      "Epoch 118/1000, Train Loss: 1.4030, Val Loss: 1.4721, Best Val Loss: 1.4721\n",
      "Epoch 123/1000, Train Loss: 1.3849, Val Loss: 1.4679, Best Val Loss: 1.4679\n",
      "Epoch 148/1000, Train Loss: 1.3477, Val Loss: 1.4632, Best Val Loss: 1.4632\n",
      "Epoch 163/1000, Train Loss: 1.3379, Val Loss: 1.4627, Best Val Loss: 1.4627\n",
      "Epoch 177/1000, Train Loss: 1.3122, Val Loss: 1.4546, Best Val Loss: 1.4546\n",
      "Early stopping at epoch 227, Best Val Loss: 1.4546\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 146.2940, Val Loss: 5.4899, Best Val Loss: 5.4899\n",
      "Epoch 2/1000, Train Loss: 5.4204, Val Loss: 5.3091, Best Val Loss: 5.3091\n",
      "Epoch 3/1000, Train Loss: 5.1159, Val Loss: 4.7727, Best Val Loss: 4.7727\n",
      "Epoch 4/1000, Train Loss: 3.6349, Val Loss: 2.8482, Best Val Loss: 2.8482\n",
      "Epoch 5/1000, Train Loss: 2.6633, Val Loss: 2.5193, Best Val Loss: 2.5193\n",
      "Epoch 6/1000, Train Loss: 2.4006, Val Loss: 2.3077, Best Val Loss: 2.3077\n",
      "Epoch 7/1000, Train Loss: 2.2133, Val Loss: 2.1463, Best Val Loss: 2.1463\n",
      "Epoch 8/1000, Train Loss: 2.0690, Val Loss: 2.0415, Best Val Loss: 2.0415\n",
      "Epoch 9/1000, Train Loss: 1.9671, Val Loss: 1.9429, Best Val Loss: 1.9429\n",
      "Epoch 10/1000, Train Loss: 1.9073, Val Loss: 1.8865, Best Val Loss: 1.8865\n",
      "Epoch 11/1000, Train Loss: 1.8564, Val Loss: 1.8466, Best Val Loss: 1.8466\n",
      "Epoch 12/1000, Train Loss: 1.8172, Val Loss: 1.8336, Best Val Loss: 1.8336\n",
      "Epoch 13/1000, Train Loss: 1.7897, Val Loss: 1.8014, Best Val Loss: 1.8014\n",
      "Epoch 14/1000, Train Loss: 1.7645, Val Loss: 1.7358, Best Val Loss: 1.7358\n",
      "Epoch 20/1000, Train Loss: 1.6923, Val Loss: 1.7046, Best Val Loss: 1.7046\n",
      "Epoch 21/1000, Train Loss: 1.6948, Val Loss: 1.6751, Best Val Loss: 1.6751\n",
      "Epoch 26/1000, Train Loss: 1.6591, Val Loss: 1.6488, Best Val Loss: 1.6488\n",
      "Epoch 29/1000, Train Loss: 1.6335, Val Loss: 1.6466, Best Val Loss: 1.6466\n",
      "Epoch 31/1000, Train Loss: 1.6178, Val Loss: 1.6373, Best Val Loss: 1.6373\n",
      "Epoch 32/1000, Train Loss: 1.6126, Val Loss: 1.6118, Best Val Loss: 1.6118\n",
      "Epoch 35/1000, Train Loss: 1.5985, Val Loss: 1.5989, Best Val Loss: 1.5989\n",
      "Epoch 46/1000, Train Loss: 1.5554, Val Loss: 1.5798, Best Val Loss: 1.5798\n",
      "Epoch 53/1000, Train Loss: 1.5369, Val Loss: 1.5593, Best Val Loss: 1.5593\n",
      "Epoch 56/1000, Train Loss: 1.5183, Val Loss: 1.5528, Best Val Loss: 1.5528\n",
      "Epoch 62/1000, Train Loss: 1.4986, Val Loss: 1.5082, Best Val Loss: 1.5082\n",
      "Epoch 84/1000, Train Loss: 1.4559, Val Loss: 1.4932, Best Val Loss: 1.4932\n",
      "Epoch 98/1000, Train Loss: 1.4209, Val Loss: 1.4757, Best Val Loss: 1.4757\n",
      "Epoch 111/1000, Train Loss: 1.4075, Val Loss: 1.4755, Best Val Loss: 1.4755\n",
      "Early stopping at epoch 161, Best Val Loss: 1.4755\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 136.0681, Val Loss: 5.5166, Best Val Loss: 5.5166\n",
      "Epoch 2/1000, Train Loss: 5.4717, Val Loss: 5.4300, Best Val Loss: 5.4300\n",
      "Epoch 3/1000, Train Loss: 5.3959, Val Loss: 5.3616, Best Val Loss: 5.3616\n",
      "Epoch 4/1000, Train Loss: 5.3300, Val Loss: 5.2981, Best Val Loss: 5.2981\n",
      "Epoch 5/1000, Train Loss: 5.2583, Val Loss: 5.2105, Best Val Loss: 5.2105\n",
      "Epoch 6/1000, Train Loss: 5.1009, Val Loss: 4.9166, Best Val Loss: 4.9166\n",
      "Epoch 7/1000, Train Loss: 3.6637, Val Loss: 2.4311, Best Val Loss: 2.4311\n",
      "Epoch 8/1000, Train Loss: 2.3013, Val Loss: 2.2142, Best Val Loss: 2.2142\n",
      "Epoch 9/1000, Train Loss: 2.1166, Val Loss: 2.0726, Best Val Loss: 2.0726\n",
      "Epoch 10/1000, Train Loss: 1.9951, Val Loss: 1.9630, Best Val Loss: 1.9630\n",
      "Epoch 11/1000, Train Loss: 1.9182, Val Loss: 1.8829, Best Val Loss: 1.8829\n",
      "Epoch 12/1000, Train Loss: 1.8701, Val Loss: 1.8554, Best Val Loss: 1.8554\n",
      "Epoch 14/1000, Train Loss: 1.7993, Val Loss: 1.7911, Best Val Loss: 1.7911\n",
      "Epoch 15/1000, Train Loss: 1.7747, Val Loss: 1.7792, Best Val Loss: 1.7792\n",
      "Epoch 16/1000, Train Loss: 1.7606, Val Loss: 1.7317, Best Val Loss: 1.7317\n",
      "Epoch 17/1000, Train Loss: 1.7402, Val Loss: 1.7177, Best Val Loss: 1.7177\n",
      "Epoch 21/1000, Train Loss: 1.7078, Val Loss: 1.6855, Best Val Loss: 1.6855\n",
      "Epoch 25/1000, Train Loss: 1.6734, Val Loss: 1.6710, Best Val Loss: 1.6710\n",
      "Epoch 30/1000, Train Loss: 1.6324, Val Loss: 1.6423, Best Val Loss: 1.6423\n",
      "Epoch 31/1000, Train Loss: 1.6424, Val Loss: 1.6365, Best Val Loss: 1.6365\n",
      "Epoch 36/1000, Train Loss: 1.6050, Val Loss: 1.5872, Best Val Loss: 1.5872\n",
      "Epoch 42/1000, Train Loss: 1.5753, Val Loss: 1.5756, Best Val Loss: 1.5756\n",
      "Epoch 43/1000, Train Loss: 1.5724, Val Loss: 1.5754, Best Val Loss: 1.5754\n",
      "Epoch 53/1000, Train Loss: 1.5328, Val Loss: 1.5477, Best Val Loss: 1.5477\n",
      "Epoch 58/1000, Train Loss: 1.5197, Val Loss: 1.5411, Best Val Loss: 1.5411\n",
      "Epoch 67/1000, Train Loss: 1.4949, Val Loss: 1.5329, Best Val Loss: 1.5329\n",
      "Epoch 68/1000, Train Loss: 1.4875, Val Loss: 1.5203, Best Val Loss: 1.5203\n",
      "Epoch 71/1000, Train Loss: 1.4808, Val Loss: 1.5186, Best Val Loss: 1.5186\n",
      "Epoch 72/1000, Train Loss: 1.4825, Val Loss: 1.5152, Best Val Loss: 1.5152\n",
      "Epoch 77/1000, Train Loss: 1.4668, Val Loss: 1.5053, Best Val Loss: 1.5053\n",
      "Epoch 79/1000, Train Loss: 1.4688, Val Loss: 1.5053, Best Val Loss: 1.5053\n",
      "Epoch 85/1000, Train Loss: 1.4453, Val Loss: 1.4947, Best Val Loss: 1.4947\n",
      "Epoch 116/1000, Train Loss: 1.4047, Val Loss: 1.4908, Best Val Loss: 1.4908\n",
      "Epoch 118/1000, Train Loss: 1.4037, Val Loss: 1.4799, Best Val Loss: 1.4799\n",
      "Epoch 120/1000, Train Loss: 1.4016, Val Loss: 1.4781, Best Val Loss: 1.4781\n",
      "Epoch 136/1000, Train Loss: 1.3865, Val Loss: 1.4640, Best Val Loss: 1.4640\n",
      "Early stopping at epoch 186, Best Val Loss: 1.4640\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 155.8700, Val Loss: 5.5282, Best Val Loss: 5.5282\n",
      "Epoch 2/1000, Train Loss: 5.4919, Val Loss: 5.4471, Best Val Loss: 5.4471\n",
      "Epoch 3/1000, Train Loss: 5.4167, Val Loss: 5.3760, Best Val Loss: 5.3760\n",
      "Epoch 4/1000, Train Loss: 5.3457, Val Loss: 5.3070, Best Val Loss: 5.3070\n",
      "Epoch 5/1000, Train Loss: 5.2741, Val Loss: 5.2338, Best Val Loss: 5.2338\n",
      "Epoch 6/1000, Train Loss: 5.1823, Val Loss: 5.1058, Best Val Loss: 5.1058\n",
      "Epoch 7/1000, Train Loss: 4.4191, Val Loss: 2.4886, Best Val Loss: 2.4886\n",
      "Epoch 8/1000, Train Loss: 2.3027, Val Loss: 2.2030, Best Val Loss: 2.2030\n",
      "Epoch 9/1000, Train Loss: 2.0907, Val Loss: 2.0482, Best Val Loss: 2.0482\n",
      "Epoch 10/1000, Train Loss: 1.9787, Val Loss: 1.9346, Best Val Loss: 1.9346\n",
      "Epoch 11/1000, Train Loss: 1.9098, Val Loss: 1.8906, Best Val Loss: 1.8906\n",
      "Epoch 12/1000, Train Loss: 1.8581, Val Loss: 1.8301, Best Val Loss: 1.8301\n",
      "Epoch 13/1000, Train Loss: 1.8196, Val Loss: 1.8050, Best Val Loss: 1.8050\n",
      "Epoch 14/1000, Train Loss: 1.7939, Val Loss: 1.7681, Best Val Loss: 1.7681\n",
      "Epoch 16/1000, Train Loss: 1.7497, Val Loss: 1.7491, Best Val Loss: 1.7491\n",
      "Epoch 18/1000, Train Loss: 1.7158, Val Loss: 1.6969, Best Val Loss: 1.6969\n",
      "Epoch 20/1000, Train Loss: 1.6988, Val Loss: 1.6718, Best Val Loss: 1.6718\n",
      "Epoch 25/1000, Train Loss: 1.6542, Val Loss: 1.6605, Best Val Loss: 1.6605\n",
      "Epoch 26/1000, Train Loss: 1.6373, Val Loss: 1.6589, Best Val Loss: 1.6589\n",
      "Epoch 27/1000, Train Loss: 1.6306, Val Loss: 1.6385, Best Val Loss: 1.6385\n",
      "Epoch 29/1000, Train Loss: 1.6234, Val Loss: 1.6197, Best Val Loss: 1.6197\n",
      "Epoch 31/1000, Train Loss: 1.6008, Val Loss: 1.6024, Best Val Loss: 1.6024\n",
      "Epoch 37/1000, Train Loss: 1.5734, Val Loss: 1.5858, Best Val Loss: 1.5858\n",
      "Epoch 41/1000, Train Loss: 1.5413, Val Loss: 1.5607, Best Val Loss: 1.5607\n",
      "Epoch 50/1000, Train Loss: 1.5234, Val Loss: 1.5337, Best Val Loss: 1.5337\n",
      "Epoch 59/1000, Train Loss: 1.4849, Val Loss: 1.5189, Best Val Loss: 1.5189\n",
      "Epoch 60/1000, Train Loss: 1.4851, Val Loss: 1.5171, Best Val Loss: 1.5171\n",
      "Epoch 68/1000, Train Loss: 1.4631, Val Loss: 1.5112, Best Val Loss: 1.5112\n",
      "Epoch 71/1000, Train Loss: 1.4633, Val Loss: 1.4922, Best Val Loss: 1.4922\n",
      "Epoch 81/1000, Train Loss: 1.4392, Val Loss: 1.4888, Best Val Loss: 1.4888\n",
      "Epoch 85/1000, Train Loss: 1.4289, Val Loss: 1.4822, Best Val Loss: 1.4822\n",
      "Epoch 106/1000, Train Loss: 1.4021, Val Loss: 1.4778, Best Val Loss: 1.4778\n",
      "Epoch 110/1000, Train Loss: 1.3998, Val Loss: 1.4757, Best Val Loss: 1.4757\n",
      "Epoch 121/1000, Train Loss: 1.3739, Val Loss: 1.4718, Best Val Loss: 1.4718\n",
      "Epoch 124/1000, Train Loss: 1.3739, Val Loss: 1.4607, Best Val Loss: 1.4607\n",
      "Early stopping at epoch 174, Best Val Loss: 1.4607\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 149.0918, Val Loss: 5.5321, Best Val Loss: 5.5321\n",
      "Epoch 2/1000, Train Loss: 5.4665, Val Loss: 5.4007, Best Val Loss: 5.4007\n",
      "Epoch 3/1000, Train Loss: 5.2780, Val Loss: 5.0506, Best Val Loss: 5.0506\n",
      "Epoch 4/1000, Train Loss: 3.9500, Val Loss: 2.6507, Best Val Loss: 2.6507\n",
      "Epoch 5/1000, Train Loss: 2.4823, Val Loss: 2.3550, Best Val Loss: 2.3550\n",
      "Epoch 6/1000, Train Loss: 2.2563, Val Loss: 2.1852, Best Val Loss: 2.1852\n",
      "Epoch 7/1000, Train Loss: 2.1056, Val Loss: 2.0652, Best Val Loss: 2.0652\n",
      "Epoch 8/1000, Train Loss: 1.9959, Val Loss: 1.9611, Best Val Loss: 1.9611\n",
      "Epoch 9/1000, Train Loss: 1.9147, Val Loss: 1.8893, Best Val Loss: 1.8893\n",
      "Epoch 11/1000, Train Loss: 1.8164, Val Loss: 1.7953, Best Val Loss: 1.7953\n",
      "Epoch 14/1000, Train Loss: 1.7573, Val Loss: 1.7731, Best Val Loss: 1.7731\n",
      "Epoch 15/1000, Train Loss: 1.7366, Val Loss: 1.7670, Best Val Loss: 1.7670\n",
      "Epoch 16/1000, Train Loss: 1.7328, Val Loss: 1.7456, Best Val Loss: 1.7456\n",
      "Epoch 17/1000, Train Loss: 1.7095, Val Loss: 1.7143, Best Val Loss: 1.7143\n",
      "Epoch 20/1000, Train Loss: 1.6931, Val Loss: 1.7065, Best Val Loss: 1.7065\n",
      "Epoch 21/1000, Train Loss: 1.6751, Val Loss: 1.6628, Best Val Loss: 1.6628\n",
      "Epoch 22/1000, Train Loss: 1.6805, Val Loss: 1.6556, Best Val Loss: 1.6556\n",
      "Epoch 28/1000, Train Loss: 1.6240, Val Loss: 1.6475, Best Val Loss: 1.6475\n",
      "Epoch 29/1000, Train Loss: 1.6338, Val Loss: 1.6111, Best Val Loss: 1.6111\n",
      "Epoch 31/1000, Train Loss: 1.6236, Val Loss: 1.6072, Best Val Loss: 1.6072\n",
      "Epoch 37/1000, Train Loss: 1.5827, Val Loss: 1.5677, Best Val Loss: 1.5677\n",
      "Epoch 43/1000, Train Loss: 1.5506, Val Loss: 1.5617, Best Val Loss: 1.5617\n",
      "Epoch 55/1000, Train Loss: 1.5172, Val Loss: 1.5589, Best Val Loss: 1.5589\n",
      "Epoch 57/1000, Train Loss: 1.5172, Val Loss: 1.5520, Best Val Loss: 1.5520\n",
      "Epoch 58/1000, Train Loss: 1.5034, Val Loss: 1.5428, Best Val Loss: 1.5428\n",
      "Epoch 59/1000, Train Loss: 1.5112, Val Loss: 1.5247, Best Val Loss: 1.5247\n",
      "Epoch 64/1000, Train Loss: 1.4891, Val Loss: 1.5101, Best Val Loss: 1.5101\n",
      "Epoch 71/1000, Train Loss: 1.4815, Val Loss: 1.4876, Best Val Loss: 1.4876\n",
      "Epoch 98/1000, Train Loss: 1.4380, Val Loss: 1.4735, Best Val Loss: 1.4735\n",
      "Epoch 104/1000, Train Loss: 1.4202, Val Loss: 1.4694, Best Val Loss: 1.4694\n",
      "Epoch 120/1000, Train Loss: 1.3970, Val Loss: 1.4655, Best Val Loss: 1.4655\n",
      "Epoch 123/1000, Train Loss: 1.3869, Val Loss: 1.4585, Best Val Loss: 1.4585\n",
      "Epoch 150/1000, Train Loss: 1.3569, Val Loss: 1.4471, Best Val Loss: 1.4471\n",
      "Early stopping at epoch 200, Best Val Loss: 1.4471\n",
      "Epistemic Variance: 1.062888\n",
      "Aleatoric Variance: 8.562996\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n",
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           2.002\n",
      "  RMSE          2.835\n",
      "  MDAE          1.488\n",
      "  MARPD         1.748\n",
      "  R2            0.859\n",
      "  Correlation   0.927\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.056\n",
      "  Mean-absolute Calibration Error       0.049\n",
      "  Miscalibration Area                   0.050\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.060\n",
      "     Group Size: 0.56 -- Calibration Error: 0.053\n",
      "     Group Size: 1.00 -- Calibration Error: 0.049\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.068\n",
      "     Group Size: 0.56 -- Calibration Error: 0.060\n",
      "     Group Size: 1.00 -- Calibration Error: 0.056\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   3.103\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   2.308\n",
      "  CRPS                      1.439\n",
      "  Check Score               0.726\n",
      "  Interval Score            7.231\n",
      "{'accuracy': {'mae': 2.002245351507819, 'rmse': np.float64(2.8345774111503172), 'mdae': 1.4884895263671893, 'marpd': np.float64(1.7482938314818661), 'r2': 0.8592881031088279, 'corr': np.float64(0.9270416616514542)}, 'avg_calibration': {'rms_cal': np.float64(0.05558319477866588), 'ma_cal': np.float64(0.04918965682976061), 'miscal_area': np.float64(0.04968652205026324)}, 'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.38697475, 0.06013439, 0.05824183, 0.0547757 , 0.05389836,\n",
      "       0.05312857, 0.05243129, 0.05138945, 0.05062409, 0.04918966]), 'adv_group_cali_stderr': array([0.0513166 , 0.00397666, 0.00298457, 0.00353275, 0.00170636,\n",
      "       0.00153221, 0.00081974, 0.00077715, 0.0003225 , 0.        ])}, 'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.44040378, 0.06839682, 0.06408941, 0.06141868, 0.06131169,\n",
      "       0.05982098, 0.05885718, 0.05805729, 0.05701867, 0.05558319]), 'adv_group_cali_stderr': array([0.04872949, 0.00300247, 0.00346317, 0.00339922, 0.00307647,\n",
      "       0.00174888, 0.00095905, 0.00065121, 0.0005797 , 0.        ])}}, 'sharpness': {'sharp': np.float32(3.102561)}, 'scoring_rule': {'nll': np.float64(2.3084278103554716), 'crps': np.float64(1.438561657426977), 'check': np.float64(0.7263557890662778), 'interval': np.float64(7.231437432069968)}}\n",
      "coverage: 0.9673171200615207, MPIW: 11.18751369228199\n",
      "Run 10 with seed 54321\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 152.4131, Val Loss: 5.4973, Best Val Loss: 5.4973\n",
      "Epoch 2/1000, Train Loss: 5.4104, Val Loss: 5.3215, Best Val Loss: 5.3215\n",
      "Epoch 3/1000, Train Loss: 5.1926, Val Loss: 4.9797, Best Val Loss: 4.9797\n",
      "Epoch 4/1000, Train Loss: 3.7707, Val Loss: 2.7194, Best Val Loss: 2.7194\n",
      "Epoch 5/1000, Train Loss: 2.5363, Val Loss: 2.3971, Best Val Loss: 2.3971\n",
      "Epoch 6/1000, Train Loss: 2.2746, Val Loss: 2.1909, Best Val Loss: 2.1909\n",
      "Epoch 7/1000, Train Loss: 2.1028, Val Loss: 2.0503, Best Val Loss: 2.0503\n",
      "Epoch 8/1000, Train Loss: 1.9975, Val Loss: 1.9679, Best Val Loss: 1.9679\n",
      "Epoch 9/1000, Train Loss: 1.9177, Val Loss: 1.8769, Best Val Loss: 1.8769\n",
      "Epoch 12/1000, Train Loss: 1.7952, Val Loss: 1.8083, Best Val Loss: 1.8083\n",
      "Epoch 13/1000, Train Loss: 1.7733, Val Loss: 1.7581, Best Val Loss: 1.7581\n",
      "Epoch 14/1000, Train Loss: 1.7434, Val Loss: 1.7503, Best Val Loss: 1.7503\n",
      "Epoch 15/1000, Train Loss: 1.7348, Val Loss: 1.7352, Best Val Loss: 1.7352\n",
      "Epoch 16/1000, Train Loss: 1.7079, Val Loss: 1.6967, Best Val Loss: 1.6967\n",
      "Epoch 21/1000, Train Loss: 1.6599, Val Loss: 1.6627, Best Val Loss: 1.6627\n",
      "Epoch 25/1000, Train Loss: 1.6361, Val Loss: 1.6332, Best Val Loss: 1.6332\n",
      "Epoch 30/1000, Train Loss: 1.6213, Val Loss: 1.6303, Best Val Loss: 1.6303\n",
      "Epoch 32/1000, Train Loss: 1.6022, Val Loss: 1.6084, Best Val Loss: 1.6084\n",
      "Epoch 37/1000, Train Loss: 1.5689, Val Loss: 1.5926, Best Val Loss: 1.5926\n",
      "Epoch 39/1000, Train Loss: 1.5633, Val Loss: 1.5770, Best Val Loss: 1.5770\n",
      "Epoch 43/1000, Train Loss: 1.5366, Val Loss: 1.5541, Best Val Loss: 1.5541\n",
      "Epoch 52/1000, Train Loss: 1.5093, Val Loss: 1.5287, Best Val Loss: 1.5287\n",
      "Epoch 61/1000, Train Loss: 1.4836, Val Loss: 1.5184, Best Val Loss: 1.5184\n",
      "Epoch 64/1000, Train Loss: 1.4781, Val Loss: 1.5135, Best Val Loss: 1.5135\n",
      "Epoch 70/1000, Train Loss: 1.4633, Val Loss: 1.5087, Best Val Loss: 1.5087\n",
      "Epoch 72/1000, Train Loss: 1.4626, Val Loss: 1.4880, Best Val Loss: 1.4880\n",
      "Epoch 84/1000, Train Loss: 1.4343, Val Loss: 1.4851, Best Val Loss: 1.4851\n",
      "Epoch 106/1000, Train Loss: 1.3927, Val Loss: 1.4675, Best Val Loss: 1.4675\n",
      "Early stopping at epoch 156, Best Val Loss: 1.4675\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 146.0775, Val Loss: 5.4760, Best Val Loss: 5.4760\n",
      "Epoch 2/1000, Train Loss: 5.4396, Val Loss: 5.3977, Best Val Loss: 5.3977\n",
      "Epoch 3/1000, Train Loss: 5.3660, Val Loss: 5.3297, Best Val Loss: 5.3297\n",
      "Epoch 4/1000, Train Loss: 5.2976, Val Loss: 5.2646, Best Val Loss: 5.2646\n",
      "Epoch 5/1000, Train Loss: 5.2290, Val Loss: 5.1900, Best Val Loss: 5.1900\n",
      "Epoch 6/1000, Train Loss: 5.1239, Val Loss: 5.0257, Best Val Loss: 5.0257\n",
      "Epoch 7/1000, Train Loss: 3.9331, Val Loss: 2.3822, Best Val Loss: 2.3822\n",
      "Epoch 8/1000, Train Loss: 2.2247, Val Loss: 2.1251, Best Val Loss: 2.1251\n",
      "Epoch 9/1000, Train Loss: 2.0386, Val Loss: 1.9758, Best Val Loss: 1.9758\n",
      "Epoch 10/1000, Train Loss: 1.9364, Val Loss: 1.9149, Best Val Loss: 1.9149\n",
      "Epoch 11/1000, Train Loss: 1.8751, Val Loss: 1.8834, Best Val Loss: 1.8834\n",
      "Epoch 12/1000, Train Loss: 1.8279, Val Loss: 1.8110, Best Val Loss: 1.8110\n",
      "Epoch 13/1000, Train Loss: 1.7973, Val Loss: 1.7856, Best Val Loss: 1.7856\n",
      "Epoch 14/1000, Train Loss: 1.7713, Val Loss: 1.7716, Best Val Loss: 1.7716\n",
      "Epoch 15/1000, Train Loss: 1.7540, Val Loss: 1.7536, Best Val Loss: 1.7536\n",
      "Epoch 16/1000, Train Loss: 1.7334, Val Loss: 1.7451, Best Val Loss: 1.7451\n",
      "Epoch 18/1000, Train Loss: 1.7076, Val Loss: 1.7398, Best Val Loss: 1.7398\n",
      "Epoch 19/1000, Train Loss: 1.6953, Val Loss: 1.7127, Best Val Loss: 1.7127\n",
      "Epoch 21/1000, Train Loss: 1.6747, Val Loss: 1.6839, Best Val Loss: 1.6839\n",
      "Epoch 22/1000, Train Loss: 1.6649, Val Loss: 1.6645, Best Val Loss: 1.6645\n",
      "Epoch 23/1000, Train Loss: 1.6552, Val Loss: 1.6418, Best Val Loss: 1.6418\n",
      "Epoch 27/1000, Train Loss: 1.6336, Val Loss: 1.6217, Best Val Loss: 1.6217\n",
      "Epoch 28/1000, Train Loss: 1.6210, Val Loss: 1.6201, Best Val Loss: 1.6201\n",
      "Epoch 35/1000, Train Loss: 1.5848, Val Loss: 1.6171, Best Val Loss: 1.6171\n",
      "Epoch 36/1000, Train Loss: 1.5788, Val Loss: 1.6120, Best Val Loss: 1.6120\n",
      "Epoch 38/1000, Train Loss: 1.5683, Val Loss: 1.6017, Best Val Loss: 1.6017\n",
      "Epoch 40/1000, Train Loss: 1.5591, Val Loss: 1.6008, Best Val Loss: 1.6008\n",
      "Epoch 41/1000, Train Loss: 1.5509, Val Loss: 1.5644, Best Val Loss: 1.5644\n",
      "Epoch 49/1000, Train Loss: 1.5142, Val Loss: 1.5581, Best Val Loss: 1.5581\n",
      "Epoch 52/1000, Train Loss: 1.5056, Val Loss: 1.5446, Best Val Loss: 1.5446\n",
      "Epoch 54/1000, Train Loss: 1.5026, Val Loss: 1.5352, Best Val Loss: 1.5352\n",
      "Epoch 56/1000, Train Loss: 1.4824, Val Loss: 1.5291, Best Val Loss: 1.5291\n",
      "Epoch 57/1000, Train Loss: 1.4859, Val Loss: 1.5276, Best Val Loss: 1.5276\n",
      "Epoch 62/1000, Train Loss: 1.4776, Val Loss: 1.5249, Best Val Loss: 1.5249\n",
      "Epoch 63/1000, Train Loss: 1.4667, Val Loss: 1.4974, Best Val Loss: 1.4974\n",
      "Epoch 76/1000, Train Loss: 1.4394, Val Loss: 1.4859, Best Val Loss: 1.4859\n",
      "Epoch 97/1000, Train Loss: 1.4048, Val Loss: 1.4665, Best Val Loss: 1.4665\n",
      "Epoch 100/1000, Train Loss: 1.3943, Val Loss: 1.4539, Best Val Loss: 1.4539\n",
      "Early stopping at epoch 150, Best Val Loss: 1.4539\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 126.4248, Val Loss: 5.4964, Best Val Loss: 5.4964\n",
      "Epoch 2/1000, Train Loss: 5.4576, Val Loss: 5.4176, Best Val Loss: 5.4176\n",
      "Epoch 3/1000, Train Loss: 5.3856, Val Loss: 5.3495, Best Val Loss: 5.3495\n",
      "Epoch 4/1000, Train Loss: 5.3170, Val Loss: 5.2833, Best Val Loss: 5.2833\n",
      "Epoch 5/1000, Train Loss: 5.2476, Val Loss: 5.2095, Best Val Loss: 5.2095\n",
      "Epoch 6/1000, Train Loss: 5.1469, Val Loss: 5.0590, Best Val Loss: 5.0590\n",
      "Epoch 7/1000, Train Loss: 4.3259, Val Loss: 2.4461, Best Val Loss: 2.4461\n",
      "Epoch 8/1000, Train Loss: 2.2752, Val Loss: 2.2072, Best Val Loss: 2.2072\n",
      "Epoch 9/1000, Train Loss: 2.0858, Val Loss: 2.0422, Best Val Loss: 2.0422\n",
      "Epoch 10/1000, Train Loss: 1.9747, Val Loss: 1.9449, Best Val Loss: 1.9449\n",
      "Epoch 11/1000, Train Loss: 1.9025, Val Loss: 1.8930, Best Val Loss: 1.8930\n",
      "Epoch 12/1000, Train Loss: 1.8476, Val Loss: 1.8620, Best Val Loss: 1.8620\n",
      "Epoch 13/1000, Train Loss: 1.8159, Val Loss: 1.8171, Best Val Loss: 1.8171\n",
      "Epoch 15/1000, Train Loss: 1.7673, Val Loss: 1.7643, Best Val Loss: 1.7643\n",
      "Epoch 16/1000, Train Loss: 1.7496, Val Loss: 1.7258, Best Val Loss: 1.7258\n",
      "Epoch 20/1000, Train Loss: 1.7061, Val Loss: 1.7030, Best Val Loss: 1.7030\n",
      "Epoch 24/1000, Train Loss: 1.6715, Val Loss: 1.6985, Best Val Loss: 1.6985\n",
      "Epoch 25/1000, Train Loss: 1.6657, Val Loss: 1.6516, Best Val Loss: 1.6516\n",
      "Epoch 29/1000, Train Loss: 1.6429, Val Loss: 1.6198, Best Val Loss: 1.6198\n",
      "Epoch 33/1000, Train Loss: 1.6222, Val Loss: 1.6186, Best Val Loss: 1.6186\n",
      "Epoch 34/1000, Train Loss: 1.6197, Val Loss: 1.6018, Best Val Loss: 1.6018\n",
      "Epoch 38/1000, Train Loss: 1.5937, Val Loss: 1.5734, Best Val Loss: 1.5734\n",
      "Epoch 42/1000, Train Loss: 1.5683, Val Loss: 1.5628, Best Val Loss: 1.5628\n",
      "Epoch 53/1000, Train Loss: 1.5284, Val Loss: 1.5394, Best Val Loss: 1.5394\n",
      "Epoch 55/1000, Train Loss: 1.5250, Val Loss: 1.5310, Best Val Loss: 1.5310\n",
      "Epoch 67/1000, Train Loss: 1.4811, Val Loss: 1.5134, Best Val Loss: 1.5134\n",
      "Epoch 72/1000, Train Loss: 1.4653, Val Loss: 1.5109, Best Val Loss: 1.5109\n",
      "Epoch 73/1000, Train Loss: 1.4692, Val Loss: 1.5081, Best Val Loss: 1.5081\n",
      "Epoch 80/1000, Train Loss: 1.4525, Val Loss: 1.4917, Best Val Loss: 1.4917\n",
      "Epoch 88/1000, Train Loss: 1.4276, Val Loss: 1.4829, Best Val Loss: 1.4829\n",
      "Epoch 97/1000, Train Loss: 1.4144, Val Loss: 1.4649, Best Val Loss: 1.4649\n",
      "Epoch 111/1000, Train Loss: 1.3830, Val Loss: 1.4569, Best Val Loss: 1.4569\n",
      "Epoch 122/1000, Train Loss: 1.3765, Val Loss: 1.4561, Best Val Loss: 1.4561\n",
      "Early stopping at epoch 172, Best Val Loss: 1.4561\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 146.9858, Val Loss: 5.5283, Best Val Loss: 5.5283\n",
      "Epoch 2/1000, Train Loss: 5.3990, Val Loss: 5.2957, Best Val Loss: 5.2957\n",
      "Epoch 3/1000, Train Loss: 5.2424, Val Loss: 5.1832, Best Val Loss: 5.1832\n",
      "Epoch 4/1000, Train Loss: 5.0501, Val Loss: 4.7764, Best Val Loss: 4.7764\n",
      "Epoch 5/1000, Train Loss: 3.3445, Val Loss: 2.5178, Best Val Loss: 2.5178\n",
      "Epoch 6/1000, Train Loss: 2.3426, Val Loss: 2.2378, Best Val Loss: 2.2378\n",
      "Epoch 7/1000, Train Loss: 2.1263, Val Loss: 2.0592, Best Val Loss: 2.0592\n",
      "Epoch 8/1000, Train Loss: 2.0106, Val Loss: 1.9641, Best Val Loss: 1.9641\n",
      "Epoch 9/1000, Train Loss: 1.9299, Val Loss: 1.9403, Best Val Loss: 1.9403\n",
      "Epoch 10/1000, Train Loss: 1.8691, Val Loss: 1.8440, Best Val Loss: 1.8440\n",
      "Epoch 11/1000, Train Loss: 1.8297, Val Loss: 1.8204, Best Val Loss: 1.8204\n",
      "Epoch 12/1000, Train Loss: 1.8044, Val Loss: 1.7902, Best Val Loss: 1.7902\n",
      "Epoch 13/1000, Train Loss: 1.7817, Val Loss: 1.7798, Best Val Loss: 1.7798\n",
      "Epoch 14/1000, Train Loss: 1.7595, Val Loss: 1.7365, Best Val Loss: 1.7365\n",
      "Epoch 17/1000, Train Loss: 1.7168, Val Loss: 1.7000, Best Val Loss: 1.7000\n",
      "Epoch 20/1000, Train Loss: 1.6886, Val Loss: 1.6993, Best Val Loss: 1.6993\n",
      "Epoch 24/1000, Train Loss: 1.6559, Val Loss: 1.6444, Best Val Loss: 1.6444\n",
      "Epoch 26/1000, Train Loss: 1.6516, Val Loss: 1.6173, Best Val Loss: 1.6173\n",
      "Epoch 31/1000, Train Loss: 1.6101, Val Loss: 1.5984, Best Val Loss: 1.5984\n",
      "Epoch 32/1000, Train Loss: 1.6014, Val Loss: 1.5922, Best Val Loss: 1.5922\n",
      "Epoch 39/1000, Train Loss: 1.5719, Val Loss: 1.5922, Best Val Loss: 1.5922\n",
      "Epoch 43/1000, Train Loss: 1.5505, Val Loss: 1.5861, Best Val Loss: 1.5861\n",
      "Epoch 45/1000, Train Loss: 1.5371, Val Loss: 1.5780, Best Val Loss: 1.5780\n",
      "Epoch 50/1000, Train Loss: 1.5215, Val Loss: 1.5342, Best Val Loss: 1.5342\n",
      "Epoch 62/1000, Train Loss: 1.4819, Val Loss: 1.5297, Best Val Loss: 1.5297\n",
      "Epoch 65/1000, Train Loss: 1.4765, Val Loss: 1.5209, Best Val Loss: 1.5209\n",
      "Epoch 67/1000, Train Loss: 1.4625, Val Loss: 1.5205, Best Val Loss: 1.5205\n",
      "Epoch 74/1000, Train Loss: 1.4435, Val Loss: 1.5124, Best Val Loss: 1.5124\n",
      "Epoch 77/1000, Train Loss: 1.4471, Val Loss: 1.5102, Best Val Loss: 1.5102\n",
      "Epoch 81/1000, Train Loss: 1.4348, Val Loss: 1.5072, Best Val Loss: 1.5072\n",
      "Epoch 85/1000, Train Loss: 1.4173, Val Loss: 1.4914, Best Val Loss: 1.4914\n",
      "Epoch 86/1000, Train Loss: 1.4253, Val Loss: 1.4848, Best Val Loss: 1.4848\n",
      "Epoch 95/1000, Train Loss: 1.4056, Val Loss: 1.4781, Best Val Loss: 1.4781\n",
      "Epoch 96/1000, Train Loss: 1.4081, Val Loss: 1.4685, Best Val Loss: 1.4685\n",
      "Early stopping at epoch 146, Best Val Loss: 1.4685\n",
      "Using device: cuda\n",
      "Epoch 1/1000, Train Loss: 164.6082, Val Loss: 5.4890, Best Val Loss: 5.4890\n",
      "Epoch 2/1000, Train Loss: 5.4259, Val Loss: 5.3738, Best Val Loss: 5.3738\n",
      "Epoch 3/1000, Train Loss: 5.3381, Val Loss: 5.2973, Best Val Loss: 5.2973\n",
      "Epoch 4/1000, Train Loss: 5.2605, Val Loss: 5.2174, Best Val Loss: 5.2174\n",
      "Epoch 5/1000, Train Loss: 5.1497, Val Loss: 5.0421, Best Val Loss: 5.0421\n",
      "Epoch 6/1000, Train Loss: 3.9402, Val Loss: 2.4329, Best Val Loss: 2.4329\n",
      "Epoch 7/1000, Train Loss: 2.2445, Val Loss: 2.1191, Best Val Loss: 2.1191\n",
      "Epoch 8/1000, Train Loss: 2.0453, Val Loss: 2.0039, Best Val Loss: 2.0039\n",
      "Epoch 9/1000, Train Loss: 1.9464, Val Loss: 1.9239, Best Val Loss: 1.9239\n",
      "Epoch 10/1000, Train Loss: 1.8794, Val Loss: 1.8630, Best Val Loss: 1.8630\n",
      "Epoch 11/1000, Train Loss: 1.8333, Val Loss: 1.8486, Best Val Loss: 1.8486\n",
      "Epoch 12/1000, Train Loss: 1.7977, Val Loss: 1.8070, Best Val Loss: 1.8070\n",
      "Epoch 14/1000, Train Loss: 1.7495, Val Loss: 1.7782, Best Val Loss: 1.7782\n",
      "Epoch 15/1000, Train Loss: 1.7385, Val Loss: 1.7389, Best Val Loss: 1.7389\n",
      "Epoch 17/1000, Train Loss: 1.7139, Val Loss: 1.7188, Best Val Loss: 1.7188\n",
      "Epoch 18/1000, Train Loss: 1.6987, Val Loss: 1.7057, Best Val Loss: 1.7057\n",
      "Epoch 20/1000, Train Loss: 1.6765, Val Loss: 1.6951, Best Val Loss: 1.6951\n",
      "Epoch 21/1000, Train Loss: 1.6690, Val Loss: 1.6537, Best Val Loss: 1.6537\n",
      "Epoch 23/1000, Train Loss: 1.6566, Val Loss: 1.6372, Best Val Loss: 1.6372\n",
      "Epoch 24/1000, Train Loss: 1.6475, Val Loss: 1.6266, Best Val Loss: 1.6266\n",
      "Epoch 26/1000, Train Loss: 1.6442, Val Loss: 1.6238, Best Val Loss: 1.6238\n",
      "Epoch 31/1000, Train Loss: 1.5993, Val Loss: 1.6236, Best Val Loss: 1.6236\n",
      "Epoch 32/1000, Train Loss: 1.5962, Val Loss: 1.6007, Best Val Loss: 1.6007\n",
      "Epoch 34/1000, Train Loss: 1.5787, Val Loss: 1.5834, Best Val Loss: 1.5834\n",
      "Epoch 36/1000, Train Loss: 1.5712, Val Loss: 1.5636, Best Val Loss: 1.5636\n",
      "Epoch 45/1000, Train Loss: 1.5251, Val Loss: 1.5614, Best Val Loss: 1.5614\n",
      "Epoch 46/1000, Train Loss: 1.5308, Val Loss: 1.5598, Best Val Loss: 1.5598\n",
      "Epoch 48/1000, Train Loss: 1.5131, Val Loss: 1.5474, Best Val Loss: 1.5474\n",
      "Epoch 55/1000, Train Loss: 1.4939, Val Loss: 1.5389, Best Val Loss: 1.5389\n",
      "Epoch 61/1000, Train Loss: 1.4689, Val Loss: 1.5110, Best Val Loss: 1.5110\n",
      "Epoch 70/1000, Train Loss: 1.4478, Val Loss: 1.4975, Best Val Loss: 1.4975\n",
      "Epoch 80/1000, Train Loss: 1.4181, Val Loss: 1.4873, Best Val Loss: 1.4873\n",
      "Epoch 87/1000, Train Loss: 1.4069, Val Loss: 1.4670, Best Val Loss: 1.4670\n",
      "Early stopping at epoch 137, Best Val Loss: 1.4670\n",
      "Epistemic Variance: 0.883687\n",
      "Aleatoric Variance: 8.539065\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n",
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           2.000\n",
      "  RMSE          2.804\n",
      "  MDAE          1.498\n",
      "  MARPD         1.746\n",
      "  R2            0.862\n",
      "  Correlation   0.929\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.034\n",
      "  Mean-absolute Calibration Error       0.030\n",
      "  Miscalibration Area                   0.030\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.041\n",
      "     Group Size: 0.56 -- Calibration Error: 0.033\n",
      "     Group Size: 1.00 -- Calibration Error: 0.030\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.051\n",
      "     Group Size: 0.56 -- Calibration Error: 0.039\n",
      "     Group Size: 1.00 -- Calibration Error: 0.034\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   3.070\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   2.287\n",
      "  CRPS                      1.430\n",
      "  Check Score               0.722\n",
      "  Interval Score            7.155\n",
      "{'accuracy': {'mae': 2.000357220558138, 'rmse': np.float64(2.804365292208052), 'mdae': 1.4979763671875048, 'marpd': np.float64(1.7458260402008459), 'r2': 0.8622716511926961, 'corr': np.float64(0.9289494033230589)}, 'avg_calibration': {'rms_cal': np.float64(0.03353264538314034), 'ma_cal': np.float64(0.029502872617358785), 'miscal_area': np.float64(0.029792291844238053)}, 'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.33657576, 0.04091656, 0.03878122, 0.03648396, 0.0337211 ,\n",
      "       0.0330841 , 0.0320073 , 0.03181464, 0.0310446 , 0.02950287]), 'adv_group_cali_stderr': array([6.27262914e-02, 4.19769565e-03, 2.92451592e-03, 1.97986875e-03,\n",
      "       1.77260800e-03, 9.33724881e-04, 9.66312851e-04, 7.03263147e-04,\n",
      "       5.39564710e-04, 3.65711820e-18])}, 'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'adv_group_cali_mean': array([0.40850285, 0.05072484, 0.04288856, 0.04112198, 0.03922002,\n",
      "       0.03901502, 0.03641525, 0.03623458, 0.03516878, 0.03353265]), 'adv_group_cali_stderr': array([0.06658946, 0.00886003, 0.00367999, 0.00228225, 0.00258743,\n",
      "       0.00188458, 0.00123757, 0.00078242, 0.00044609, 0.        ])}}, 'sharpness': {'sharp': np.float32(3.0696504)}, 'scoring_rule': {'nll': np.float64(2.2872669730478723), 'crps': np.float64(1.4295357139213063), 'check': np.float64(0.7218040163903423), 'interval': np.float64(7.154689907372937)}}\n",
      "coverage: 0.9562626165529174, MPIW: 10.516738976388988\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "predictions_list = []\n",
    "list_of_seeds = [42, 123, 777, 2024, 5250, 8888, 9876, 10001, 31415, 54321]\n",
    "DE_prediction_path = r\"C:\\Users\\test\\Masterarbeit\\models\\Modelresults\\Deep Ensembles\"\n",
    "DE_result_path = r\"C:\\Users\\test\\OneDrive\\Master Management und Engineering\\Masterarbeit\\Experimente\\Evaluation\\10 Runs\\Deep Ensembles\"\n",
    "\n",
    "for run, seed in enumerate(list_of_seeds):\n",
    "\n",
    "    print(f\"Run {run+1} with seed {seed}\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    #create an ensemble of 5 networks with the defined net architecture and optimizer\n",
    "    nets_ops = create_ensemble(5, input_dim = X_train.shape[1], hidden_dims=[320,224,156], \n",
    "                            do_rate=0, loss_type='heteroscedastic', lr=0.0004, weight_decay=0.0001)\n",
    "\n",
    "    # lists to store the output means and log variances of each network in the ensemble\n",
    "    outputs_mean = []\n",
    "    outputs_log_var = []\n",
    "\n",
    "    #train the ensemble of networks and make predictions on the test set\n",
    "    for net, ops in nets_ops:\n",
    "        model = train_model(model= net, X_train_tensor=X_train_tensor, y_train_tensor=y_train_tensor,\n",
    "                            X_val_tensor= X_val_tensor, y_val_tensor=y_val_tensor, batch_size=64, \n",
    "                            optimizer=ops, n_epochs=1000, patience=50, loss_type='heteroscedastic',\n",
    "                            )\n",
    "\n",
    "        # set the model to evaluation mode and make predictions on the test set\n",
    "        model.eval()   \n",
    "        with torch.no_grad():\n",
    "            output_mean, output_log_var = model(X_test_tensor.to(device))\n",
    "            # Detach and convert to numpy arrays\n",
    "            output_mean_np, output_log_var_np = output_mean.detach().cpu().numpy(), output_log_var.detach().cpu().numpy()\n",
    "            outputs_mean.append(output_mean_np)\n",
    "            outputs_log_var.append(np.exp(output_log_var_np))\n",
    "\n",
    "    outputs_mean = np.array(outputs_mean)\n",
    "    outputs_log_var = np.array(outputs_log_var)\n",
    "\n",
    "    # Calculate the mean and standard deviation of the predictions on the test data\n",
    "    DE_mean = np.mean(outputs_mean, axis = 0).reshape(-1) # reshape to 1D array\n",
    "\n",
    "    # Calculate epistemic\n",
    "    DE_epistemic_var_heteroscedastic = np.var(outputs_mean, axis=0)\n",
    "    print(f\"Epistemic Variance: {DE_epistemic_var_heteroscedastic.mean():.6f}\")\n",
    "\n",
    "    # Calculate aleatoric variance (heteroscedastic)\n",
    "    DE_aleatoric_var_heteroscedastic = np.mean(outputs_log_var, axis=0)\n",
    "    print(f\"Aleatoric Variance: {DE_aleatoric_var_heteroscedastic.mean():.6f}\")\n",
    "\n",
    "    # Calculate total standard deviation\n",
    "    DE_std = np.sqrt(DE_epistemic_var_heteroscedastic + DE_aleatoric_var_heteroscedastic).reshape(-1) # reshape to 1D array\n",
    "\n",
    "    # Calculate and print all metrics inclunding RMSE, MAE, R²-Score, NLL, CRPS\n",
    "    pnn_metrics = uct.metrics.get_all_metrics(DE_mean, DE_std, y_test)\n",
    "    print(pnn_metrics)\n",
    "\n",
    "    # use own function to calculate coverage and MPIW\n",
    "    ev_intervals = evaluate_intervals(DE_mean, DE_std, y_test, coverage=0.95)\n",
    "    print(f'coverage: {ev_intervals[\"coverage\"]}, MPIW: {ev_intervals[\"MPIW\"]}')\n",
    "\n",
    "    predictions_per_run = {\n",
    "        'mean_prediction': DE_mean,\n",
    "        'std_prediction': DE_std,\n",
    "    }\n",
    "\n",
    "    results_per_run = {\n",
    "    'RMSE': pnn_metrics['accuracy']['rmse'],\n",
    "    'MAE': pnn_metrics['accuracy']['mae'],\n",
    "    'R2': pnn_metrics['accuracy']['r2'], \n",
    "    'Correlation' : pnn_metrics['accuracy']['corr'],\n",
    "    'NLL': pnn_metrics['scoring_rule']['nll'],\n",
    "    'CRPS': pnn_metrics['scoring_rule']['crps'],\n",
    "    'coverage': ev_intervals[\"coverage\"],\n",
    "    'MPIW': ev_intervals[\"MPIW\"],\n",
    "    }\n",
    "\n",
    "    predictions_list.append(predictions_per_run)\n",
    "    results_list.append(results_per_run)\n",
    "#save the predictions \n",
    "with open(os.path.join(DE_prediction_path, \"DE_predictions_list.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(predictions_list, f)\n",
    "\n",
    "#save the results in an excel file\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.to_excel(os.path.join(DE_result_path, \"DE_results.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b02d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_prediction_path = r\"C:\\Users\\test\\Masterarbeit\\models\\Modelresults\\Deep Ensembles\"\n",
    "DE_result_path = r\"C:\\Users\\test\\OneDrive\\Master Management und Engineering\\Masterarbeit\\Experimente\\Evaluation\\10 Runs\\Deep Ensembles\"\n",
    "with open(os.path.join(DE_prediction_path, \"DE_predictions_list.pkl\"), \"rb\") as f:\n",
    "    predictions_list = pickle.load(f)\n",
    "\n",
    "mean_list = []\n",
    "std_list = []\n",
    "\n",
    "for id, run in enumerate(predictions_list):\n",
    "    # extract mean and std predictions\n",
    "    mean = run['mean_prediction']\n",
    "    std = run['std_prediction']\n",
    "    \n",
    "    # append to lists\n",
    "    mean_list.append(mean)\n",
    "    std_list.append(std)\n",
    "    \n",
    "    # calibration Curve with UCT\n",
    "    uct.viz.plot_calibration(mean, std, y_test)\n",
    "    plt.savefig(os.path.join(DE_result_path, f\"calibration_run_{id+1}.svg\"), format ='svg')\n",
    "    plt.savefig(os.path.join(DE_result_path, f\"calibration_run_{id+1}.png\"), format ='png')\n",
    "    plt.close()\n",
    "\n",
    "    # adversarial group calibration\n",
    "    uct.viz.plot_adversarial_group_calibration(mean, std, y_test)\n",
    "    plt.savefig(os.path.join(DE_result_path, f\"adversarial_group_calibration_run_{id+1}.svg\"), format ='svg')\n",
    "    plt.savefig(os.path.join(DE_result_path, f\"adversarial_group_calibration_run_{id+1}.png\"), format ='png')\n",
    "    plt.close()\n",
    "\n",
    "# predictions_list enthält pro Run ein Array mit 10403 Werten\n",
    "mean_matrix = np.array(mean_list)  # Shape: (n_runs, 10403)\n",
    "std_matrix = np.array(std_list)    # Shape: (n_runs, 10403)\n",
    "\n",
    "# Mittelwert und Std für jeden Datenpunkt über alle Runs\n",
    "mean_per_datapoint = np.mean(mean_matrix, axis=0)  # Shape: (10403,)\n",
    "std_per_datapoint = np.mean(std_matrix, axis=0)    # Shape: (10403,)\n",
    "\n",
    "# calibration Curve with UCT\n",
    "uct.viz.plot_calibration(mean_per_datapoint, std_per_datapoint, y_test)\n",
    "plt.savefig(os.path.join(DE_result_path, \"calibration_run_mean.svg\"), format ='svg')\n",
    "plt.savefig(os.path.join(DE_result_path, \"calibration_run_mean.png\"), format ='png')\n",
    "plt.close()\n",
    "\n",
    "# adversarial group calibration\n",
    "uct.viz.plot_adversarial_group_calibration(mean_per_datapoint, std_per_datapoint, y_test)\n",
    "plt.savefig(os.path.join(DE_result_path, \"adversarial_group_calibration_run_mean.svg\"), format ='svg')\n",
    "plt.savefig(os.path.join(DE_result_path, \"adversarial_group_calibration_run_mean.png\"), format ='png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
