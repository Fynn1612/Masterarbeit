{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0981020c",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# path for desktop PC\n",
    "sys.path.append(r'C:\\Users\\test\\Masterarbeit')\n",
    "# path for surface PC\n",
    "#sys.path.append(r'C:\\Users\\Surface\\Masterarbeit')\n",
    "\n",
    "%matplotlib inline\n",
    "# path for desktop PC\n",
    "path = r\"C:\\Users\\test\\Masterarbeit\\data\\WZ_2_Feature_Engineered_Fynn6.xlsx\"\n",
    "# path for surface PC\n",
    "#path = r\"C:\\Users\\Surface\\Masterarbeit\\data\\Produktionsdaten\\WZ_2_Feature_Engineered_Fynn6.xlsx\"\n",
    "\n",
    "df = pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405e4f5",
   "metadata": {},
   "source": [
    "Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import prep\n",
    "import importlib\n",
    "importlib.reload(prep)\n",
    "\n",
    "# set the target variable\n",
    "target = 'C1_V01_delta_kan'\n",
    "#print(df.columns)\n",
    "# get the numerical features\n",
    "data_num = df.drop(['C1_V01_delta_kan'], axis = 1, inplace=False)\n",
    "#print(data_num.columns)\n",
    "# get the target values\n",
    "data_labels = df[target].to_numpy()\n",
    "\n",
    "# split the data into training, validation and test sets\n",
    "# 60% training, 20%, validation, 20% test\n",
    "X_temp, X_test_prep, y_temp, y_test = train_test_split(data_num, data_labels, test_size= 0.2, random_state=42)\n",
    "X_train_prep, X_val_prep, y_train, y_val = train_test_split(X_temp, y_temp, test_size= 0.25, random_state=42)\n",
    "\n",
    "# use coustom function \"cat_transform\" from prep.py to map the categorical features with their frequencies\n",
    "X_train_prep, X_val_prep, X_test_prep = prep.cat_transform(X_train_prep, X_val_prep, X_test_prep, ['BT_NR', 'STP_NR'])\n",
    "print(X_train_prep.columns)\n",
    "\n",
    "# pipeline for preprocessing the data\n",
    "# Standard Scaler for distribution with 0 mean and 1 std., normal distributed data\n",
    "data_pipeline = Pipeline([\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# get the feature names after preprocessing for the feature importance\n",
    "feature_names = X_train_prep.columns\n",
    "\n",
    "# fit the pipeline to the data and transform it\n",
    "X_train = data_pipeline.fit_transform(X_train_prep)\n",
    "X_val = data_pipeline.transform(X_val_prep)\n",
    "X_test = data_pipeline.transform(X_test_prep)\n",
    "\n",
    "# convert the data to PyTorch tensors\n",
    "# and add an extra dimension for the target variable\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float() \n",
    "y_train_tensor = torch.from_numpy(y_train).float().unsqueeze(1) # Add extra dimension for compatibility\n",
    "y_val_tensor = torch.from_numpy(y_val).float().unsqueeze(1)\n",
    "y_test_tensor = torch.from_numpy(y_test).float().unsqueeze(1)\n",
    "\n",
    "# print the shapes of the data\n",
    "print(data_num.shape, X_train_tensor.shape, X_val_tensor.shape, X_test_tensor.shape)\n",
    "# print(pd.DataFrame(X_train, columns=feature_names).describe())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
