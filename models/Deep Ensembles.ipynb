{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0981020c",
   "metadata": {},
   "source": [
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23cc680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "if os.environ['COMPUTERNAME'] == 'FYNN':            # name of surface PC\n",
    "    sys.path.append(r'C:\\Users\\Surface\\Masterarbeit')\n",
    "elif os.environ['COMPUTERNAME'] == 'FYNNS-PC':  # desktop name\n",
    "    sys.path.append(r'C:\\Users\\test\\Masterarbeit')\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"Unbekannter Computername: \" + os.environ['COMPUTERNAME'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405e4f5",
   "metadata": {},
   "source": [
    "Load, Transform and Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e35e781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31206, 37]) torch.Size([10402, 37]) torch.Size([10403, 37])\n",
      "Index(['Ist_Blechhaltergewicht', 'Ist_Gegenhaltekraft_HL_1', 'T2', 'K1', 'T20',\n",
      "       'K7', 'T23', 'K8', 'DS_10', 'GS_10', 'SD_10', 'LS_10', 'PP_10', 'TT_10',\n",
      "       'TM5_10', 'RF_10', 'TD_10', 'is_weekend', 'dayofweek_sin',\n",
      "       'dayofweek_cos', 'month_sin', 'month_cos', 'hour_sin', 'hour_cos',\n",
      "       'day_sin', 'day_cos', 'quarter_sin', 'quarter_cos', 'week_sin',\n",
      "       'week_cos', 'Diff_Hubzahl', 'Diff_Ziehtiefe',\n",
      "       'Diff_Ziehkissenverstellu', 'Diff_Stoesselverstellung-mm',\n",
      "       'Diff_Gewichtsausgleich', 'BT_NR_freq', 'STP_NR_freq'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import data_prep\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, feature_names = data_prep.load_tranform_and_split_data('C1_V01_delta_kan', split_ratio=(0.6, 0.2, 0.2))\n",
    "\n",
    "# convert the data to PyTorch tensors\n",
    "# and add an extra dimension for the target variable\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float() \n",
    "y_train_tensor = torch.from_numpy(y_train).float().unsqueeze(1) # Add extra dimension for compatibility\n",
    "y_val_tensor = torch.from_numpy(y_val).float().unsqueeze(1)\n",
    "y_test_tensor = torch.from_numpy(y_test).float().unsqueeze(1)\n",
    "print(X_train_tensor.shape, X_val_tensor.shape, X_test_tensor.shape)\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89386231",
   "metadata": {},
   "source": [
    "Create an Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cac12561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "(Custom_NN_Model(\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=37, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (mean_layer): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (var_layer): Linear(in_features=16, out_features=1, bias=True)\n",
      "), Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0042\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "import NN_model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# number of networks\n",
    "n = 5\n",
    "nets_ops = []\n",
    "for i in range(n):\n",
    "    net = NN_model.Custom_NN_Model(input_dim=X_train.shape[1], hidden_dims=[256, 64, 16], output_dim=1, do_rate=0, loss_type='heteroscedastic').to(device)  # Create model instance and move to device\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0042, weight_decay=0.0001)  # Create optimizer\n",
    "    nets_ops.append((net, optimizer))\n",
    "    \n",
    "print(nets_ops[0])  # Print the first model to check if it was created correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b2146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "Prediction for first training sample: -0.012562423944473267\n",
      "Prediction for first training sample: 0.0683891698718071\n"
     ]
    }
   ],
   "source": [
    "model = NN_model.Custom_NN_Model(input_dim=X_train.shape[1], hidden_dims=[256, 64, 16], output_dim=1, do_rate=0, loss_type='heteroscedastic').to(device)  # Create model instance and move to device\n",
    "\n",
    "prediction = model(X_train_tensor[0])\n",
    "mu, log_var = prediction = model(X_train_tensor[0])\n",
    "print(type(prediction))\n",
    "print(f\"Prediction for first training sample: {prediction[0].item()}\")\n",
    "print(f\"Prediction for first training sample: {prediction[1].item()}\")\n",
    "print(mu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cebde7",
   "metadata": {},
   "source": [
    "Method for Training the Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, optimizer, batch_size=64, n_epochs=1000, patience=20, device = 'cpu'):\n",
    "    \n",
    "    # DataLoader for batching the data\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "        \n",
    "    # Early Stopping values\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "    net.to(device)  # Move model to device\n",
    "    for epoch in range(n_epochs):\n",
    "        net.train()\n",
    "        batch_losses = []\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = NN_model.heteroscedastic_loss(net, X_batch, y_batch) # does a forward pass and computes the loss  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "        loss_history.append(loss.item())\n",
    "            \n",
    "        # calculate validation loss\n",
    "        model.eval()                            # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Forward pass\n",
    "            val_loss = NN_model.heteroscedastic_loss(model, X_val_tensor, y_val_tensor)\n",
    "                \n",
    "            val_loss_history.append(val_loss.item())\n",
    "            \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {np.mean(batch_losses):.4f}, Val Loss: {val_loss.item():.4f}, Best Val Loss: {best_val_loss:.4f}\")\n",
    "            \n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}, Best Val Loss: {best_val_loss:.4f}\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break    \n",
    "            \n",
    "    \n",
    "    outputs = [net(X_test_tensor)]\n",
    "    outputs = np.hstack(torch.stack(outputs).detach().numpy())\n",
    "    \n",
    "    return loss_history, val_loss_history, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd0789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 57.6877, Val Loss: 6416.5942, Best Val Loss: 6416.5942\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m val_loss_histories = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m net, ops \u001b[38;5;129;01min\u001b[39;00m nets_ops:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     loss_history, val_loss_history, outputs = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mops\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     outputs.append(outputs)\n\u001b[32m      7\u001b[39m     loss_histories.append(loss_history)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(net, optimizer, batch_size, n_epochs, patience)\u001b[39m\n\u001b[32m     19\u001b[39m optimizer.zero_grad()\n\u001b[32m     20\u001b[39m loss = NN_model.heteroscedastic_loss(net, X_batch, y_batch) \u001b[38;5;66;03m# does a forward pass and computes the loss  \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m optimizer.step()\n\u001b[32m     23\u001b[39m batch_losses.append(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Surface\\Masterarbeit\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Surface\\Masterarbeit\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Surface\\Masterarbeit\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "outputs = []\n",
    "loss_histories = []\n",
    "val_loss_histories = []\n",
    "for net, ops in nets_ops:\n",
    "    loss_history, val_loss_history, outputs = train_model(net, ops, device=device)\n",
    "    outputs.append(outputs)\n",
    "    loss_histories.append(loss_history)\n",
    "    val_loss_histories.append(val_loss_history)\n",
    "    \n",
    "outputs = np.vstack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a845ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! handling the test data\n",
    "# Select a random subset of test data for visualization\n",
    "num_points = 100\n",
    "np.random.seed(42)\n",
    "\n",
    "random_indices = np.random.choice(len(X_test), num_points, replace=False)\n",
    "random_indices.sort()\n",
    "\n",
    "# Calculate the mean and standard deviation of the predictions on the test data\n",
    "mean_test_pred_heteroscedastic = np.mean(outputs, axis = 0)\n",
    "\n",
    "# # Calculate epistemic\n",
    "# epistemic_var_heteroscedastic = np.var(y_test_pred_heteroscedastic_mean, axis=0)\n",
    "# print(f\"Epistemic Variance: {epistemic_var_heteroscedastic.mean():.6f}\")\n",
    "\n",
    "# # Calculate aleatoric variance (heteroscedastic)\n",
    "# aleatoric_var_heteroscedastic = np.mean(y_test_pred_heteroscedastic_var, axis=0)\n",
    "# print(f\"Aleatoric Variance: {aleatoric_var_heteroscedastic.mean():.6f}\")\n",
    "\n",
    "# # Calculate total standard deviation\n",
    "# total_std_heteroscedastic = np.sqrt(epistemic_var_heteroscedastic + aleatoric_var_heteroscedastic)\n",
    "# print(f\"Total Standard Deviation: {total_std_heteroscedastic.mean():.6f}\")\n",
    "\n",
    "#print(f' y_test std: {y_test_tensor.std().item()}')\n",
    "\n",
    "# Calculate R² score for the test data\n",
    "r2_test = r2_score(y_test_tensor.cpu(), mean_test_pred_heteroscedastic)\n",
    "print(f\"R² on Test Data: {r2_test:.3f}\")\n",
    "\n",
    "# Assign descriptive variable names for MC Dropout mean and standard deviation\n",
    "mc_mean = mean_test_pred_heteroscedastic  # Predicted mean for each test point\n",
    "mc_std = np.std(outputs, axis = 0)   # Predicted standard deviation for uncertainty\n",
    "\n",
    "# Define the confidence interval bounds (95% CI ≈ mean ± 2*std)\n",
    "mc_lower_bound = (mc_mean - 2 * mc_std).reshape(-1)  # Ensure it is a 1D array\n",
    "mc_upper_bound = (mc_mean+ 2 * mc_std).reshape(-1)  # Ensure it is a 1D array\n",
    "\n",
    "print(f\"Shape of mc_lower_bound: {mc_lower_bound.shape}, mc_upper_bound: {mc_upper_bound.shape}\")\n",
    "\n",
    "# calculate the coverage of the confidence interval\n",
    "in_interval = (y_test >= mc_lower_bound) & (y_test <= mc_upper_bound)\n",
    "# number of true\n",
    "counter = 0\n",
    "# Print whether each true value is within the confidence interval\n",
    "for i in in_interval:\n",
    "    if i:\n",
    "        counter += 1\n",
    "\n",
    "print(f\"Number of true values within the confidence interval: {counter} out of {len(in_interval)}\")\n",
    "\n",
    "# Calculate coverage (percentage of true values within the CI)\n",
    "coverage = np.mean(in_interval) * 100  # in percentage\n",
    "\n",
    "# Print the coverage value\n",
    "print(f\"Coverage: {coverage:.2f}%\")\n",
    "\n",
    "# Check whether each true value from random indices lies within the 95% confidence interval\n",
    "# If yes, the point will be green; if not, red\n",
    "in_interval_rand_indi = (y_test[random_indices] >= mc_lower_bound[random_indices]) & (y_test[random_indices] <= mc_upper_bound[random_indices])\n",
    "colors = ['tab:green' if inside else 'tab:red' for inside in in_interval_rand_indi]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "x_axis = np.arange(num_points)  # Create an index axis for plotting\n",
    "\n",
    "# Plot the predicted mean\n",
    "plt.plot(x_axis, mc_mean[random_indices], label=\"Prediction (mean)\", color='tab:blue')\n",
    "\n",
    "# Plot the confidence interval as a shaded region\n",
    "plt.fill_between(x_axis, mc_lower_bound[random_indices], mc_upper_bound[random_indices], alpha=0.4,\n",
    "                 color='tab:blue', label='95% Confidence Interval')\n",
    "\n",
    "# Scatter plot of true values with color-coded points based on interval inclusion\n",
    "plt.scatter(x_axis, y_test_tensor[random_indices].cpu().flatten(), label=\"True Values\", c=colors, s=25, zorder=3)\n",
    "\n",
    "# Final plot settings\n",
    "plt.title(\"MC Dropout Prediction with Uncertainty\")\n",
    "plt.xlabel(\"Test Point Index\")\n",
    "plt.ylabel(\"x_Einzug [mm]\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
