{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f5830b",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c21d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# path for desktop PC\n",
    "# sys.path.append(r'C:\\Users\\test\\Masterarbeit')\n",
    "# path for surface PC\n",
    "sys.path.append(r'C:\\Users\\Surface\\Masterarbeit')\n",
    "\n",
    "%matplotlib inline\n",
    "# path for desktop PC\n",
    "#path = r\"C:\\Users\\test\\Masterarbeit\\data\\WZ_2_Feature_Engineered_Fynn6.xlsx\"\n",
    "# path for surface PC\n",
    "path = r\"C:\\Users\\Surface\\Masterarbeit\\data\\Produktionsdaten\\WZ_2_Feature_Engineered_Fynn6.xlsx\"\n",
    "\n",
    "df = pd.read_excel(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c96b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8038b6ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import prep\n",
    "import importlib\n",
    "importlib.reload(prep)\n",
    "\n",
    "# set the target variable\n",
    "target = 'C1_V01_delta_kan'\n",
    "#print(df.columns)\n",
    "# get the numerical features\n",
    "data_num = df.drop(['C1_V01_delta_kan'], axis = 1, inplace=False)\n",
    "#print(data_num.columns)\n",
    "# get the target values\n",
    "data_labels = df[target].to_numpy()\n",
    "\n",
    "# split the data into training, validation and test sets\n",
    "# 60% training, 20%, validation, 20% test\n",
    "X_temp, X_test_prep, y_temp, y_test = train_test_split(data_num, data_labels, test_size= 0.2, random_state=42)\n",
    "X_train_prep, X_val_prep, y_train, y_val = train_test_split(X_temp, y_temp, test_size= 0.25, random_state=42)\n",
    "\n",
    "# use coustom function \"cat_transform\" from prep.py to map the categorical features with their frequencies\n",
    "X_train_prep, X_val_prep, X_test_prep = prep.cat_transform(X_train_prep, X_val_prep, X_test_prep, ['BT_NR', 'STP_NR'])\n",
    "print(X_train_prep.columns)\n",
    "\n",
    "# pipeline for preprocessing the data\n",
    "# Standard Scaler for distribution with 0 mean and 1 std., normal distributed data\n",
    "data_pipeline = Pipeline([\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# get the feature names after preprocessing for the feature importance\n",
    "feature_names = X_train_prep.columns\n",
    "\n",
    "# fit the pipeline to the data and transform it\n",
    "X_train = data_pipeline.fit_transform(X_train_prep)\n",
    "X_val = data_pipeline.transform(X_val_prep)\n",
    "X_test = data_pipeline.transform(X_test_prep)\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float() \n",
    "y_train_tensor = torch.from_numpy(y_train).float().unsqueeze(1) # Add extra dimension for compatibility\n",
    "y_val_tensor = torch.from_numpy(y_val).float().unsqueeze(1)\n",
    "y_test_tensor = torch.from_numpy(y_test).float().unsqueeze(1)\n",
    "\n",
    "# print the shapes of the data\n",
    "print(data_num.shape, X_train_tensor.shape, X_val_tensor.shape, X_test_tensor.shape)\n",
    "# print(pd.DataFrame(X_train, columns=feature_names).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d973b47",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# create a class for Neural Network with a custom architecture\n",
    "class Cusom_NN_Model(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, do_rate):\n",
    "        \"\"\"\n",
    "        Neural Network model with a custom architecture.\n",
    "        @param input_dim:   number of input features\n",
    "        @param hidden_dims: list of integers representing the number of neurons in each hidden layer e.g. [64, 128, 64, 32]\n",
    "        @param output_dim:  number of output features (usually 1 for regression tasks)\n",
    "        @param do_rate:     dropout rate for regularization\n",
    "               \n",
    "        \"\"\"\n",
    "        super(Cusom_NN_Model, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.do_rate = do_rate\n",
    "        \n",
    "        # create the layers of the model\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        last_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            self.layers.append(torch.nn.Linear(last_dim, dim))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Dropout(do_rate))\n",
    "            last_dim = dim\n",
    "        \n",
    "        # output layer\n",
    "        self.layers.append(torch.nn.Linear(last_dim, output_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# training functions for the model, optimizer Adam, loss function MSELoss, data loader for batching the data, early stopping\n",
    "def train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, batch_size=128, n_epochs=1000, lr=0.01, weight_decay=0.0001, patience=20):\n",
    "        \n",
    "    \"\"\"\n",
    "        Function for training neural Network.\n",
    "        @param model            The neural network model to be trained.\n",
    "        @param X_train_tensor   The matrix of features for the training data.\n",
    "        @param y_train_tensor   The vector of target values for the training data.\n",
    "        @param X_val_tensor     The matrix of features for the validation data.\n",
    "        @param y_val_tensor     The vector of target values for the validation data.\n",
    "        @param batch_size       The size of the batches for training.\n",
    "        @param n_epochs         The number of epochs for training.\n",
    "        @param lr               The learning rate for the optimizer.\n",
    "        @param weight_decay     The weight decay for the optimizer.\n",
    "        @param patience         The number of epochs with no improvement after which training will be stopped.\n",
    "                \n",
    "        @return model          The trained neural network model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # DataLoader for batching the data\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Define Mean Squared Error loss function\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    # Adam optimizer with weight decay for regularization\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay=weight_decay)  \n",
    "\n",
    "    # Early Stopping values\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()                           # Set model to training mode\n",
    "        batch_losses = []\n",
    "        for X_batch, y_batch in train_loader:   # loop over all batches in the DataLoader\n",
    "            optimizer.zero_grad()               # Reset gradients\n",
    "            y_pred = model(X_batch)             # Forward pass\n",
    "            loss = loss_fn(y_pred, y_batch)     # Compute MSE loss\n",
    "            loss.backward()                     # Backpropagation\n",
    "            optimizer.step()                    # Update weights\n",
    "            batch_losses.append(loss.item())   \n",
    "        loss_history.append(loss.item())    # Save loss value\n",
    "\n",
    "        # calculate validation loss\n",
    "        model.eval()                            # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val_tensor)           # Forward pass on validation set\n",
    "            val_loss = loss_fn(y_val_pred, y_val_tensor)  # Compute MSE loss on validation set\n",
    "            val_loss_history.append(val_loss.item())\n",
    "            \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {np.mean(batch_losses):.4f}, Val Loss: {val_loss.item():.4f}, Best Val Loss: {best_val_loss:.4f}\")\n",
    "            \n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}, Best Val Loss: {best_val_loss:.4f}\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break     \n",
    "           \n",
    "    # plt.figure(figsize=(8, 4))\n",
    "    # plt.plot(loss_history, label='Train Loss', color='tab:blue')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('MSE Loss')\n",
    "    # plt.title('Training Loss over Epochs')\n",
    "    # plt.grid(True)\n",
    "    # plt.legend()\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca4f8e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    # Hyperparameter-Sampling durch Optuna\n",
    "    hidden_dims = trial.suggest_categorical(\"hidden_dims\", [[128, 64], [256, 128, 64], [512, 256, 128, 64]])\n",
    "    do_rate = trial.suggest_float(\"do_rate\", 0.05, 0.4)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    # Modell initialisieren\n",
    "    model = Cusom_NN_Model(\n",
    "        input_dim=X_train_tensor.shape[1],\n",
    "        hidden_dims=hidden_dims,\n",
    "        output_dim=1,\n",
    "        do_rate=do_rate\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    # Nutze deine bestehende train_model Funktion\n",
    "    trained_model = train_model(\n",
    "        model,\n",
    "        X_train_tensor, y_train_tensor,\n",
    "        X_val_tensor, y_val_tensor,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=500,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        patience=15\n",
    "    )\n",
    "\n",
    "    # Validation Loss berechnen\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    trained_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = trained_model(X_val_tensor)\n",
    "        val_loss = loss_fn(y_val_pred, y_val_tensor).item()\n",
    "\n",
    "    # Logging\n",
    "    trial.set_user_attr(\"val_loss\", val_loss)\n",
    "\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e607f2c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "X_val_tensor = X_val_tensor.to(device)\n",
    "y_val_tensor = y_val_tensor.to(device)\n",
    "\n",
    "\n",
    "study.optimize(objective, n_trials=50, timeout=None)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Validation Loss: {trial.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd44f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "best_params = trial.params\n",
    "\n",
    "best_model = Cusom_NN_Model(\n",
    "    input_dim=X_train_tensor.shape[1],\n",
    "    hidden_dims=best_params['hidden_dims'],\n",
    "    output_dim=1,\n",
    "    do_rate=best_params['do_rate']\n",
    ").to(device)\n",
    "\n",
    "trained_model = train_model(\n",
    "    best_model,\n",
    "    X_train_tensor, y_train_tensor,\n",
    "    X_val_tensor, y_val_tensor,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    n_epochs=500,        # ggf. höher setzen\n",
    "    lr=best_params['lr'],\n",
    "    weight_decay=best_params['weight_decay'],\n",
    "    patience=20          # etwas höher für finale Trainingsläufe\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01674e82",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# selcet Device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move training and val data  to device\n",
    "X_train_tensor = X_train_tensor.to(device)  \n",
    "X_val_tensor = X_val_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "y_val_tensor = y_val_tensor.to(device)\n",
    "    \n",
    "\n",
    "# DataLoader for batching the data\n",
    "batch_size = 64  # Define batch size\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Define a fully connected neural network with ReLU activations and Dropout\n",
    "dp = 0.27  # Dropout probability set to 5%\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(37, 64),     # Input layer -> 64 neurons\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Dropout(p = dp),   # Dropout Layer with 5% Neurons set to 0\n",
    "#     torch.nn.Linear(64, 128),   # Hidden layer -> 128 neurons\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Dropout(p = dp),   # Dropout Layer with 5% Neurons set to 0\n",
    "#     torch.nn.Linear(128, 64),   # Hidden layer -> 64 neurons\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Dropout(p = dp),   # Dropout Layer with 5% Neurons set to 0\n",
    "#     torch.nn.Linear(64, 1)      # Output layer -> 1 value (regression)\n",
    "# ).to(device)  # Move model to device (GPU or CPU)\n",
    "\n",
    "model = Cusom_NN_Model(input_dim=X_train.shape[1], hidden_dims=[128, 64], output_dim=1, do_rate=dp).to(device)  # Create model instance and move to device\n",
    "\n",
    "# Define Mean Squared Error loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "# Set learning rate and optimizer\n",
    "lr = 0.002\n",
    "weight_decay = 0.0001  # Weight decay for regularization\n",
    "\n",
    "# Adam optimizer with weight decay for regularization\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay=weight_decay)  \n",
    "\n",
    "# Early Stopping Parameter\n",
    "patience = 20\n",
    "best_val_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "n_epochs = 1000\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()                           # Set model to training mode\n",
    "    batch_losses = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()               # Reset gradients\n",
    "        y_pred = model(X_batch)             # Forward pass\n",
    "        loss = loss_fn(y_pred, y_batch)     # Compute MSE loss\n",
    "        loss.backward()                     # Backpropagation\n",
    "        optimizer.step()                    # Update weights\n",
    "        batch_losses.append(loss.item())   \n",
    "    loss_history.append(loss.item())    # Save loss value\n",
    "\n",
    "    # calculate validation loss\n",
    "    model.eval()                            # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model(X_val_tensor)           # Forward pass on validation set\n",
    "        val_loss = loss_fn(y_val_pred, y_val_tensor)  # Compute MSE loss on validation set\n",
    "        val_loss_history.append(val_loss.item())\n",
    "        \n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {np.mean(batch_losses):.4f}, Val Loss: {val_loss.item():.4f}, Best Val Loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "        \n",
    "    # Update plot every 100 epochs\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(loss_history, label='Train Loss', color='tab:blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#print(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee00db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model1 = Cusom_NN_Model(input_dim=X_train.shape[1], hidden_dims=[128, 64], output_dim=1, do_rate=0.27).to(device)  # Create model instance and move to device\n",
    "\n",
    "tr_model = train_model(model1, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, batch_size=64, n_epochs=1000, lr=0.002, weight_decay=0.0001, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b38440",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# inference with multiple forward passes\n",
    "# keep the model in training mode to keep dropout active\n",
    "model.train()\n",
    "model.to('cpu')  # Ensure the model is on the correct device\n",
    "X_tr = X_train_tensor.to('cpu')  # Ensure the input data is on the correct device\n",
    "X_te = X_test_tensor.to('cpu')  # Ensure the test data is on the correct device\n",
    "# Number of stochastic forward passes for MC Dropout\n",
    "n_samples = 250\n",
    "\n",
    "# Make multiple stochastic predictions (MC Dropout) on the train data\n",
    "y_train_pred = torch.stack([model(X_tr) for i in range(n_samples)]).detach().cpu().numpy() #list comprehension for the number of stochastic forward passes for MC Dropout\n",
    "\n",
    "# Make multiple stochastic predictions (MC Dropout) on the test data\n",
    "y_test_pred = torch.stack([model(X_te) for i in range(n_samples)]).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f166ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "l= 1  # length scale for uncertainty estimation\n",
    "N= len(X_train_tensor)  # Number of training samples\n",
    "\n",
    "tau = (1- dp)*l**2 / (2*weight_decay*N)  # Calculate tau for uncertainty estimation\n",
    "print(f\"tau: {tau:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff23c66",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#! handling the train data\n",
    "# Calculate the mean and standard deviation of the predictions on the train data\n",
    "y_train_pred_mean = y_train_pred.mean(axis = 0)\n",
    "y_train_Pred_std = y_train_pred.std(axis = 0)\n",
    "\n",
    "# Calculate R² score on the train data\n",
    "r2_train = r2_score(y_train, y_train_pred_mean)\n",
    "print(f\"R² on Train Data: {r2_train:.3f}\")\n",
    "\n",
    "#! handling the test data\n",
    "# Select a random subset of test data for visualization\n",
    "num_points = 100\n",
    "np.random.seed(42)\n",
    "\n",
    "random_indices = np.random.choice(len(X_te), num_points, replace=False)\n",
    "random_indices.sort()\n",
    "\n",
    "# Calculate the mean and standard deviation of the predictions on the test data\n",
    "y_test_pred_mean = y_test_pred.mean(axis = 0)\n",
    "y_test_pred_std = y_test_pred.std(axis = 0)\n",
    "# Adjust standard deviation for uncertainty estimation\n",
    "#y_test_pred_std = y_test_pred_std + (1/tau)\n",
    "\n",
    "# Calculate R² score for the test data\n",
    "r2_test = r2_score(y_te.cpu(), y_test_pred_mean)\n",
    "print(f\"R² on Test Data: {r2_test:.3f}\")\n",
    "\n",
    "# calculate NLL for the test data\n",
    "ll = (torch.logsumexp(-0.5 * tau * (y_te.cpu() - y_test_pred_mean)**2,0) -np.log(n_samples) - 0.5 * np.log(2*np.pi) + 0.5 * np.log(tau))\n",
    "test_ll = ll.mean()  # Mean Negative Log Likelihood\n",
    "test_ll = test_ll.to('cpu')  # Move to CPU for printing\n",
    "nll = -test_ll\n",
    "print(f\"Negative Log Likelihood (NLL) on Test Data: {test_ll:.3f}\")\n",
    "\n",
    "\n",
    "# Assign descriptive variable names for MC Dropout mean and standard deviation\n",
    "mc_mean = y_test_pred_mean.flatten()  # Predicted mean for each test point\n",
    "mc_std = y_test_pred_std.flatten()    # Predicted standard deviation for uncertainty\n",
    "\n",
    "# Define the confidence interval bounds (95% CI ≈ mean ± 2*std)\n",
    "mc_lower_bound = mc_mean[random_indices] - 2 * mc_std[random_indices]\n",
    "mc_upper_bound = mc_mean[random_indices] + 2 * mc_std[random_indices]\n",
    "\n",
    "# Check whether each true value lies within the 95% confidence interval\n",
    "# If yes, the point will be green; if not, red\n",
    "in_interval = (y_test[random_indices] >= mc_lower_bound) & (y_test[random_indices] <= mc_upper_bound)\n",
    "colors = ['tab:green' if inside else 'tab:red' for inside in in_interval]\n",
    "\n",
    "# Calculate coverage (percentage of true values within the CI)\n",
    "coverage = np.mean(in_interval) * 100  # in percentage\n",
    "\n",
    "# Print the coverage value\n",
    "print(f\"Coverage: {coverage:.2f}%\")\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "x_axis = np.arange(num_points)  # Create an index axis for plotting\n",
    "\n",
    "# Plot the predicted mean\n",
    "plt.plot(x_axis, mc_mean[random_indices], label=\"Prediction (mean)\", color='tab:blue')\n",
    "\n",
    "# Plot the confidence interval as a shaded region\n",
    "plt.fill_between(x_axis, mc_lower_bound, mc_upper_bound, alpha=0.4,\n",
    "                 color='tab:blue', label='95% Confidence Interval')\n",
    "\n",
    "# Scatter plot of true values with color-coded points based on interval inclusion\n",
    "plt.scatter(x_axis, y_te[random_indices].cpu().flatten(), label=\"True Values\", c=colors, s=25, zorder=3)\n",
    "\n",
    "# Final plot settings\n",
    "plt.title(\"MC Dropout Prediction with Uncertainty\")\n",
    "plt.xlabel(\"Test Point Index\")\n",
    "plt.ylabel(\"x_Einzug [mm]\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab343ed6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08210e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
