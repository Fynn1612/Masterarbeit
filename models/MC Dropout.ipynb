{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f5830b",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4c21d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# path for desktop PC\n",
    "# sys.path.append(r'C:\\Users\\test\\Masterarbeit')\n",
    "# path for surface PC\n",
    "sys.path.append(r'C:\\Users\\Surface\\Masterarbeit')\n",
    "\n",
    "%matplotlib inline\n",
    "# path for desktop PC\n",
    "#path = r\"C:\\Users\\test\\Masterarbeit\\data\\WZ_2_Feature_Engineered_Fynn6.xlsx\"\n",
    "# path for surface PC\n",
    "path = r\"C:\\Users\\Surface\\Masterarbeit\\data\\Produktionsdaten\\WZ_2_Feature_Engineered_Fynn6.xlsx\"\n",
    "\n",
    "df = pd.read_excel(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5604983d",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110c96b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde03c4",
   "metadata": {},
   "source": [
    "Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8038b6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Ist_Blechhaltergewicht', 'Ist_Gegenhaltekraft_HL_1', 'T2', 'K1', 'T20',\n",
      "       'K7', 'T23', 'K8', 'DS_10', 'GS_10', 'SD_10', 'LS_10', 'PP_10', 'TT_10',\n",
      "       'TM5_10', 'RF_10', 'TD_10', 'is_weekend', 'dayofweek_sin',\n",
      "       'dayofweek_cos', 'month_sin', 'month_cos', 'hour_sin', 'hour_cos',\n",
      "       'day_sin', 'day_cos', 'quarter_sin', 'quarter_cos', 'week_sin',\n",
      "       'week_cos', 'Diff_Hubzahl', 'Diff_Ziehtiefe',\n",
      "       'Diff_Ziehkissenverstellu', 'Diff_Stoesselverstellung-mm',\n",
      "       'Diff_Gewichtsausgleich', 'BT_NR_freq', 'STP_NR_freq'],\n",
      "      dtype='object')\n",
      "(52011, 37) torch.Size([31206, 37]) torch.Size([10402, 37]) torch.Size([10403, 37])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import prep\n",
    "import importlib\n",
    "importlib.reload(prep)\n",
    "\n",
    "# set the target variable\n",
    "target = 'C1_V01_delta_kan'\n",
    "#print(df.columns)\n",
    "# get the numerical features\n",
    "data_num = df.drop(['C1_V01_delta_kan'], axis = 1, inplace=False)\n",
    "#print(data_num.columns)\n",
    "# get the target values\n",
    "data_labels = df[target].to_numpy()\n",
    "\n",
    "# split the data into training, validation and test sets\n",
    "# 60% training, 20%, validation, 20% test\n",
    "X_temp, X_test_prep, y_temp, y_test = train_test_split(data_num, data_labels, test_size= 0.2, random_state=42)\n",
    "X_train_prep, X_val_prep, y_train, y_val = train_test_split(X_temp, y_temp, test_size= 0.25, random_state=42)\n",
    "\n",
    "# use coustom function \"cat_transform\" from prep.py to map the categorical features with their frequencies\n",
    "X_train_prep, X_val_prep, X_test_prep = prep.cat_transform(X_train_prep, X_val_prep, X_test_prep, ['BT_NR', 'STP_NR'])\n",
    "print(X_train_prep.columns)\n",
    "\n",
    "# pipeline for preprocessing the data\n",
    "# Standard Scaler for distribution with 0 mean and 1 std., normal distributed data\n",
    "data_pipeline = Pipeline([\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# get the feature names after preprocessing for the feature importance\n",
    "feature_names = X_train_prep.columns\n",
    "\n",
    "# fit the pipeline to the data and transform it\n",
    "X_train = data_pipeline.fit_transform(X_train_prep)\n",
    "X_val = data_pipeline.transform(X_val_prep)\n",
    "X_test = data_pipeline.transform(X_test_prep)\n",
    "\n",
    "# convert the data to PyTorch tensors\n",
    "# and add an extra dimension for the target variable\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float() \n",
    "y_train_tensor = torch.from_numpy(y_train).float().unsqueeze(1) # Add extra dimension for compatibility\n",
    "y_val_tensor = torch.from_numpy(y_val).float().unsqueeze(1)\n",
    "y_test_tensor = torch.from_numpy(y_test).float().unsqueeze(1)\n",
    "\n",
    "# print the shapes of the data\n",
    "print(data_num.shape, X_train_tensor.shape, X_val_tensor.shape, X_test_tensor.shape)\n",
    "# print(pd.DataFrame(X_train, columns=feature_names).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc830b8",
   "metadata": {},
   "source": [
    "Class for a customizable NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d973b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for Neural Network with a custom architecture\n",
    "class Cusom_NN_Model(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, do_rate):\n",
    "        \"\"\"\n",
    "        Neural Network model with a custom architecture.\n",
    "        @param input_dim:   number of input features\n",
    "        @param hidden_dims: list of integers representing the number of neurons in each hidden layer e.g. [64, 128, 64, 32]\n",
    "        @param output_dim:  number of output features (usually 1 for regression tasks)\n",
    "        @param do_rate:     dropout rate for regularization\n",
    "               \n",
    "        \"\"\"\n",
    "        super(Cusom_NN_Model, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.do_rate = do_rate\n",
    "        \n",
    "        # create the layers of the model\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        last_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            self.layers.append(torch.nn.Linear(last_dim, dim))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Dropout(do_rate))\n",
    "            last_dim = dim\n",
    "        \n",
    "        # output layer\n",
    "        self.layers.append(torch.nn.Linear(last_dim, output_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d63df0",
   "metadata": {},
   "source": [
    "Training function for NN including data loader with batch sampling, early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbf4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training functions for the model, optimizer Adam, loss function MSELoss, data loader for batching the data, early stopping\n",
    "def train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, batch_size=128, n_epochs=1000, lr=0.01, weight_decay=0.0001, patience=20):\n",
    "        \n",
    "    \"\"\"\n",
    "        Function for training neural Network.\n",
    "        @param model            The neural network model to be trained.\n",
    "        @param X_train_tensor   The matrix of features for the training data.\n",
    "        @param y_train_tensor   The vector of target values for the training data.\n",
    "        @param X_val_tensor     The matrix of features for the validation data.\n",
    "        @param y_val_tensor     The vector of target values for the validation data.\n",
    "        @param batch_size       The size of the batches for training.\n",
    "        @param n_epochs         The number of epochs for training.\n",
    "        @param lr               The learning rate for the optimizer.\n",
    "        @param weight_decay     The weight decay for the optimizer.\n",
    "        @param patience         The number of epochs with no improvement after which training will be stopped.\n",
    "                \n",
    "        @return model          The trained neural network model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # DataLoader for batching the data\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Define Mean Squared Error loss function\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    # Adam optimizer with weight decay for regularization\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay=weight_decay)  \n",
    "\n",
    "    # Early Stopping values\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()                           # Set model to training mode\n",
    "        batch_losses = []\n",
    "        for X_batch, y_batch in train_loader:   # loop over all batches in the DataLoader\n",
    "            optimizer.zero_grad()               # Reset gradients\n",
    "            y_pred = model(X_batch)             # Forward pass\n",
    "            loss = loss_fn(y_pred, y_batch)     # Compute MSE loss\n",
    "            loss.backward()                     # Backpropagation\n",
    "            optimizer.step()                    # Update weights\n",
    "            batch_losses.append(loss.item())   \n",
    "        loss_history.append(loss.item())    # Save loss value\n",
    "\n",
    "        # calculate validation loss\n",
    "        model.eval()                            # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val_tensor)           # Forward pass on validation set\n",
    "            val_loss = loss_fn(y_val_pred, y_val_tensor)  # Compute MSE loss on validation set\n",
    "            val_loss_history.append(val_loss.item())\n",
    "            \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {np.mean(batch_losses):.4f}, Val Loss: {val_loss.item():.4f}, Best Val Loss: {best_val_loss:.4f}\")\n",
    "            \n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}, Best Val Loss: {best_val_loss:.4f}\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break     \n",
    "           \n",
    "    # plt.figure(figsize=(8, 4))\n",
    "    # plt.plot(loss_history, label='Train Loss', color='tab:blue')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('MSE Loss')\n",
    "    # plt.title('Training Loss over Epochs')\n",
    "    # plt.grid(True)\n",
    "    # plt.legend()\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1446b1f7",
   "metadata": {},
   "source": [
    "Hyperparameter Search with Optuna from Chat GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca4f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    # Hyperparameter-Sampling durch Optuna\n",
    "    hidden_dims = trial.suggest_categorical(\"hidden_dims\", [[128, 64], [256, 128, 64], [512, 256, 128, 64]])\n",
    "    do_rate = trial.suggest_float(\"do_rate\", 0.05, 0.4)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    # Modell initialisieren\n",
    "    model = Cusom_NN_Model(\n",
    "        input_dim=X_train_tensor.shape[1],\n",
    "        hidden_dims=hidden_dims,\n",
    "        output_dim=1,\n",
    "        do_rate=do_rate\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    # Nutze deine bestehende train_model Funktion\n",
    "    trained_model = train_model(\n",
    "        model,\n",
    "        X_train_tensor, y_train_tensor,\n",
    "        X_val_tensor, y_val_tensor,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=500,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        patience=15\n",
    "    )\n",
    "\n",
    "    # Validation Loss berechnen\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    trained_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = trained_model(X_val_tensor)\n",
    "        val_loss = loss_fn(y_val_pred, y_val_tensor).item()\n",
    "\n",
    "    # Logging\n",
    "    trial.set_user_attr(\"val_loss\", val_loss)\n",
    "\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e607f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "X_val_tensor = X_val_tensor.to(device)\n",
    "y_val_tensor = y_val_tensor.to(device)\n",
    "\n",
    "\n",
    "study.optimize(objective, n_trials=50, timeout=None)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Validation Loss: {trial.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = trial.params\n",
    "\n",
    "best_model = Cusom_NN_Model(\n",
    "    input_dim=X_train_tensor.shape[1],\n",
    "    hidden_dims=best_params['hidden_dims'],\n",
    "    output_dim=1,\n",
    "    do_rate=best_params['do_rate']\n",
    ").to(device)\n",
    "\n",
    "trained_model = train_model(\n",
    "    best_model,\n",
    "    X_train_tensor, y_train_tensor,\n",
    "    X_val_tensor, y_val_tensor,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    n_epochs=500,        # ggf. höher setzen\n",
    "    lr=best_params['lr'],\n",
    "    weight_decay=best_params['weight_decay'],\n",
    "    patience=20          # etwas höher für finale Trainingsläufe\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01674e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selcet Device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move training and val data  to device\n",
    "X_train_tensor = X_train_tensor.to(device)  \n",
    "X_val_tensor = X_val_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "y_val_tensor = y_val_tensor.to(device)\n",
    "    \n",
    "\n",
    "# DataLoader for batching the data\n",
    "batch_size = 64  # Define batch size\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Define a fully connected neural network with ReLU activations and Dropout\n",
    "dp = 0.27  # Dropout probability set to 5%\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(37, 64),     # Input layer -> 64 neurons\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Dropout(p = dp),   # Dropout Layer with 5% Neurons set to 0\n",
    "#     torch.nn.Linear(64, 128),   # Hidden layer -> 128 neurons\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Dropout(p = dp),   # Dropout Layer with 5% Neurons set to 0\n",
    "#     torch.nn.Linear(128, 64),   # Hidden layer -> 64 neurons\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Dropout(p = dp),   # Dropout Layer with 5% Neurons set to 0\n",
    "#     torch.nn.Linear(64, 1)      # Output layer -> 1 value (regression)\n",
    "# ).to(device)  # Move model to device (GPU or CPU)\n",
    "\n",
    "model = Cusom_NN_Model(input_dim=X_train.shape[1], hidden_dims=[128, 64], output_dim=1, do_rate=dp).to(device)  # Create model instance and move to device\n",
    "\n",
    "# Define Mean Squared Error loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "# Set learning rate and optimizer\n",
    "lr = 0.002\n",
    "weight_decay = 0.0001  # Weight decay for regularization\n",
    "\n",
    "# Adam optimizer with weight decay for regularization\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay=weight_decay)  \n",
    "\n",
    "# Early Stopping Parameter\n",
    "patience = 20\n",
    "best_val_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "n_epochs = 1000\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()                           # Set model to training mode\n",
    "    batch_losses = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()               # Reset gradients\n",
    "        y_pred = model(X_batch)             # Forward pass\n",
    "        loss = loss_fn(y_pred, y_batch)     # Compute MSE loss\n",
    "        loss.backward()                     # Backpropagation\n",
    "        optimizer.step()                    # Update weights\n",
    "        batch_losses.append(loss.item())   \n",
    "    loss_history.append(loss.item())    # Save loss value\n",
    "\n",
    "    # calculate validation loss\n",
    "    model.eval()                            # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model(X_val_tensor)           # Forward pass on validation set\n",
    "        val_loss = loss_fn(y_val_pred, y_val_tensor)  # Compute MSE loss on validation set\n",
    "        val_loss_history.append(val_loss.item())\n",
    "        \n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {np.mean(batch_losses):.4f}, Val Loss: {val_loss.item():.4f}, Best Val Loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "        \n",
    "    # Update plot every 100 epochs\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(loss_history, label='Train Loss', color='tab:blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#print(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beee00db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 1346.3194, Val Loss: 48.0672, Best Val Loss: 48.0672\n",
      "Epoch 2/1000, Train Loss: 246.5900, Val Loss: 38.2359, Best Val Loss: 38.2359\n",
      "Epoch 3/1000, Train Loss: 231.7932, Val Loss: 33.2871, Best Val Loss: 33.2871\n",
      "Epoch 4/1000, Train Loss: 226.1216, Val Loss: 25.3333, Best Val Loss: 25.3333\n",
      "Epoch 6/1000, Train Loss: 216.4175, Val Loss: 24.4475, Best Val Loss: 24.4475\n",
      "Epoch 9/1000, Train Loss: 201.3809, Val Loss: 24.1394, Best Val Loss: 24.1394\n",
      "Epoch 10/1000, Train Loss: 198.4407, Val Loss: 22.9779, Best Val Loss: 22.9779\n",
      "Epoch 11/1000, Train Loss: 191.9616, Val Loss: 20.6268, Best Val Loss: 20.6268\n",
      "Epoch 15/1000, Train Loss: 175.3066, Val Loss: 19.1763, Best Val Loss: 19.1763\n",
      "Epoch 31/1000, Train Loss: 120.0331, Val Loss: 18.9210, Best Val Loss: 18.9210\n",
      "Epoch 33/1000, Train Loss: 116.9069, Val Loss: 18.3543, Best Val Loss: 18.3543\n",
      "Epoch 41/1000, Train Loss: 104.8898, Val Loss: 18.0899, Best Val Loss: 18.0899\n",
      "Epoch 43/1000, Train Loss: 99.8069, Val Loss: 18.0878, Best Val Loss: 18.0878\n",
      "Epoch 45/1000, Train Loss: 96.7447, Val Loss: 17.4111, Best Val Loss: 17.4111\n",
      "Epoch 47/1000, Train Loss: 93.4937, Val Loss: 17.2364, Best Val Loss: 17.2364\n",
      "Epoch 50/1000, Train Loss: 82.2012, Val Loss: 15.4488, Best Val Loss: 15.4488\n",
      "Epoch 56/1000, Train Loss: 75.4685, Val Loss: 15.3438, Best Val Loss: 15.3438\n",
      "Epoch 62/1000, Train Loss: 68.9409, Val Loss: 14.3425, Best Val Loss: 14.3425\n",
      "Epoch 63/1000, Train Loss: 68.5563, Val Loss: 13.9299, Best Val Loss: 13.9299\n",
      "Epoch 74/1000, Train Loss: 59.2834, Val Loss: 13.4236, Best Val Loss: 13.4236\n",
      "Epoch 77/1000, Train Loss: 57.0968, Val Loss: 13.1118, Best Val Loss: 13.1118\n",
      "Epoch 84/1000, Train Loss: 53.7083, Val Loss: 12.8333, Best Val Loss: 12.8333\n",
      "Epoch 86/1000, Train Loss: 52.1064, Val Loss: 12.7564, Best Val Loss: 12.7564\n",
      "Epoch 90/1000, Train Loss: 50.2250, Val Loss: 12.5708, Best Val Loss: 12.5708\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m model1 = Cusom_NN_Model(input_dim=X_train.shape[\u001b[32m1\u001b[39m], hidden_dims=[\u001b[32m128\u001b[39m, \u001b[32m64\u001b[39m], output_dim=\u001b[32m1\u001b[39m, do_rate=\u001b[32m0.27\u001b[39m).to(device)  \u001b[38;5;66;03m# Create model instance and move to device\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tr_model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.002\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, batch_size, n_epochs, lr, weight_decay, patience)\u001b[39m\n\u001b[32m     42\u001b[39m     loss = loss_fn(y_pred, y_batch)     \u001b[38;5;66;03m# Compute MSE loss\u001b[39;00m\n\u001b[32m     43\u001b[39m     loss.backward()                     \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m                    \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[32m     45\u001b[39m     batch_losses.append(loss.item())   \n\u001b[32m     46\u001b[39m loss_history.append(loss.item())    \u001b[38;5;66;03m# Save loss value\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Surface\\Masterarbeit\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Surface\\Masterarbeit\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Surface\\Masterarbeit\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Surface\\Masterarbeit\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Surface\\Masterarbeit\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Surface\\Masterarbeit\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:414\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    412\u001b[39m                 grad = grad.add(param, alpha=weight_decay)\n\u001b[32m    413\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m             grad = \u001b[43mgrad\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.is_complex(param):\n\u001b[32m    417\u001b[39m     grad = torch.view_as_real(grad)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1 = Cusom_NN_Model(input_dim=X_train.shape[1], hidden_dims=[128, 64], output_dim=1, do_rate=0.27).to(device)  # Create model instance and move to device\n",
    "\n",
    "tr_model = train_model(model1, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, batch_size=64, n_epochs=1000, lr=0.002, weight_decay=0.0001, patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db25fd",
   "metadata": {},
   "source": [
    "Inference on the Train and test data without aleatoric uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b38440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with multiple forward passes\n",
    "# keep the model in training mode to keep dropout active\n",
    "model.train()\n",
    "model.to('cpu')  # Ensure the model is on the correct device\n",
    "X_tr = X_train_tensor.to('cpu')  # Ensure the input data is on the correct device\n",
    "X_te = X_test_tensor.to('cpu')  # Ensure the test data is on the correct device\n",
    "# Number of stochastic forward passes for MC Dropout\n",
    "n_samples = 250\n",
    "\n",
    "# Make multiple stochastic predictions (MC Dropout) on the train data\n",
    "y_train_pred = torch.stack([model(X_tr) for i in range(n_samples)]).detach().cpu().numpy() #list comprehension for the number of stochastic forward passes for MC Dropout\n",
    "\n",
    "# Make multiple stochastic predictions (MC Dropout) on the test data\n",
    "y_test_pred = torch.stack([model(X_te) for i in range(n_samples)]).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f166ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "l= 1  # length scale for uncertainty estimation\n",
    "N= len(X_train_tensor)  # Number of training samples\n",
    "\n",
    "tau = (1- dp)*l**2 / (2*weight_decay*N)  # Calculate tau for uncertainty estimation\n",
    "print(f\"tau: {tau:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e0bd3",
   "metadata": {},
   "source": [
    "Plot the uncertainty interval for the NN with MC Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff23c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! handling the train data\n",
    "# Calculate the mean and standard deviation of the predictions on the train data\n",
    "y_train_pred_mean = y_train_pred.mean(axis = 0)\n",
    "y_train_Pred_std = y_train_pred.std(axis = 0)\n",
    "\n",
    "# Calculate R² score on the train data\n",
    "r2_train = r2_score(y_train, y_train_pred_mean)\n",
    "print(f\"R² on Train Data: {r2_train:.3f}\")\n",
    "\n",
    "#! handling the test data\n",
    "# Select a random subset of test data for visualization\n",
    "num_points = 100\n",
    "np.random.seed(42)\n",
    "\n",
    "random_indices = np.random.choice(len(X_te), num_points, replace=False)\n",
    "random_indices.sort()\n",
    "\n",
    "# Calculate the mean and standard deviation of the predictions on the test data\n",
    "y_test_pred_mean = y_test_pred.mean(axis = 0)\n",
    "y_test_pred_std = y_test_pred.std(axis = 0)\n",
    "# Adjust standard deviation for uncertainty estimation\n",
    "#y_test_pred_std = y_test_pred_std + (1/tau)\n",
    "\n",
    "# Calculate R² score for the test data\n",
    "r2_test = r2_score(y_test_tensor.cpu(), y_test_pred_mean)\n",
    "print(f\"R² on Test Data: {r2_test:.3f}\")\n",
    "\n",
    "# calculate NLL for the test data\n",
    "ll = (torch.logsumexp(-0.5 * tau * (y_test_tensor.cpu()[None] - y_test_pred)**2,0) -np.log(n_samples) - 0.5 * np.log(2*np.pi) + 0.5 * np.log(tau))\n",
    "test_ll = ll.mean()  # Mean Negative Log Likelihood\n",
    "test_ll = test_ll.to('cpu')  # Move to CPU for printing\n",
    "nll = -test_ll\n",
    "print(f\"Negative Log Likelihood (NLL) on Test Data: {test_ll:.3f}\")\n",
    "\n",
    "\n",
    "# Assign descriptive variable names for MC Dropout mean and standard deviation\n",
    "mc_mean = y_test_pred_mean.flatten()  # Predicted mean for each test point\n",
    "mc_std = y_test_pred_std.flatten()    # Predicted standard deviation for uncertainty\n",
    "\n",
    "# Define the confidence interval bounds (95% CI ≈ mean ± 2*std)\n",
    "mc_lower_bound = mc_mean[random_indices] - 2 * mc_std[random_indices]\n",
    "mc_upper_bound = mc_mean[random_indices] + 2 * mc_std[random_indices]\n",
    "\n",
    "# Check whether each true value lies within the 95% confidence interval\n",
    "# If yes, the point will be green; if not, red\n",
    "in_interval = (y_test[random_indices] >= mc_lower_bound) & (y_test[random_indices] <= mc_upper_bound)\n",
    "colors = ['tab:green' if inside else 'tab:red' for inside in in_interval]\n",
    "\n",
    "# Calculate coverage (percentage of true values within the CI)\n",
    "coverage = np.mean(in_interval) * 100  # in percentage\n",
    "\n",
    "# Print the coverage value\n",
    "print(f\"Coverage: {coverage:.2f}%\")\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "x_axis = np.arange(num_points)  # Create an index axis for plotting\n",
    "\n",
    "# Plot the predicted mean\n",
    "plt.plot(x_axis, mc_mean[random_indices], label=\"Prediction (mean)\", color='tab:blue')\n",
    "\n",
    "# Plot the confidence interval as a shaded region\n",
    "plt.fill_between(x_axis, mc_lower_bound, mc_upper_bound, alpha=0.4,\n",
    "                 color='tab:blue', label='95% Confidence Interval')\n",
    "\n",
    "# Scatter plot of true values with color-coded points based on interval inclusion\n",
    "plt.scatter(x_axis, y_test_tensor[random_indices].cpu().flatten(), label=\"True Values\", c=colors, s=25, zorder=3)\n",
    "\n",
    "# Final plot settings\n",
    "plt.title(\"MC Dropout Prediction with Uncertainty\")\n",
    "plt.xlabel(\"Test Point Index\")\n",
    "plt.ylabel(\"x_Einzug [mm]\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
