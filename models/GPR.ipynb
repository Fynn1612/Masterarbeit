{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd9aac2",
   "metadata": {},
   "source": [
    "Implementation of Sparse GPR\n",
    "\n",
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import r2_score\n",
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP\n",
    "from torch.distributions import Normal\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import uncertainty_toolbox as uct\n",
    "\n",
    "# define the device for the setting\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# check the computer name and set the path accordingly\n",
    "if os.environ['COMPUTERNAME'] == 'FYNN':            # name of surface PC\n",
    "    sys.path.append(r'C:\\Users\\Surface\\Masterarbeit')\n",
    "elif os.environ['COMPUTERNAME'] == 'FYNNS-PC':  # desktop name\n",
    "    sys.path.append(r'C:\\Users\\test\\Masterarbeit')\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"Unbekannter Computername: \" + os.environ['COMPUTERNAME'])\n",
    "\n",
    "from utils.data_prep import load_tranform_and_split_data, set_seed\n",
    "from utils.metrics import evaluate_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c0938",
   "metadata": {},
   "source": [
    "Transformation Pipeline for Approximating GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9592945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and transform the data, split it into training, validation, and test sets\n",
    "# the split ratio is 60% training, 20% validation, and 20%\n",
    "# return the feature names for later use\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, feature_names = load_tranform_and_split_data('C1_V01_delta_kan', split_ratio=(0.6, 0.2, 0.2))\n",
    "\n",
    "# convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float() \n",
    "# Add extra dimension for compatibility to the target tensors\n",
    "y_train_tensor = torch.from_numpy(y_train).float() \n",
    "y_val_tensor = torch.from_numpy(y_val).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float()\n",
    "\n",
    "# Move tensors to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7df249",
   "metadata": {},
   "source": [
    "Stochastic Variational GP Regression Implementation\n",
    "\n",
    "Natural Gradient Descent with Variational Models for better and faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170376b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a TensorDataset and DataLoader for the training data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# define the GP model\n",
    "class GPModel(ApproximateGP):\n",
    "    \"\"\"\n",
    "    Stochastic Variational Gaussian Process model with learnable inducing points.\n",
    "    \n",
    "    Args:\n",
    "        inducing_points (torch.Tensor): Initial locations of inducing points (shape: [num_inducing, input_dim])\n",
    "        kernel (gpytorch.kernels.Kernel): Kernel function (e.g., RBF, RQ)\n",
    "    \n",
    "    Attributes:\n",
    "        mean_module: Constant mean function initialized with training data mean\n",
    "        covar_module: Kernel function for covariance\n",
    "    \"\"\"\n",
    "    def __init__(self, inducing_points, kernel):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0)) #default value\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        ) #default value\n",
    "\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean() # Constant mean function\n",
    "        self.covar_module  = kernel\n",
    "        self.mean_module.initialize(constant=y_train_tensor.mean().item())  # Initialize the mean with the mean of the target values of the training dataset\n",
    "\n",
    " #  define the forward method               \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f74832",
   "metadata": {},
   "source": [
    "Create Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter optimization of SVGP model.\n",
    "    \n",
    "    Optimizes learning rates (NGD, Adam) using validation NLL as metric.\n",
    "    Early stopping prevents overfitting during hyperparameter search.\n",
    "    \n",
    "    Args:\n",
    "        trial (optuna.Trial): Optuna trial object for suggesting hyperparameters\n",
    "        \n",
    "    Returns:\n",
    "        float: Best validation loss (negative ELBO) achieved during training\n",
    "        \n",
    "    Note:\n",
    "        Hyperparameter ranges and kernel choice are justified in Chapter X.Y\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the variance of the target values of the training dataset for initializing the likelihood noise\n",
    "    y_var = y_train_tensor.var(unbiased=False).item()\n",
    "    noise = 1e-2 * y_var # Initialize noise as 1% of target variance (prevents underestimation)\n",
    "\n",
    "    # Suggest hyperparameters for optimization\n",
    "    lr_ngd = trial.suggest_float('lr', 1e-3, 1e-1, log=True)\n",
    "    lr_adam = trial.suggest_float('lr_adam', 1e-3, 1e-1, log=True)\n",
    "\n",
    "    #Rational Quadratic Kernel\n",
    "    rational_quadratic_kernel = gpytorch.kernels.RQKernel(ard_num_dims=X_train.shape[1], \n",
    "                                                          alpha_constraint=gpytorch.constraints.Interval(0.1, 10.0)) # Set ARD for all input dimensions\n",
    "    rational_quadratic_kernel.lengthscale = torch.ones(X_train.shape[1]) # Initialize lengthscale to 1 for all dimensions\n",
    "    rational_quadratic_kernel.outputscale = y_var # Initialize outputscale to the variance of the target values\n",
    "    kernel = gpytorch.kernels.ScaleKernel(rational_quadratic_kernel) # Wrap RQ kernel in ScaleKernel to have outputscale parameter\n",
    "\n",
    "    # Define the inducing points\n",
    "    num_inducing_points = 1000\n",
    "\n",
    "    # Use KMeans to select inducing points that represent the data distribution better\n",
    "    kmeans = KMeans(n_clusters=num_inducing_points, random_state=42).fit(X_train)\n",
    "    inducing_points = torch.from_numpy(kmeans.cluster_centers_).float()\n",
    "\n",
    "    # Initialize the GP model and likelihood\n",
    "    model = GPModel(inducing_points = inducing_points, kernel=kernel)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    likelihood.noise = noise\n",
    "\n",
    "    print(f'Model: {model}')\n",
    "    print(f'Mean: {model.mean_module.constant.item()}')\n",
    "    print(f'Lengthscale: {model.covar_module.base_kernel.lengthscale}')\n",
    "    print(f'Outputscale: {model.covar_module.outputscale}')\n",
    "    print(f'Likelihood Noise: {likelihood.noise.item()}')\n",
    "    # Move model and likelihood to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "    num_epochs = 100\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    # Use Natural Gradient Descent for the variational parameters and Adam for the hyperparameters of the kernel and likelihood\n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), \n",
    "                                                   num_data= y_train_tensor.size(0), \n",
    "                                                   lr=lr_ngd)\n",
    "\n",
    "    hyperparameter_optimizer = torch.optim.Adam([{'params': model.hyperparameters()},\n",
    "                                                 {'params': likelihood.parameters()},\n",
    "                                                 ],\n",
    "                                                  lr=lr_adam)\n",
    "\n",
    "    # VariationalELBO is used for training\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=len(train_loader.dataset))\n",
    "    # Early stopping parameters\n",
    "    best_val_loss = np.inf\n",
    "    patience = 10\n",
    "    epochs_no_improve = 0\n",
    "    decimal_places = 3\n",
    "    tolerance = 10 ** (-decimal_places) # Early stopping tolerance: 0.001 (prevents premature stopping on small fluctuations)\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # Zero gradients from previous iteration        \n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "            hyperparameter_optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        # print every ten epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader):.4f}')         # Print the average loss for the epoch\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        with torch.no_grad():\n",
    "            f_val_preds = model(X_val_tensor)\n",
    "            val_loss = -mll(f_val_preds, y_val_tensor).item()  \n",
    "            \n",
    "            val_preds = likelihood(f_val_preds)     \n",
    "            # Extract mean and standard deviation, Compute Negative Log-Likelihood (NLL) on validation set \n",
    "            # NLL = -log(p(y|x)) measures calibration quality of uncertainty estimates\n",
    "            val_mean = val_preds.mean.cpu()\n",
    "            val_std = val_preds.stddev.cpu()\n",
    "            val_std = val_std.clamp_min(1e-6) # Minimum std to avoid log(0) in NLL computation\n",
    "            nll_per_point = -Normal(val_mean, val_std).log_prob(y_val_tensor.cpu()).numpy()\n",
    "            val_nll = nll_per_point.mean().item()\n",
    "            \n",
    "            r2_score_val = r2_score(y_val, val_mean)\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Validation Loss: {val_loss} Validation NLL: {val_nll} R²: {r2_score_val:.3f}')\n",
    "            # Report the intermediate value to Optuna\n",
    "            trial.report(val_loss, step=epoch)\n",
    "\n",
    "            # Handle pruning based on the intermediate value\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            if abs(val_nll - best_val_loss) < tolerance:\n",
    "\n",
    "                epochs_no_improve += 1\n",
    "            else:\n",
    "                epochs_no_improve = 0\n",
    "                best_val_loss = val_loss\n",
    "                # best_model_state = model.state_dict()\n",
    "                # best_likelihood_state = likelihood.state_dict()   \n",
    "            \n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping at iteration {epoch + 1} with best validation loss: {best_val_loss:.3f}')\n",
    "                # model.load_state_dict(best_model_state)\n",
    "                # likelihood.load_state_dict(best_likelihood_state)\n",
    "\n",
    "                break\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c3843",
   "metadata": {},
   "source": [
    "Execute Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f56cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a study object for Optuna\n",
    "study = optuna.create_study(\n",
    "\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(),                       #TPE (Tree-structured Parzen Estimator) sampler by default\n",
    "    pruner=optuna.pruners.MedianPruner(        \n",
    "        n_startup_trials=5,                                    # Number of trials to run before pruning starts\n",
    "        n_warmup_steps=5                                        # Number of warmup steps before pruning starts)\n",
    "    )\n",
    ")\n",
    "\n",
    "# optimize the objective function with Optuna\n",
    "# timeout=None means no time limit for the optimization, all trials will be run\n",
    "study.optimize(objective, n_trials=50, timeout=None, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc0425",
   "metadata": {},
   "source": [
    "Make 10 Runs with different Random Seed to evaluate GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb5d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store results and predictions for each run\n",
    "results_list = []\n",
    "predictions_list = []\n",
    "list_of_seeds = [42, 123, 777, 2024, 5250, 8888, 9876, 10001, 31415, 54321]\n",
    "DE_prediction_path = r\"C:\\Users\\test\\Masterarbeit\\models\\Modelresults\\GPR\"\n",
    "DE_result_path = r\"C:\\Users\\test\\OneDrive\\Master Management und Engineering\\Masterarbeit\\Experimente\\Evaluation\\10 Runs\\GPR\"\n",
    "\n",
    "for run, seed in enumerate(list_of_seeds):\n",
    "\n",
    "    print(f\"Run {run+1} with seed {seed}\")\n",
    "    set_seed(seed)\n",
    "    # Calculate the variance of the target values of the training dataset for initializing the likelihood noise\n",
    "    y_var = y_train_tensor.var(unbiased=False).item()\n",
    "    noise = 1e-2 * y_var # Initialize noise as 1% of target variance (prevents underestimation)\n",
    "    #Rational Quadratic Kernel and initialization of hyperparameters\n",
    "    rational_quadratic_kernel = gpytorch.kernels.RQKernel(ard_num_dims=X_train.shape[1], \n",
    "                                                        alpha_constraint=gpytorch.constraints.Interval(0.1, 10.0)\n",
    "                                                        )\n",
    "    rational_quadratic_kernel.lengthscale = torch.ones(X_train.shape[1])\n",
    "    rational_quadratic_kernel.outputscale = y_var\n",
    "    kernel = gpytorch.kernels.ScaleKernel(rational_quadratic_kernel)\n",
    "    #define the number of inducing points, increased to 2000 for better performance\n",
    "    num_inducing_points = 2000\n",
    "    # select inducing points with k-means\n",
    "    kmeans = KMeans(n_clusters=num_inducing_points, random_state=42).fit(X_train)\n",
    "    inducing_points = torch.from_numpy(kmeans.cluster_centers_).float()\n",
    "    # create model and likelihood\n",
    "    model = GPModel(inducing_points = inducing_points, kernel=kernel)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    # Initialize the likelihood noise\n",
    "    likelihood.noise = noise\n",
    "    print(f'Model: {model}')\n",
    "    print(f'Mean: {model.mean_module.constant.item()}')\n",
    "    print(f'Lengthscale: {model.covar_module.base_kernel.lengthscale}')\n",
    "    print(f'Outputscale: {model.covar_module.outputscale}')\n",
    "    print(f'Likelihood Noise: {likelihood.noise.item()}')\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "    num_epochs = 100\n",
    "    # set the model and likelihood in training mode\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    # define the optimizers\n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), \n",
    "                                                   num_data= y_train_tensor.size(0), \n",
    "                                                   lr=0.03)\n",
    "    hyperparameter_optimizer = torch.optim.Adam([{'params': model.hyperparameters()},\n",
    "                                                 {'params': likelihood.parameters()},\n",
    "                                                 ], \n",
    "                                                 lr=0.04)\n",
    "\n",
    "    # VariationalELBO is used for training\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=len(train_loader.dataset))\n",
    "\n",
    "    # Early stopping parameters\n",
    "    best_val_loss = np.inf\n",
    "    patience = 10\n",
    "    epochs_no_improve = 0\n",
    "    decimal_places = 3\n",
    "    tolerance = 10 ** (-decimal_places) # Early stopping tolerance: 0.001 (prevents premature stopping on small fluctuations)\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # Zero gradients from previous iteration        \n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "            hyperparameter_optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        # print every ten epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader):.4f}')         # Print the average loss for the epoch\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            f_val_preds = model(X_val_tensor)\n",
    "            val_loss = -mll(f_val_preds, y_val_tensor).item()  \n",
    "            \n",
    "            # Extract mean and standard deviation, Compute Negative Log-Likelihood (NLL) on validation set \n",
    "            # NLL = -log(p(y|x)) measures calibration quality of uncertainty estimates\n",
    "            val_preds = likelihood(f_val_preds)     \n",
    "            val_mean = val_preds.mean.cpu()\n",
    "            val_std = val_preds.stddev.cpu()\n",
    "            val_std = val_std.clamp_min(1e-6) # Minimum std to avoid log(0) in NLL computation\n",
    "            nll_per_point = -Normal(val_mean, val_std).log_prob(y_val_tensor.cpu()).numpy()\n",
    "            val_nll = nll_per_point.mean().item()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Validation Loss: {val_loss} Validation NLL: {val_nll}')\n",
    "            \n",
    "            # Early stopping check\n",
    "            if abs(val_nll - best_val_loss) < tolerance:\n",
    "\n",
    "                epochs_no_improve += 1\n",
    "            else:\n",
    "                epochs_no_improve = 0\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "                best_likelihood_state = likelihood.state_dict()   \n",
    "            \n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping at iteration {epoch + 1} with best validation loss: {best_val_loss:.3f}')\n",
    "                model.load_state_dict(best_model_state)\n",
    "                likelihood.load_state_dict(best_likelihood_state)\n",
    "\n",
    "                break\n",
    "\n",
    "\n",
    "    # Evaluation on the test set            \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = []\n",
    "    stddevs = []\n",
    "    with torch.no_grad():\n",
    "        # Make predictions on the test set\n",
    "        preds = likelihood(model(X_test_tensor))      \n",
    "        # Mean:\n",
    "        means.append(preds.mean.cpu())        \n",
    "        # Standard Deviation\n",
    "        stddevs.append(preds.stddev.cpu())\n",
    "\n",
    "    GPR_means = torch.cat(means)\n",
    "    GPR_stddevs = torch.cat(stddevs)\n",
    "    #convert to numpy arrays\n",
    "    GPR_means = GPR_means.numpy()\n",
    "    GPR_stddevs = GPR_stddevs.numpy()\n",
    "\n",
    "    # Calculate and print all metrics inclunding RMSE, MAE, R²-Score, NLL, CRPS\n",
    "    pnn_metrics = uct.metrics.get_all_metrics(GPR_means, GPR_stddevs, y_test)\n",
    "    print(pnn_metrics)\n",
    "\n",
    "    # use own function to calculate coverage and MPIW\n",
    "    ev_intervals = evaluate_intervals(GPR_means, GPR_stddevs, y_test, coverage=0.95)\n",
    "    print(f'coverage: {ev_intervals[\"coverage\"]}, MPIW: {ev_intervals[\"MPIW\"]}')\n",
    "\n",
    "    # Store predictions and results for this run\n",
    "    predictions_per_run = {\n",
    "                            'mean_prediction': GPR_means,\n",
    "                            'std_prediction': GPR_stddevs,\n",
    "    }\n",
    "\n",
    "    results_per_run = {\n",
    "                        'RMSE': pnn_metrics['accuracy']['rmse'],\n",
    "                        'MAE': pnn_metrics['accuracy']['mae'],\n",
    "                        'R2': pnn_metrics['accuracy']['r2'], \n",
    "                        'Correlation' : pnn_metrics['accuracy']['corr'],\n",
    "                        'NLL': pnn_metrics['scoring_rule']['nll'],\n",
    "                        'CRPS': pnn_metrics['scoring_rule']['crps'],\n",
    "                        'coverage': ev_intervals[\"coverage\"],\n",
    "                        'MPIW': ev_intervals[\"MPIW\"],\n",
    "                        }\n",
    "\n",
    "    predictions_list.append(predictions_per_run)\n",
    "    results_list.append(results_per_run)\n",
    "    \n",
    "#save the predictions \n",
    "with open(os.path.join(DE_prediction_path, \"GPR_predictions_list.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(predictions_list, f)\n",
    "\n",
    "#save the results in an excel file\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.to_excel(os.path.join(DE_result_path, \"GPR_results.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5193cb",
   "metadata": {},
   "source": [
    "Evaluation of the 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61498d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_prediction_path = r\"C:\\Users\\test\\Masterarbeit\\models\\Modelresults\\GPR\"\n",
    "DE_result_path = r\"C:\\Users\\test\\OneDrive\\Master Management und Engineering\\Masterarbeit\\Experimente\\Evaluation\\10 Runs\\GPR\"\n",
    "with open(os.path.join(DE_prediction_path, \"GPR_predictions_list.pkl\"), \"rb\") as f:\n",
    "    predictions_list = pickle.load(f)\n",
    "\n",
    "# Lists to store mean and std predictions across runs\n",
    "mean_list = []\n",
    "std_list = []\n",
    "\n",
    "for id, run in enumerate(predictions_list):\n",
    "    # extract mean and std predictions\n",
    "    mean = run['mean_prediction']\n",
    "    std = run['std_prediction']\n",
    "    \n",
    "    # append to lists\n",
    "    mean_list.append(mean)\n",
    "    std_list.append(std)\n",
    "    \n",
    "    # calibration Curve with UCT\n",
    "    uct.viz.plot_calibration(mean, std, y_test)\n",
    "    plt.savefig(os.path.join(DE_result_path, f\"calibration_run_{id+1}.svg\"), format ='svg')\n",
    "    plt.close()\n",
    "\n",
    "    # adversarial group calibration\n",
    "    uct.viz.plot_adversarial_group_calibration(mean, std, y_test)\n",
    "    plt.savefig(os.path.join(DE_result_path, f\"adversarial_group_calibration_run_{id+1}.svg\"), format ='svg')\n",
    "    plt.close()\n",
    "\n",
    "# predictions_list enthält pro Run ein Array mit 10403 Werten\n",
    "mean_matrix = np.array(mean_list)  # Shape: (n_runs, 10403)\n",
    "std_matrix = np.array(std_list)    # Shape: (n_runs, 10403)\n",
    "\n",
    "# Mittelwert und Std für jeden Datenpunkt über alle Runs\n",
    "mean_per_datapoint = np.mean(mean_matrix, axis=0)  # Shape: (10403,)\n",
    "std_per_datapoint = np.mean(std_matrix, axis=0)    # Shape: (10403,)\n",
    "\n",
    "# calibration Curve with UCT\n",
    "uct.viz.plot_calibration(mean_per_datapoint, std_per_datapoint, y_test)\n",
    "plt.savefig(os.path.join(DE_result_path, \"calibration_run_mean.svg\"), format ='svg')\n",
    "plt.close()\n",
    "\n",
    "# adversarial group calibration\n",
    "uct.viz.plot_adversarial_group_calibration(mean_per_datapoint, std_per_datapoint, y_test)\n",
    "plt.savefig(os.path.join(DE_result_path, \"adversarial_group_calibration_run_mean.svg\"), format ='svg')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
