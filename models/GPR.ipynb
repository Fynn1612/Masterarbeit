{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd9aac2",
   "metadata": {},
   "source": [
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import r2_score\n",
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP\n",
    "from torch.distributions import Normal\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# define the device for the setting\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# check the computer name and set the path accordingly\n",
    "if os.environ['COMPUTERNAME'] == 'FYNN':            # name of surface PC\n",
    "    sys.path.append(r'C:\\Users\\Surface\\Masterarbeit')\n",
    "elif os.environ['COMPUTERNAME'] == 'FYNNS-PC':  # desktop name\n",
    "    sys.path.append(r'C:\\Users\\test\\Masterarbeit')\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"Unbekannter Computername: \" + os.environ['COMPUTERNAME'])\n",
    "\n",
    "from utils.data_prep import load_tranform_and_split_data, set_seed\n",
    "from utils.metrices import evaluate_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c0938",
   "metadata": {},
   "source": [
    "Transformation Pipeline for Approximating GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9592945",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, feature_names = load_tranform_and_split_data('C1_V01_delta_kan', split_ratio=(0.6, 0.2, 0.2))\n",
    "\n",
    "# convert the data to PyTorch tensors\n",
    "# and add an extra dimension for the target variable\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float() \n",
    "y_train_tensor = torch.from_numpy(y_train).float() # Add extra dimension for compatibility\n",
    "y_val_tensor = torch.from_numpy(y_val).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float()\n",
    "print(X_train_tensor.shape, X_val_tensor.shape, X_test_tensor.shape)\n",
    "print(feature_names)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7df249",
   "metadata": {},
   "source": [
    "Stochastic Variational GP Regression Implementation\n",
    "\n",
    "Natural Gradient Descent with Variational Models for better and faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170376b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a TensorDataset and DataLoader for the training data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points, kernel):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module  = kernel\n",
    "\n",
    "        #self.mean_module.initialize(constant=y_train_tensor.mean().item())  # Initialize the mean to the mean of the training targets\n",
    "                \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "print(y_train_tensor.var(unbiased=False).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f74832",
   "metadata": {},
   "source": [
    "Create Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # Estimate the variance of the training targets for initializing the likelihood noise\n",
    "    y_var = y_train_tensor.var(unbiased=False).item()\n",
    "    noise = 1e-2 * y_var\n",
    "\n",
    "    # Suggest hyperparameters for optimization\n",
    "    lr_ngd = trial.suggest_float('lr', 1e-3, 1e-1, log=True)\n",
    "    lr_adam = trial.suggest_float('lr_adam', 1e-3, 1e-1, log=True)\n",
    "\n",
    "    #Rational Quadratic Kernel\n",
    "    rational_quadratic_kernel = gpytorch.kernels.RQKernel(ard_num_dims=X_train.shape[1], \n",
    "                                                        alpha_constraint=gpytorch.constraints.Interval(0.1, 10.0))\n",
    "    rational_quadratic_kernel.lengthscale = torch.ones(X_train.shape[1])\n",
    "    rational_quadratic_kernel.outputscale = y_var\n",
    "    rational_quadratic_kernel1 = gpytorch.kernels.ScaleKernel(rational_quadratic_kernel)\n",
    "\n",
    "    kernel = rational_quadratic_kernel1\n",
    "\n",
    "    kernel_name = type(kernel.base_kernel).__name__\n",
    "    #filename = f'Modelsaves/Apprx_GP_{kernel_name}_{idx}.pth'\n",
    "    print(kernel_name)\n",
    "\n",
    "    # Define the inducing points\n",
    "    # Randomly select 1000 inducing points from the training data\n",
    "    num_inducing_points = 1000\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_inducing_points, random_state=42).fit(X_train)\n",
    "    inducing_points = torch.from_numpy(kmeans.cluster_centers_).float()\n",
    "\n",
    "    model = GPModel(inducing_points = inducing_points, kernel=kernel)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    likelihood.noise = noise\n",
    "\n",
    "    print(f'Model: {model}')\n",
    "    print(f'Mean: {model.mean_module.constant.item()}')\n",
    "    print(f'Lengthscale: {model.covar_module.base_kernel.lengthscale}')\n",
    "    print(f'Outputscale: {model.covar_module.outputscale}')\n",
    "    print(f'Likelihood Noise: {likelihood.noise.item()}')\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "    num_epochs = 100\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data= y_train_tensor.size(0), lr=lr_ngd)\n",
    "\n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=lr_adam)\n",
    "\n",
    "    # VariationalELBO is used for training\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=len(train_loader.dataset))\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    patience = 10\n",
    "    epochs_no_improve = 0\n",
    "    decimal_places = 3\n",
    "    tolerance = 10 ** (-decimal_places)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # Zero gradients from previous iteration        \n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            # Print the loss for every tenth batch\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "            hyperparameter_optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        # print every ten epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader):.4f}')         # Print the average loss for the epoch\n",
    "\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            f_val_preds = model(X_val_tensor)\n",
    "            val_loss = -mll(f_val_preds, y_val_tensor).item()  \n",
    "            \n",
    "            val_preds = likelihood(f_val_preds)     \n",
    "            val_mean = val_preds.mean.cpu()\n",
    "            val_std = val_preds.stddev.cpu()\n",
    "            val_std = val_std.clamp_min(1e-6)\n",
    "            nll_per_point = -Normal(val_mean, val_std).log_prob(y_val_tensor.cpu()).numpy()\n",
    "            val_nll = nll_per_point.mean().item()\n",
    "            \n",
    "            r2_score_val = r2_score(y_val, val_mean)\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Validation Loss: {val_loss} Validation NLL: {val_nll} R²: {r2_score_val:.3f}')\n",
    "\n",
    "            trial.report(val_loss, step=epoch)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            if abs(val_nll - best_val_loss) < tolerance:\n",
    "\n",
    "                epochs_no_improve += 1\n",
    "            else:\n",
    "                epochs_no_improve = 0\n",
    "                best_val_loss = val_loss\n",
    "                # best_model_state = model.state_dict()\n",
    "                # best_likelihood_state = likelihood.state_dict()   \n",
    "            \n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping at iteration {epoch + 1} with best validation loss: {best_val_loss:.3f}')\n",
    "                # model.load_state_dict(best_model_state)\n",
    "                # likelihood.load_state_dict(best_likelihood_state)\n",
    "\n",
    "                break\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c3843",
   "metadata": {},
   "source": [
    "Execute Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f56cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# create a study object for Optuna\n",
    "study = optuna.create_study(\n",
    "\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(),                       #TPE (Tree-structured Parzen Estimator) sampler by default\n",
    "    pruner=optuna.pruners.MedianPruner(        \n",
    "        n_startup_trials=5,                                    # Number of trials to run before pruning starts\n",
    "        n_warmup_steps=5                                        # Number of warmup steps before pruning starts)\n",
    "    )\n",
    ")\n",
    "\n",
    "# move the tensors to the device\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "X_val_tensor = X_val_tensor.to(device)\n",
    "y_val_tensor = y_val_tensor.to(device)\n",
    "\n",
    "# optimize the objective function with Optuna\n",
    "# timeout=None means no time limit for the optimization, all trials will be run\n",
    "study.optimize(objective, n_trials=50, timeout=None, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9b2aa",
   "metadata": {},
   "source": [
    "Train best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_seed(SEED)\n",
    "# Estimate the variance of the training targets for initializing the likelihood noise\n",
    "y_var = y_train_tensor.var(unbiased=False).item()\n",
    "noise = 1e-2 * y_var\n",
    "\n",
    "#Rational Quadratic Kernel\n",
    "rational_quadratic_kernel = gpytorch.kernels.RQKernel(ard_num_dims=X_train.shape[1], \n",
    "                                                    #alpha_constraint=gpytorch.constraints.Interval(0.1, 10.0)\n",
    "                                                    )\n",
    "rational_quadratic_kernel.lengthscale = torch.ones(X_train.shape[1])\n",
    "rational_quadratic_kernel.outputscale = y_var\n",
    "rational_quadratic_kernel.mean_module.initialize(constant=y_train_tensor.mean().item())\n",
    "rational_quadratic_kernel1 = gpytorch.kernels.ScaleKernel(rational_quadratic_kernel)\n",
    "\n",
    "kernel = rational_quadratic_kernel1\n",
    "\n",
    "kernel_name = type(kernel.base_kernel).__name__\n",
    "#filename = f'Modelsaves/Apprx_GP_{kernel_name}_{idx}.pth'\n",
    "print(kernel_name)\n",
    "\n",
    "# Define the inducing points\n",
    "# Randomly select 1000 inducing points from the training data\n",
    "num_inducing_points = 2000\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_inducing_points, random_state=42).fit(X_train)\n",
    "inducing_points = torch.from_numpy(kmeans.cluster_centers_).float()\n",
    "\n",
    "model = GPModel(inducing_points = inducing_points, kernel=kernel)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "likelihood.noise = noise\n",
    "\n",
    "print(f'Model: {model}')\n",
    "print(f'Mean: {model.mean_module.constant.item()}')\n",
    "print(f'Lengthscale: {model.covar_module.base_kernel.lengthscale}')\n",
    "print(f'Outputscale: {model.covar_module.outputscale}')\n",
    "print(f'Likelihood Noise: {likelihood.noise.item()}')\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data= y_train_tensor.size(0), lr=0.03)\n",
    "\n",
    "hyperparameter_optimizer = torch.optim.Adam([\n",
    "    {'params': model.hyperparameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.04)\n",
    "\n",
    "# VariationalELBO is used for training\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=len(train_loader.dataset))\n",
    "\n",
    "best_val_loss = np.inf\n",
    "patience = 10\n",
    "epochs_no_improve = 0\n",
    "decimal_places = 3\n",
    "tolerance = 10 ** (-decimal_places)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # Zero gradients from previous iteration        \n",
    "        variational_ngd_optimizer.zero_grad()\n",
    "        hyperparameter_optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        # Print the loss for every tenth batch\n",
    "        loss.backward()\n",
    "        variational_ngd_optimizer.step()\n",
    "        hyperparameter_optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    # print every ten epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader):.4f}')         # Print the average loss for the epoch\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        f_val_preds = model(X_val_tensor)\n",
    "        val_loss = -mll(f_val_preds, y_val_tensor).item()  \n",
    "        \n",
    "        val_preds = likelihood(f_val_preds)     \n",
    "        val_mean = val_preds.mean.cpu()\n",
    "        val_std = val_preds.stddev.cpu()\n",
    "        val_std = val_std.clamp_min(1e-6)\n",
    "        nll_per_point = -Normal(val_mean, val_std).log_prob(y_val_tensor.cpu()).numpy()\n",
    "        val_nll = nll_per_point.mean().item()\n",
    "        \n",
    "        r2_score_val = r2_score(y_val, val_mean)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Validation Loss: {val_loss} Validation NLL: {val_nll} R²: {r2_score_val:.3f}')\n",
    "\n",
    "        if abs(val_nll - best_val_loss) < tolerance:\n",
    "\n",
    "            epochs_no_improve += 1\n",
    "        else:\n",
    "            epochs_no_improve = 0\n",
    "            best_val_loss = val_loss\n",
    "            # best_model_state = model.state_dict()\n",
    "            # best_likelihood_state = likelihood.state_dict()   \n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at iteration {epoch + 1} with best validation loss: {best_val_loss:.3f}')\n",
    "            # model.load_state_dict(best_model_state)\n",
    "            # likelihood.load_state_dict(best_likelihood_state)\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00854d9d",
   "metadata": {},
   "source": [
    "Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45acc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uncertainty_toolbox as uct\n",
    "print(model)\n",
    "print(model.mean_module.constant.item())\n",
    "print(model.covar_module.base_kernel.lengthscale)\n",
    "print(model.covar_module.outputscale)\n",
    "print(likelihood.noise.item())\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "means = []\n",
    "variances = []\n",
    "stddevs = []\n",
    "with torch.no_grad():\n",
    "    # Make predictions on the test set\n",
    "\n",
    "    preds = likelihood(model(X_test_tensor))\n",
    "    \n",
    "    # Mean:\n",
    "    means.append(preds.mean.cpu())\n",
    "    \n",
    "    # Variance\n",
    "    variances.append(preds.variance.cpu())\n",
    "    \n",
    "    # Standard Deviation\n",
    "    stddevs.append(preds.stddev.cpu())\n",
    "\n",
    "means = torch.cat(means)\n",
    "variances = torch.cat(variances)\n",
    "stddevs = torch.cat(stddevs)  \n",
    "\n",
    "pnn_metrics = uct.metrics.get_all_metrics(means.numpy(), stddevs.numpy(), y_test)\n",
    "print(pnn_metrics)\n",
    "\n",
    "# use own function to calculate coverage and MPIW\n",
    "ev_intervals = evaluate_intervals(means.numpy(), stddevs.numpy(), y_test, coverage=0.95)\n",
    "print(f'coverage: {ev_intervals[\"coverage\"]}, MPIW: {ev_intervals[\"MPIW\"]}')\n",
    "\n",
    "# calibration Curve with UCT\n",
    "uct.viz.plot_calibration(means.numpy(), stddevs.numpy(), y_test)\n",
    "\n",
    "# adversarial group calibration\n",
    "uct.viz.plot_adversarial_group_calibration(means.numpy(), stddevs.numpy(), y_test)\n",
    "\n",
    "# check type of means, variances, stddevs\n",
    "print(type(means), type(variances), type(stddevs))\n",
    "print(\"Means shape:\", means.shape)\n",
    "print(\"Variances shape:\", variances.shape)\n",
    "print(\"Standard Deviations shape:\", stddevs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc0425",
   "metadata": {},
   "source": [
    "Make 10 Runs with different Random Seed to evaluate GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb5d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "predictions_list = []\n",
    "list_of_seeds = [42, 123, 777, 2024, 5250, 8888, 9876, 10001, 31415, 54321]\n",
    "DE_prediction_path = r\"C:\\Users\\test\\Masterarbeit\\models\\Modelresults\\GPR\"\n",
    "DE_result_path = r\"C:\\Users\\test\\OneDrive\\Master Management und Engineering\\Masterarbeit\\Experimente\\Evaluation\\10 Runs\\GPR\"\n",
    "\n",
    "for run, seed in enumerate(list_of_seeds):\n",
    "\n",
    "    print(f\"Run {run+1} with seed {seed}\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    #\n",
    "    num_inducing_points = 2000\n",
    "    kmeans = KMeans(n_clusters=num_inducing_points, random_state=42).fit(X_train)\n",
    "    inducing_points = torch.from_numpy(kmeans.cluster_centers_).float()\n",
    "    model = GPModel(inducing_points = inducing_points, kernel=kernel)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    likelihood.noise = noise\n",
    "\n",
    "    print(f'Model: {model}')\n",
    "    print(f'Mean: {model.mean_module.constant.item()}')\n",
    "    print(f'Lengthscale: {model.covar_module.base_kernel.lengthscale}')\n",
    "    print(f'Outputscale: {model.covar_module.outputscale}')\n",
    "    print(f'Likelihood Noise: {likelihood.noise.item()}')\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "    num_epochs = 100\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data= y_train_tensor.size(0), lr=0.03)\n",
    "\n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.04)\n",
    "\n",
    "    # VariationalELBO is used for training\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=len(train_loader.dataset))\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    patience = 10\n",
    "    epochs_no_improve = 0\n",
    "    decimal_places = 3\n",
    "    tolerance = 10 ** (-decimal_places)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # Zero gradients from previous iteration        \n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            # Print the loss for every tenth batch\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "            hyperparameter_optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        # print every ten epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader):.4f}')         # Print the average loss for the epoch\n",
    "\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            f_val_preds = model(X_val_tensor)\n",
    "            val_loss = -mll(f_val_preds, y_val_tensor).item()  \n",
    "            \n",
    "            val_preds = likelihood(f_val_preds)     \n",
    "            val_mean = val_preds.mean.cpu()\n",
    "            val_std = val_preds.stddev.cpu()\n",
    "            val_std = val_std.clamp_min(1e-6)\n",
    "            nll_per_point = -Normal(val_mean, val_std).log_prob(y_val_tensor.cpu()).numpy()\n",
    "            val_nll = nll_per_point.mean().item()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Validation Loss: {val_loss} Validation NLL: {val_nll}')\n",
    "\n",
    "            if abs(val_nll - best_val_loss) < tolerance:\n",
    "\n",
    "                epochs_no_improve += 1\n",
    "            else:\n",
    "                epochs_no_improve = 0\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "                best_likelihood_state = likelihood.state_dict()   \n",
    "            \n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping at iteration {epoch + 1} with best validation loss: {best_val_loss:.3f}')\n",
    "                model.load_state_dict(best_model_state)\n",
    "                likelihood.load_state_dict(best_likelihood_state)\n",
    "\n",
    "                break\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = []\n",
    "    stddevs = []\n",
    "    with torch.no_grad():\n",
    "        # Make predictions on the test set\n",
    "        preds = likelihood(model(X_test_tensor))      \n",
    "        # Mean:\n",
    "        means.append(preds.mean.cpu())        \n",
    "        # Standard Deviation\n",
    "        stddevs.append(preds.stddev.cpu())\n",
    "\n",
    "    GPR_means = torch.cat(means)\n",
    "    GPR_stddevs = torch.cat(stddevs)\n",
    "\n",
    "    # Calculate and print all metrics inclunding RMSE, MAE, R²-Score, NLL, CRPS\n",
    "    pnn_metrics = uct.metrics.get_all_metrics(GPR_means, GPR_stddevs, y_test)\n",
    "    print(pnn_metrics)\n",
    "\n",
    "    # use own function to calculate coverage and MPIW\n",
    "    ev_intervals = evaluate_intervals(GPR_means, GPR_stddevs, y_test, coverage=0.95)\n",
    "    print(f'coverage: {ev_intervals[\"coverage\"]}, MPIW: {ev_intervals[\"MPIW\"]}')\n",
    "\n",
    "    predictions_per_run = {\n",
    "        'mean_prediction': GPR_means,\n",
    "        'std_prediction': GPR_stddevs,\n",
    "    }\n",
    "\n",
    "    results_per_run = {\n",
    "    'RMSE': pnn_metrics['accuracy']['rmse'],\n",
    "    'MAE': pnn_metrics['accuracy']['mae'],\n",
    "    'R2': pnn_metrics['accuracy']['r2'], \n",
    "    'Correlation' : pnn_metrics['accuracy']['corr'],\n",
    "    'NLL': pnn_metrics['scoring_rule']['nll'],\n",
    "    'CRPS': pnn_metrics['scoring_rule']['crps'],\n",
    "    'coverage': ev_intervals[\"coverage\"],\n",
    "    'MPIW': ev_intervals[\"MPIW\"],\n",
    "    }\n",
    "\n",
    "    predictions_list.append(predictions_per_run)\n",
    "    results_list.append(results_per_run)\n",
    "#save the predictions \n",
    "with open(os.path.join(DE_prediction_path, \"GPR_predictions_list.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(predictions_list, f)\n",
    "\n",
    "#save the results in an excel file\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.to_excel(os.path.join(DE_result_path, \"GPR_results.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61498d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DE_prediction_path, \"GPR_predictions_list.pkl\"), \"rb\") as f:\n",
    "    predictions_list = pickle.load(f)\n",
    "\n",
    "mean_list = []\n",
    "std_list = []\n",
    "\n",
    "for id, run in enumerate(predictions_list):\n",
    "    # extract mean and std predictions\n",
    "    mean = run['mean_prediction']\n",
    "    std = run['std_prediction']\n",
    "    \n",
    "    # append to lists\n",
    "    mean_list.append(mean)\n",
    "    std_list.append(std)\n",
    "    \n",
    "    # calibration Curve with UCT\n",
    "    uct.viz.plot_calibration(mean, std, y_test)\n",
    "    plt.savefig(os.path.join(DE_result_path, f\"calibration_run_{id+1}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # adversarial group calibration\n",
    "    uct.viz.plot_adversarial_group_calibration(mean, std, y_test)\n",
    "    plt.savefig(os.path.join(DE_result_path, f\"adversarial_group_calibration_run_{id+1}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# predictions_list enthält pro Run ein Array mit 10403 Werten\n",
    "mean_matrix = np.array(mean_list)  # Shape: (n_runs, 10403)\n",
    "std_matrix = np.array(std_list)    # Shape: (n_runs, 10403)\n",
    "\n",
    "# Mittelwert und Std für jeden Datenpunkt über alle Runs\n",
    "mean_per_datapoint = np.mean(mean_matrix, axis=0)  # Shape: (10403,)\n",
    "std_per_datapoint = np.mean(std_matrix, axis=0)    # Shape: (10403,)\n",
    "\n",
    "# calibration Curve with UCT\n",
    "uct.viz.plot_calibration(mean_per_datapoint, std_per_datapoint, y_test)\n",
    "plt.savefig(os.path.join(DE_result_path, \"calibration_run_mean.png\"))\n",
    "plt.close()\n",
    "\n",
    "# adversarial group calibration\n",
    "uct.viz.plot_adversarial_group_calibration(mean_per_datapoint, std_per_datapoint, y_test)\n",
    "plt.savefig(os.path.join(DE_result_path, \"adversarial_group_calibration_run_mean.png\"))\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
