{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGBoost for outputting Probalitiy Distribution instead of single value\n",
    "\n",
    "https://stanfordmlgroup.github.io/ngboost/1-useage.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ngboost import NGBRegressor\n",
    "from ngboost.scores import LogScore\n",
    "from ngboost.distns import  Normal\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import uncertainty_toolbox as uct\n",
    "\n",
    "# check the computer name and set the path accordingly\n",
    "if os.environ['COMPUTERNAME'] == 'FYNN':            # name of surface PC\n",
    "    sys.path.append(r'C:\\Users\\Surface\\Masterarbeit')\n",
    "elif os.environ['COMPUTERNAME'] == 'FYNNS-PC':  # desktop name\n",
    "    sys.path.append(r'C:\\Users\\test\\Masterarbeit')\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"Unbekannter Computername: \" + os.environ['COMPUTERNAME'])\n",
    "\n",
    "from utils.data_prep import load_tranform_and_split_data, set_seed\n",
    "from utils.metrices import evaluate_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Preprocessing to compare the algorithms, although Decision Trees are not sensitve to feature scalling, they do not use geometric distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and transform the data, split it into training, validation, and test sets\n",
    "# uses random state 42 internally for reproducibility\n",
    "# the split ratio is 60% training, 20% validation, and 20%\n",
    "# return the feature names for later use\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, feature_names = load_tranform_and_split_data('C1_V01_delta_kan', \n",
    "                                                                                                       split_ratio=(0.6, 0.2, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # Define the hyperparameters to tune\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 1000, step = 50)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2,log=True)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 4, 20)\n",
    "    \n",
    "    # Create the NGBRegressor model with the suggested hyperparameters\n",
    "    ngb = NGBRegressor(\n",
    "        Dist=Normal, \n",
    "        Score=LogScore, \n",
    "        Base=DecisionTreeRegressor(criterion='friedman_mse', max_depth=max_depth),\n",
    "        verbose=False, \n",
    "        n_estimators=n_estimators, \n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    ngb.fit(X_train, y_train, X_val=X_val, Y_val=y_val, early_stopping_rounds=20)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_preds = ngb.predict(X_val)\n",
    "    y_dists = ngb.pred_dist(X_val)\n",
    "    \n",
    "    # Calculate the negative log likelihood\n",
    "    nll = -y_dists.logpdf(y_val).mean()\n",
    "    rmse = root_mean_squared_error(y_val, y_preds)\n",
    "    print(f\"Trial {trial.number}: NLL={nll}, RMSE={rmse}\")\n",
    "    \n",
    "    return nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=optuna.samplers.TPESampler(),  # Use TPE sampler for hyperparameter optimization\n",
    "    pruner=optuna.pruners.MedianPruner(        \n",
    "        n_startup_trials=20,                                    # Number of trials to run before pruning starts\n",
    "        n_warmup_steps=5                                        # Number of warmup steps before pruning starts\n",
    "    )\n",
    ")\n",
    "study.optimize(objective, n_trials=200, timeout=None, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "print(study.best_params)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "ngb = NGBRegressor(Dist=Normal, \n",
    "                   Score = LogScore, \n",
    "                   Base = DecisionTreeRegressor(criterion='friedman_mse',                                                \n",
    "                                                 max_depth=9,\n",
    "                                                 random_state=SEED), \n",
    "                   verbose = True, \n",
    "                   n_estimators=750, \n",
    "                   learning_rate=0.003,\n",
    "                   random_state=SEED)\n",
    "\n",
    "ngb.fit(X_train, y_train, X_val=X_val, Y_val=y_val, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalute 1 Run Using Uncertainty Toolbox to get metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "y_test_preds = ngb.predict(X_test)\n",
    "y_test_dists = ngb.pred_dist(X_test)\n",
    "# Extract standard deviation from test predictions\n",
    "test_stddev = y_test_dists.scale\n",
    "\n",
    "# Calculate and print all metrics inclunding RMSE, MAE, R²-Score, NLL, CRPS\n",
    "pnn_metrics = uct.metrics.get_all_metrics( y_test_preds, test_stddev,y_test)\n",
    "print(pnn_metrics)\n",
    "#print(pnn_metrics['accuracy']['rmse'])\n",
    "# Calculate coverage for 95% confidence interval\n",
    "coverage_95 = uct.metrics_calibration.get_proportion_in_interval(y_test_preds, test_stddev, y_test, quantile = 0.95 )\n",
    "print(f\"Coverage 95%: {coverage_95}\")\n",
    "\n",
    "# use own function to calculate coverage and MPIW\n",
    "ev_intervals = evaluate_intervals(y_test_preds, test_stddev, y_test, coverage=0.95)\n",
    "print(f'coverage: {ev_intervals[\"coverage\"]}, MPIW: {ev_intervals[\"MPIW\"]}')\n",
    "\n",
    "# calibration Curve with UCT\n",
    "uct.viz.plot_calibration(y_test_preds, test_stddev, y_test)\n",
    "\n",
    "# adversarial group calibration\n",
    "uct.viz.plot_adversarial_group_calibration(y_test_preds, test_stddev, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do 10 Runs for a more representative Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 with seed 42\n",
      "[iter 0] loss=3.4547 val_loss=3.4396 scale=1.0000 norm=6.1507\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     13\u001b[39m ngb = NGBRegressor(Dist=Normal, \n\u001b[32m     14\u001b[39m                     Score = LogScore, \n\u001b[32m     15\u001b[39m                     Base = DecisionTreeRegressor(criterion=\u001b[33m'\u001b[39m\u001b[33mfriedman_mse\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m                     learning_rate=\u001b[32m0.003\u001b[39m,\n\u001b[32m     21\u001b[39m                     random_state=seed)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mngb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[32m     27\u001b[39m y_test_preds = ngb.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\test\\Masterarbeit\\.venv\\Lib\\site-packages\\ngboost\\ngboost.py:258\u001b[39m, in \u001b[36mNGBoost.fit\u001b[39m\u001b[34m(self, X, Y, X_val, Y_val, sample_weight, val_sample_weight, train_loss_monitor, val_loss_monitor, early_stopping_rounds)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m.scalings = []\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m.col_idxs = []\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loss_monitor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loss_monitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loss_monitor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loss_monitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\test\\Masterarbeit\\.venv\\Lib\\site-packages\\ngboost\\ngboost.py:435\u001b[39m, in \u001b[36mNGBoost.partial_fit\u001b[39m\u001b[34m(self, X, Y, X_val, Y_val, sample_weight, val_sample_weight, train_loss_monitor, val_loss_monitor, early_stopping_rounds)\u001b[39m\n\u001b[32m    432\u001b[39m loss = loss_list[-\u001b[32m1\u001b[39m]\n\u001b[32m    433\u001b[39m grads = D.grad(Y_batch, natural=\u001b[38;5;28mself\u001b[39m.natural_gradient)\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m proj_grad = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m scale = \u001b[38;5;28mself\u001b[39m.line_search(proj_grad, P_batch, Y_batch, weight_batch)\n\u001b[32m    438\u001b[39m params -= (\n\u001b[32m    439\u001b[39m     \u001b[38;5;28mself\u001b[39m.learning_rate\n\u001b[32m    440\u001b[39m     * scale\n\u001b[32m    441\u001b[39m     * np.array([m.predict(X[:, col_idx]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.base_models[-\u001b[32m1\u001b[39m]]).T\n\u001b[32m    442\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\test\\Masterarbeit\\.venv\\Lib\\site-packages\\ngboost\\ngboost.py:172\u001b[39m, in \u001b[36mNGBoost.fit_base\u001b[39m\u001b[34m(self, X, grads, sample_weight)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_base\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, grads, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m         models = [\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mBase\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads.T]\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    174\u001b[39m         models = [\n\u001b[32m    175\u001b[39m             clone(\u001b[38;5;28mself\u001b[39m.Base).fit(X, g, sample_weight=sample_weight) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads.T\n\u001b[32m    176\u001b[39m         ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\test\\Masterarbeit\\.venv\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\test\\Masterarbeit\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[39m, in \u001b[36mDecisionTreeRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m   1374\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1376\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[32m   1377\u001b[39m \n\u001b[32m   1378\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1401\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\test\\Masterarbeit\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "predictions_list = []\n",
    "list_of_seeds = [42, 123, 777, 2024, 5250, 8888, 9876, 10001, 31415, 54321]\n",
    "ngb_prediction_path = r\"C:\\Users\\test\\Masterarbeit\\models\\Modelresults\\NGBoost\"\n",
    "ngb_result_path = r\"C:\\Users\\test\\OneDrive\\Master Management und Engineering\\Masterarbeit\\Experimente\\Evaluation\\10 Runs\\NGBoost\"\n",
    "\n",
    "for run, seed in enumerate(list_of_seeds):\n",
    "\n",
    "    print(f\"Run {run+1} with seed {seed}\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Define model, use seed to intialize the base learner and ngboost\n",
    "    ngb = NGBRegressor(Dist=Normal, \n",
    "                        Score = LogScore, \n",
    "                        Base = DecisionTreeRegressor(criterion='friedman_mse',\n",
    "                                                        max_depth=9,\n",
    "                                                        random_state=seed), \n",
    "                        verbose = True, \n",
    "                        n_estimators=750, \n",
    "                        learning_rate=0.003,\n",
    "                        random_state=seed)\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    ngb.fit(X_train, y_train, X_val=X_val, Y_val=y_val, early_stopping_rounds=50)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_test_preds = ngb.predict(X_test)\n",
    "    y_test_dists = ngb.pred_dist(X_test)\n",
    "    # Extract standard deviation from test predictions\n",
    "    test_stddev = y_test_dists.scale\n",
    "\n",
    "    # Calculate and print all metrics inclunding RMSE, MAE, R²-Score, NLL, CRPS\n",
    "    pnn_metrics = uct.metrics.get_all_metrics( y_test_preds, test_stddev,y_test)\n",
    "    print(pnn_metrics)\n",
    "\n",
    "    # use own function to calculate coverage and MPIW\n",
    "    ev_intervals = evaluate_intervals(y_test_preds, test_stddev, y_test, coverage=0.95)\n",
    "    print(f'coverage: {ev_intervals[\"coverage\"]}, MPIW: {ev_intervals[\"MPIW\"]}')\n",
    "\n",
    "    predictions_per_run = {\n",
    "        'mean_prediction': y_test_preds,\n",
    "        'std_prediction': test_stddev,\n",
    "    }\n",
    "\n",
    "    results_per_run = {\n",
    "    'RMSE': pnn_metrics['accuracy']['rmse'],\n",
    "    'MAE': pnn_metrics['accuracy']['mae'],\n",
    "    'R2': pnn_metrics['accuracy']['r2'], \n",
    "    'Correlation' : pnn_metrics['accuracy']['corr'],\n",
    "    'NLL': pnn_metrics['scoring_rule']['nll'],\n",
    "    'CRPS': pnn_metrics['scoring_rule']['crps'],\n",
    "    'coverage': ev_intervals[\"coverage\"],\n",
    "    'MPIW': ev_intervals[\"MPIW\"],\n",
    "    }\n",
    "\n",
    "    predictions_list.append(predictions_per_run)\n",
    "    results_list.append(results_per_run)\n",
    "#save the predictions \n",
    "with open(os.path.join(ngb_prediction_path, \"ngboost_predictions_list.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(predictions_list, f)\n",
    "\n",
    "#save the results in an excel file\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.to_excel(os.path.join(ngb_result_path, \"ngboost_results.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngb_prediction_path = r\"C:\\Users\\test\\Masterarbeit\\models\\Modelresults\\NGBoost\"\n",
    "ngb_result_path = r\"C:\\Users\\test\\OneDrive\\Master Management und Engineering\\Masterarbeit\\Experimente\\Evaluation\\10 Runs\\NGBoost\"\n",
    "\n",
    "with open(os.path.join(ngb_prediction_path, \"ngboost_predictions_list.pkl\"), \"rb\") as f:\n",
    "    predictions_list = pickle.load(f)\n",
    "\n",
    "mean_list = []\n",
    "std_list = []\n",
    "\n",
    "for id, run in enumerate(predictions_list):\n",
    "    # extract mean and std predictions\n",
    "    mean = run['mean_prediction']\n",
    "    std = run['std_prediction']\n",
    "    \n",
    "    # append to lists\n",
    "    mean_list.append(mean)\n",
    "    std_list.append(std)\n",
    "    \n",
    "    # calibration Curve with UCT\n",
    "    uct.viz.plot_calibration(mean, std, y_test)\n",
    "    plt.savefig(os.path.join(ngb_result_path, f\"calibration_run_{id+1}.svg\"), format ='svg')\n",
    "    plt.savefig(os.path.join(ngb_result_path, f\"calibration_run_{id+1}.png\"), format ='png')\n",
    "    plt.close()\n",
    "\n",
    "    # adversarial group calibration\n",
    "    uct.viz.plot_adversarial_group_calibration(mean, std, y_test)\n",
    "    plt.savefig(os.path.join(ngb_result_path, f\"adversarial_group_calibration_run_{id+1}.svg\"), format ='svg')\n",
    "    plt.savefig(os.path.join(ngb_result_path, f\"adversarial_group_calibration_run_{id+1}.png\"), format ='png')\n",
    "    plt.close()\n",
    "\n",
    "# predictions_list enthält pro Run ein Array mit 10403 Werten\n",
    "mean_matrix = np.array(mean_list)  # Shape: (n_runs, 10403)\n",
    "std_matrix = np.array(std_list)    # Shape: (n_runs, 10403)\n",
    "\n",
    "# Mittelwert und Std für jeden Datenpunkt über alle Runs\n",
    "mean_per_datapoint = np.mean(mean_matrix, axis=0)  # Shape: (10403,)\n",
    "std_per_datapoint = np.mean(std_matrix, axis=0)    # Shape: (10403,)\n",
    "\n",
    "# calibration Curve with UCT\n",
    "uct.viz.plot_calibration(mean_per_datapoint, std_per_datapoint, y_test)\n",
    "plt.savefig(os.path.join(ngb_result_path, \"calibration_run_mean.svg\"), format ='svg')\n",
    "plt.savefig(os.path.join(ngb_result_path, \"calibration_run_mean.png\"), format ='png')\n",
    "plt.close()\n",
    "\n",
    "# adversarial group calibration\n",
    "uct.viz.plot_adversarial_group_calibration(mean_per_datapoint, std_per_datapoint, y_test)\n",
    "plt.savefig(os.path.join(ngb_result_path, \"adversarial_group_calibration_run_mean.svg\"), format ='svg')\n",
    "plt.savefig(os.path.join(ngb_result_path, \"adversarial_group_calibration_run_mean.png\"), format ='png')\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
