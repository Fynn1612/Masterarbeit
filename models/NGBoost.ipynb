{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from ngboost import NGBRegressor\n",
    "from ngboost.scores import LogScore\n",
    "from ngboost.distns import  Normal\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import pickle\n",
    "import uncertainty_toolbox as uct\n",
    "\n",
    "# check the computer name and set the path accordingly\n",
    "if os.environ['COMPUTERNAME'] == 'FYNN':            # name of surface PC\n",
    "    sys.path.append(r'C:\\Users\\Surface\\Masterarbeit')\n",
    "elif os.environ['COMPUTERNAME'] == 'FYNNS-PC':  # desktop name\n",
    "    sys.path.append(r'C:\\Users\\test\\Masterarbeit')\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"Unbekannter Computername: \" + os.environ['COMPUTERNAME'])\n",
    "\n",
    "import utils.data_prep as data_prep\n",
    "import utils.metrices as metrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Preprocessing to compare the algorithms, although Decision Trees are not sensitve to feature scalling, they do not use geometric distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and transform the data, split it into training, validation, and test sets\n",
    "# uses random state 42 internally for reproducibility\n",
    "# the split ratio is 60% training, 20% validation, and 20%\n",
    "# return the feature names for later use\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, feature_names = data_prep.load_tranform_and_split_data('C1_V01_delta_kan', split_ratio=(0.6, 0.2, 0.2))\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGBoost for outputting Probalitiy Distribution instead of single value https://stanfordmlgroup.github.io/ngboost/1-useage.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    set_seed(SEED)  # Set the seed for reproducibility\n",
    "    # Define the hyperparameters to tune\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 1000, step = 50)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2,log=True)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 4, 20)\n",
    "    \n",
    "    # Create the NGBRegressor model with the suggested hyperparameters\n",
    "    ngb = NGBRegressor(\n",
    "        Dist=Normal, \n",
    "        Score=LogScore, \n",
    "        Base=DecisionTreeRegressor(criterion='friedman_mse', max_depth=max_depth),\n",
    "        verbose=False, \n",
    "        n_estimators=n_estimators, \n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    ngb.fit(X_train, y_train, X_val=X_val, Y_val=y_val, early_stopping_rounds=20)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_preds = ngb.predict(X_val)\n",
    "    y_dists = ngb.pred_dist(X_val)\n",
    "    \n",
    "    # Calculate the negative log likelihood\n",
    "    nll = -y_dists.logpdf(y_val).mean()\n",
    "    rmse = root_mean_squared_error(y_val, y_preds)\n",
    "    print(f\"Trial {trial.number}: NLL={nll}, RMSE={rmse}\")\n",
    "    \n",
    "    return nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=optuna.samplers.TPESampler(seed = SEED),  # Use TPE sampler for hyperparameter optimization\n",
    "    pruner=optuna.pruners.MedianPruner(        \n",
    "        n_startup_trials=20,                                    # Number of trials to run before pruning starts\n",
    "        n_warmup_steps=5                                        # Number of warmup steps before pruning starts\n",
    "    )\n",
    ")\n",
    "study.optimize(objective, n_trials=200, timeout=None, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "print(study.best_params)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "ngb = NGBRegressor(Dist=Normal, \n",
    "                   Score = LogScore, \n",
    "                   Base = DecisionTreeRegressor(criterion='friedman_mse',                                                \n",
    "                                                 max_depth=9,\n",
    "                                                 random_state=SEED), \n",
    "                   verbose = True, \n",
    "                   n_estimators=750, \n",
    "                   learning_rate=0.003,\n",
    "                   random_state=SEED)\n",
    "\n",
    "ngb.fit(X_train, y_train, X_val=X_val, Y_val=y_val, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Uncertainty Toolbox to get metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "y_test_preds = ngb.predict(X_test)\n",
    "y_test_dists = ngb.pred_dist(X_test)\n",
    "# Extract standard deviation from test predictions\n",
    "test_stddev = y_test_dists.scale\n",
    "\n",
    "# Calculate and print all metrics inclunding RMSE, MAE, R²-Score, NLL, CRPS\n",
    "pnn_metrics = uct.metrics.get_all_metrics( y_test_preds, test_stddev,y_test)\n",
    "print(pnn_metrics)\n",
    "#print(pnn_metrics['accuracy']['rmse'])\n",
    "# Calculate coverage for 95% confidence interval\n",
    "coverage_95 = uct.metrics_calibration.get_proportion_in_interval(y_test_preds, test_stddev, y_test, quantile = 0.95 )\n",
    "print(f\"Coverage 95%: {coverage_95}\")\n",
    "\n",
    "# use own function to calculate coverage and MPIW\n",
    "ev_intervals = metrices.evaluate_intervals(y_test_preds, test_stddev, y_test, coverage=0.95)\n",
    "print(f'coverage: {ev_intervals[\"coverage\"]}, MPIW: {ev_intervals[\"MPIW\"]}')\n",
    "\n",
    "# calibration Curve with UCT\n",
    "uct.viz.plot_calibration(y_test_preds, test_stddev, y_test)\n",
    "\n",
    "# adversarial group calibration\n",
    "uct.viz.plot_adversarial_group_calibration(y_test_preds, test_stddev, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do 10 Runs for a more representative Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_runs = 10\n",
    "result = {}\n",
    "\n",
    "ngb_path = r\"C:\\Users\\test\\Masterarbeit\\models\\Modelsaves\\NGBoost\"\n",
    "ngb_result_path = r\"C:\\Users\\test\\Masterarbeit\\models\\Modelresults\\NGBoost\"\n",
    "\n",
    "for run in range(number_of_runs):\n",
    "\n",
    "    model_name = f'ngboost_run_{run+1}.pkl'\n",
    "    # choose a random seed for each run\n",
    "    seed = random.randint(0, 10000)\n",
    "    print(f\"Run {run+1}/{number_of_runs} with seed {seed}\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    ngb = NGBRegressor(Dist=Normal, \n",
    "                   Score = LogScore, \n",
    "                   Base = DecisionTreeRegressor(criterion='friedman_mse' , max_depth=9), \n",
    "                   verbose = True, \n",
    "                   n_estimators=750, \n",
    "                   learning_rate=0.003)\n",
    "    # Train and evaluate the model\n",
    "    ngb.fit(X_train, y_train, X_val=X_val, Y_val=y_val, early_stopping_rounds=20)\n",
    "\n",
    "    pickle.dump(ngb, open(os.path.join(ngb_path, model_name), \"wb\"))\n",
    "\n",
    "    y_test_preds = ngb.predict(X_test)\n",
    "    y_test_dists = ngb.pred_dist(X_test)\n",
    "    # Extract standard deviation from test predictions\n",
    "    test_stddev = y_test_dists.scale\n",
    "\n",
    "    # Calculate and print all metrics inclunding RMSE, MAE, R²-Score, NLL, CRPS\n",
    "    pnn_metrics = uct.metrics.get_all_metrics( y_test_preds, test_stddev,y_test)\n",
    "    print(pnn_metrics)\n",
    "\n",
    "   # use own function to calculate coverage and MPIW\n",
    "    ev_intervals = evaluate_intervals(y_test_preds, test_stddev, y_test, coverage=0.95)\n",
    "    print(f'coverage: {ev_intervals[\"coverage\"]}, MPIW: {ev_intervals[\"MPIW\"]}')\n",
    "\n",
    "    results_per_run = {\n",
    "    'mean_prediction': y_test_preds,\n",
    "    'std_prediction' : test_stddev,\n",
    "    'coverage': ev_intervals[\"coverage\"],\n",
    "    'MPIW': ev_intervals[\"MPIW\"],\n",
    "    'RMSE': pnn_metrics['accuracy']['rmse'],\n",
    "    'MAE': pnn_metrics['accuracy']['mae'],\n",
    "    'R2': pnn_metrics['accuracy']['r2'],\n",
    "    'NLL': pnn_metrics['scoring_rule']['nll'],\n",
    "    'CRPS': pnn_metrics['scoring_rule']['crps']\n",
    "    }\n",
    "\n",
    "    result[f'run_{run+1}'] = results_per_run\n",
    "\n",
    "with open(ngb_result_path, 'wb') as f:\n",
    "    pickle.dump(result, f)    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
