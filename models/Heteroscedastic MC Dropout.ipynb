{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2b26c4",
   "metadata": {},
   "source": [
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "923c3ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import r2_score\n",
    "import NN_model\n",
    "\n",
    "# define the device for the setting\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# check the computer name and set the path accordingly\n",
    "if os.environ['COMPUTERNAME'] == 'FYNN':            # name of surface PC\n",
    "    sys.path.append(r'C:\\Users\\Surface\\Masterarbeit')\n",
    "elif os.environ['COMPUTERNAME'] == 'FYNNS-PC':      # desktop name\n",
    "    sys.path.append(r'C:\\Users\\test\\Masterarbeit')\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"Unbekannter Computername: \" + os.environ['COMPUTERNAME'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab07ae",
   "metadata": {},
   "source": [
    "Load and transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_prep\n",
    "\n",
    "#load and transform the data, split it into training, validation, and test sets\n",
    "# the split ratio is 60% training, 20% validation, and 20%\n",
    "# return the feature names for later use\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, feature_names = data_prep.load_tranform_and_split_data('C1_V01_delta_kan', split_ratio=(0.6, 0.2, 0.2))\n",
    "\n",
    "# convert the data to PyTorch tensors\n",
    "# and add an extra dimension for the target variable\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float() \n",
    "y_train_tensor = torch.from_numpy(y_train).float().reshape(-1,1) # Add extra dimension for compatibility\n",
    "y_val_tensor = torch.from_numpy(y_val).float().reshape(-1,1)\n",
    "y_test_tensor = torch.from_numpy(y_test).float().reshape(-1,1)\n",
    "print(X_train_tensor.shape, X_val_tensor.shape, X_test_tensor.shape)\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9230c24c",
   "metadata": {},
   "source": [
    "Hyperparameter Search with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    " \n",
    "    # hyperparameter for sampling with Optuna\n",
    "    n_layer = trial.suggest_int(\"n_layer\", 2, 4)  # number of hidden layers\n",
    "    n_neurons = trial.suggest_int(\"n_neurons\", 32, 256, step=32)  # number of neurons in each hidden layer\n",
    "    do_rate = trial.suggest_float(\"do_rate\", 0.05, 0.5)     # dropout rate \n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)    # learning rate\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256]) \n",
    "    \n",
    "    # decay factor for the number of neurons in each layer\n",
    "    # e.g. if n_neurons = 256 and n_layer = 3\n",
    "    # then the hidden_dims will be [256, 128, 64]\n",
    "    decay = 0.5\n",
    "    # this creates a list of integers representing the number of neurons in each hidden layer\n",
    "    hidden_dims = [int(n_neurons * decay**i) for i in range(n_layer)]\n",
    "    print(f\"Hidden dimensions: {hidden_dims}\")\n",
    "        \n",
    "    # generate the model with the sampled hyperparameters\n",
    "    # and move it to the device (GPU or CPU)\n",
    "    model = NN_model.Custom_NN_Model(\n",
    "        input_dim=X_train_tensor.shape[1],\n",
    "        hidden_dims=hidden_dims,\n",
    "        output_dim=1,\n",
    "        do_rate=do_rate,\n",
    "        loss_type = 'heteroscedastic'\n",
    "    ).to(device)\n",
    "\n",
    "    # AdamW optimizer, where weight decay does not accumulate in the momentum nor variance.\n",
    "    optimizer = torch.optim.AdamW(params = model.parameters(), lr = lr, weight_decay=0.0001)  \n",
    "\n",
    "    # DataLoader for batching the data\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # integrate early stopping\n",
    "    patience = 20  # number of epochs with no improvement after which training will be stopped\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # training the model\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)  # Move data to the device (GPU or CPU)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = NN_model.heteroscedastic_loss(model, X_batch, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # validation loss calculation after each epoch\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = NN_model.heteroscedastic_loss(model, X_val_tensor, y_val_tensor)\n",
    "            \n",
    "        trial.report(val_loss, step=epoch)\n",
    "        \n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            #print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Best Val Loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}, Best Val Loss: {best_val_loss:.4f}\")\n",
    "                break\n",
    "    \n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.trial import TrialState\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# number of epochs for training\n",
    "epochs = 500\n",
    "\n",
    "# create a study object for Optuna\n",
    "study = optuna.create_study(\n",
    "    #study_name=\"MC_Dropout_Optuna_Network_architecture\",\n",
    "    #storage=\"sqlite:///mc_dropout_study.db\",                    # Use SQLite database to store the\n",
    "    #load_if_exists=True,                                        # load the study if it already exists\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(),                       #TPE (Tree-structured Parzen Estimator) sampler by default\n",
    "    pruner=optuna.pruners.MedianPruner(        \n",
    "        n_startup_trials=10,                                    # Number of trials to run before pruning starts\n",
    "        n_warmup_steps=5                                        # Number of warmup steps before pruning starts)\n",
    "    )\n",
    ")\n",
    "\n",
    "# move the tensors to the device\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "X_val_tensor = X_val_tensor.to(device)\n",
    "y_val_tensor = y_val_tensor.to(device)\n",
    "\n",
    "# optimize the objective function with Optuna\n",
    "# timeout=None means no time limit for the optimization, all trials will be run\n",
    "study.optimize(objective, n_trials=100, timeout=None, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71587491",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8acab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selcet Device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move training and val data  to device\n",
    "X_train_tensor = X_train_tensor.to(device)  \n",
    "X_val_tensor = X_val_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "y_val_tensor = y_val_tensor.to(device)\n",
    "# Create heteroscedastic model instance with best parameters and move to device\n",
    "model_heteroscedastic = NN_model.Custom_NN_Model(input_dim=X_train.shape[1], hidden_dims=[256,  64,  16], \n",
    "                                                 output_dim=1, do_rate=0.26992510108950274, \n",
    "                                                 loss_type = 'heteroscedastic').to(device)  # Create model instance and move to device\n",
    "#print(f\"Model: {model_heteroscedastic}\")\n",
    "# Train the model with the best parameters\n",
    "optimizer_heteroscedastic = torch.optim.AdamW(params = model_heteroscedastic.parameters(), lr = 0.004172847065911164, weight_decay = 0.0001)  # Use AdamW optimizer with specified learning rate and weight decay\n",
    "tr_model_heteroscedastic = NN_model.train_model(model_heteroscedastic, X_train_tensor, y_train_tensor, \n",
    "                                                X_val_tensor, y_val_tensor, batch_size=64, n_epochs=1000, \n",
    "                                                optimizer= optimizer_heteroscedastic, patience=50, \n",
    "                                                loss_type='heteroscedastic', device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a1be6",
   "metadata": {},
   "source": [
    "Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with multiple forward passes\n",
    "# keep the model in training mode to keep dropout active\n",
    "tr_model_heteroscedastic.train()\n",
    "tr_model_heteroscedastic.to('cpu')  # Ensure the model is on the correct device\n",
    "X_tr = X_train_tensor.to('cpu')  # Ensure the input data is on the correct device\n",
    "X_te = X_test_tensor.to('cpu')  # Ensure the test data is on the correct device\n",
    "# Number of stochastic forward passes for MC Dropout\n",
    "n_samples = 250\n",
    "\n",
    "# Make multiple stochastic predictions (MC Dropout) on the train data\n",
    "y_train_pred_heteroscedastic_mean = []\n",
    "y_train_pred_heteroscedastic_var= []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    mean, log_var = tr_model_heteroscedastic(X_tr)\n",
    "    y_train_pred_heteroscedastic_mean.append(mean.detach().numpy())\n",
    "    y_train_pred_heteroscedastic_var.append(torch.exp(log_var).detach().numpy())\n",
    "\n",
    "# Make multiple stochastic predictions (MC Dropout) on the test data\n",
    "y_test_pred_heteroscedastic_mean = []\n",
    "y_test_pred_heteroscedastic_var = []\n",
    "for i in range(n_samples):\n",
    "    mean, log_var = tr_model_heteroscedastic(X_te)\n",
    "    y_test_pred_heteroscedastic_mean.append(mean.detach().numpy())\n",
    "    y_test_pred_heteroscedastic_var.append(torch.exp(log_var).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446f5d3",
   "metadata": {},
   "source": [
    "Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcfc150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! handling the train data\n",
    "# Calculate the mean of the predictions on the train data\n",
    "print(f\"Shape of y_train_pred_heteroscedastic_mean: {np.array(y_train_pred_heteroscedastic_mean).shape}\")\n",
    "mean_train_pred_heteroscedastic = np.mean(y_train_pred_heteroscedastic_mean, axis=0)  # shape: (31206, 1)\n",
    "print(mean_train_pred_heteroscedastic.shape)\n",
    "print(mean_train_pred_heteroscedastic[:5])  # Print first 5 predictions for verification\n",
    "print(y_train[:5])\n",
    "# Calculate R² score on the train data\n",
    "r2_train = r2_score(y_train, mean_train_pred_heteroscedastic)\n",
    "print(f\"R² on Train Data: {r2_train:.3f}\")\n",
    "\n",
    "#! handling the test data\n",
    "# Select a random subset of test data for visualization\n",
    "num_points = 100\n",
    "np.random.seed(42)\n",
    "\n",
    "random_indices = np.random.choice(len(X_te), num_points, replace=False)\n",
    "random_indices.sort()\n",
    "\n",
    "# Calculate the mean and standard deviation of the predictions on the test data\n",
    "mean_test_pred_heteroscedastic = np.mean(y_test_pred_heteroscedastic_mean, axis = 0)\n",
    "\n",
    "# Calculate epistemic\n",
    "epistemic_var_heteroscedastic = np.var(y_test_pred_heteroscedastic_mean, axis=0)\n",
    "print(f\"Epistemic Variance: {epistemic_var_heteroscedastic.mean():.6f}\")\n",
    "\n",
    "# Calculate aleatoric variance (heteroscedastic)\n",
    "aleatoric_var_heteroscedastic = np.mean(y_test_pred_heteroscedastic_var, axis=0)\n",
    "print(f\"Aleatoric Variance: {aleatoric_var_heteroscedastic.mean():.6f}\")\n",
    "\n",
    "# Calculate total standard deviation\n",
    "total_std_heteroscedastic = np.sqrt(epistemic_var_heteroscedastic + aleatoric_var_heteroscedastic)\n",
    "print(f\"Total Standard Deviation: {total_std_heteroscedastic.mean():.6f}\")\n",
    "\n",
    "print(f' y_test std: {y_test_tensor.std().item()}')\n",
    "\n",
    "# Calculate R² score for the test data\n",
    "r2_test = r2_score(y_test_tensor.cpu(), mean_test_pred_heteroscedastic)\n",
    "print(f\"R² on Test Data: {r2_test:.3f}\")\n",
    "\n",
    "# Assign descriptive variable names for MC Dropout mean and standard deviation\n",
    "mc_mean = mean_test_pred_heteroscedastic  # Predicted mean for each test point\n",
    "mc_std = total_std_heteroscedastic    # Predicted standard deviation for uncertainty\n",
    "\n",
    "# Define the confidence interval bounds (95% CI ≈ mean ± 2*std)\n",
    "mc_lower_bound = (mc_mean - 2 * mc_std).reshape(-1)  # Ensure it is a 1D array\n",
    "mc_upper_bound = (mc_mean+ 2 * mc_std).reshape(-1)  # Ensure it is a 1D array\n",
    "\n",
    "print(f\"Shape of mc_lower_bound: {mc_lower_bound.shape}, mc_upper_bound: {mc_upper_bound.shape}\")\n",
    "\n",
    "# calculate the coverage of the confidence interval\n",
    "in_interval = (y_test >= mc_lower_bound) & (y_test <= mc_upper_bound)\n",
    "# number of true\n",
    "counter = 0\n",
    "# Print whether each true value is within the confidence interval\n",
    "for i in in_interval:\n",
    "    if i:\n",
    "        counter += 1\n",
    "\n",
    "print(f\"Number of true values within the confidence interval: {counter} out of {len(in_interval)}\")\n",
    "\n",
    "# Calculate coverage (percentage of true values within the CI)\n",
    "coverage = np.mean(in_interval) * 100  # in percentage\n",
    "\n",
    "# Print the coverage value\n",
    "print(f\"Coverage: {coverage:.2f}%\")\n",
    "\n",
    "# Check whether each true value from random indices lies within the 95% confidence interval\n",
    "# If yes, the point will be green; if not, red\n",
    "in_interval_rand_indi = (y_test[random_indices] >= mc_lower_bound[random_indices]) & (y_test[random_indices] <= mc_upper_bound[random_indices])\n",
    "colors = ['tab:green' if inside else 'tab:red' for inside in in_interval_rand_indi]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "x_axis = np.arange(num_points)  # Create an index axis for plotting\n",
    "\n",
    "# Plot the predicted mean\n",
    "plt.plot(x_axis, mc_mean[random_indices], label=\"Prediction (mean)\", color='tab:blue')\n",
    "\n",
    "# Plot the confidence interval as a shaded region\n",
    "plt.fill_between(x_axis, mc_lower_bound[random_indices], mc_upper_bound[random_indices], alpha=0.4,\n",
    "                 color='tab:blue', label='95% Confidence Interval')\n",
    "\n",
    "# Scatter plot of true values with color-coded points based on interval inclusion\n",
    "plt.scatter(x_axis, y_test_tensor[random_indices].cpu().flatten(), label=\"True Values\", c=colors, s=25, zorder=3)\n",
    "\n",
    "# Final plot settings\n",
    "plt.title(\"MC Dropout heteroscedastic Prediction with Uncertainty\")\n",
    "plt.xlabel(\"Test Point Index\")\n",
    "plt.ylabel(\"x_Einzug [mm]\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
